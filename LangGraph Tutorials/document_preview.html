
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Markdown Preview</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 40px;
                line-height: 1.6;
            }
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
            }
            th, td {
                border: 1px solid #333;
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2;
            }
            pre {
                background-color: #f4f4f4;
                padding: 10px;
                overflow-x: auto;
            }
        </style>
    </head>
    <body>
        <h1><strong>T HE U NIVERSITY OF Q UEENSLAND</strong></h1>
<h3>Enhanced Rail Test Automation</h3>
<h5>Student Name: Isha, JOSHI Course Code: ENGG7290 Supervisor: Graeme Smith – Associate Professor School of Information Technology and Electrical Engineering Submission date: 25 [th] June 2020</h5>
<p><em>Faculty of Engineering, Architecture and Information Technology</em></p>
<h2>E XECUTIVE S UMMARY</h2>
<p>Hitachi Rail STS (Hitachi) has been contracted to deliver and maintain an Automated Train Operation</p>
<p>(ATO) system for Rio Tinto Iron Ore as a part of their AutoHaul® project. This ATO system facilitates</p>
<p>the driverless movement of trains in a railway network in Western Australia. Because of its safety</p>
<p>critical nature, any modifications made to the AutoHaul® system require extensive testing before they</p>
<p>can be rolled out. Presently, this testing is performed manually and uses a lot of testing time and</p>
<p>resources. Hitachi has commissioned the design and development of an automated testing tool which</p>
<p>can be used to reduce the testing time and improve testing efficiency. The testing would focus on the</p>
<p>communication interfaces between various AutoHaul® sub-systems.</p>
<p>The primary aim of this project was to design and develop an integrated testing framework to support</p>
<p>the automated testing of the AutoHaul® project. The testing focused on testing the behaviour of the</p>
<p>Fiber Interface Processor (FIP) and the Train Control Sub-system (TCS) by testing the communication</p>
<p>interfaces between these sub-systems and other AutoHaul® sub-systems.</p>
<p>This report provides an overview of the AutoHaul® system and details about the specific interfaces</p>
<p>that testing is being automated for. A review of literature relevant to designing an automated testing</p>
<p>framework is also present.</p>
<p>This report also delivers a summary of the design and outcomes of the tool used to test the FIP’s</p>
<p>behaviour. This tool, called the FIP Tester, simulates the behaviour of the Interlocking (IXL) devices</p>
<p>that connect to the FIP. It verifies that the FIP is responding correctly to messages sent by the IXL</p>
<p>simulators in order to ensure that the FIP is behaving correctly. Furthermore, the report provides a</p>
<p>summary of the testing activities conducted to validate the behaviour of the FIP Tester.</p>
<p>Similarly, this report provides a summary of the design and project outcomes of enhancing a pre
existing tool, called the TCS Testing Suite. Prior to the project, this tool was developed at Hitachi to</p>
<p>begin to automate the TCS testing. It contains the testing framework required to automate the testing</p>
<p>procedure that is conducted by Hitachi.</p>
<p>During this project, the TCS Testing Suite’s capabilities were enhanced to make it capable of running</p>
<p>more complex tests. Additionally, a Log Parser which iterates through various log files and consolidates</p>
<p>all of the errors into a single file was developed to aid users in troubleshooting while using the TCS</p>
<p>Testing Suite. The report also provides a summary of the testing activities performed to validate the</p>
<p>behaviour of the TCS Testing Suite enhancements and the Log Parser.</p>
<p>Overall, the developed FIP Tester and the enhanced TCS Testing Suite can be reliably used to replace</p>
<p>a lot of the manual testing activities conducted at Hitachi.</p>
<p>Page | III</p>
<h2>A CKNOWLEDGEMENTS</h2>
<p>I would like to sincerely thank a number of people who have helped me throughout this project.</p>
<p>Firstly, I would like to thank my supervisor Associated Professor Graeme Smith for the guidance and</p>
<p>advice that was provided throughout the semester which helped shape this project.</p>
<p>I would also like to thank the EAIT Employability team and Dr Christopher Leonardi at the University</p>
<p>of Queensland for all of their help in organising my placement at Hitachi Rail STS.</p>
<p>I would also like to thank Hitachi Rail STS, the host company, for allowing me the opportunity to work</p>
<p>on this project and learn about the railway industry and the AutoHaul® project. In particular I would</p>
<p>like to express my gratitude to my supervisory team at Hitachi, Dr Anthony MacDonald, Lionel Van</p>
<p>Den Berg and most of all Michal Cedrych for all of the support and advice I was given throughout the</p>
<p>course of the project. I would also like to thank Ujas Soni, Andrew Mijat and Benjamin Mountford at</p>
<p>Hitachi for all of the technical advice and support they gave me throughout the project.</p>
<p>Page | IV</p>
<h2>T ABLE OF C ONTENTS</h2>
<p>Executive Summary ................................................................................................................................ III</p>
<p>Acknowledgements ................................................................................................................................ IV</p>
<p>List Of Figures ........................................................................................................................................ VII</p>
<p>List Of Tables ........................................................................................................................................ VIII</p>
<p>1.0 Introduction ................................................................................................................................ 1</p>
<p>1.1 Placement Context .................................................................................................................. 1</p>
<p>1.2 Placement Purpose ................................................................................................................. 1</p>
<p>2.0 Technical Background ................................................................................................................. 2</p>
<p>2.1 The AutoHaul® Project ............................................................................................................ 2</p>
<p>2.2 Interface Details And Current Testing Methods ..................................................................... 5</p>
<p>2.2.1 Interlocking – FIP Interface ............................................................................................. 5</p>
<p>2.2.2 TCS Interfaces .................................................................................................................. 7</p>
<p>2.3 Literature Review .................................................................................................................. 10</p>
<p>2.3.1 Designing a Test Automation Framework ..................................................................... 11</p>
<p>2.3.2 Evaluating Testing Tools ................................................................................................ 12</p>
<p>2.3.3 Designing Test Cases ..................................................................................................... 12</p>
<p>2.3.4 Summary of Literature Review ..................................................................................... 13</p>
<p>3.0 Project Description .................................................................................................................... 14</p>
<p>3.1 Aims And Objectives ............................................................................................................. 14</p>
<p>3.2 Scope ..................................................................................................................................... 14</p>
<p>3.3 Project Deliverables .............................................................................................................. 16</p>
<p>3.4 Project Management ............................................................................................................ 16</p>
<p>4 Design ........................................................................................................................................... 17</p>
<p>4.1 FIP Tester .............................................................................................................................. 17</p>
<p>4.1.1 Test Framework Design ................................................................................................. 17</p>
<p>4.1.2 Test Case Design ........................................................................................................... 19</p>
<p>4.1.3 GUI Design ..................................................................................................................... 23</p>
<p>4.1.4 Performance Assessment Design .................................................................................. 24</p>
<p>4.1.5 Documentation ............................................................................................................. 24</p>
<p>4.2 TCS Testing Suite ................................................................................................................... 25</p>
<p>4.2.1 Increasing Testing Capabilities ...................................................................................... 26</p>
<p>4.2.2 Log Parser ...................................................................................................................... 27</p>
<p>4.2.3 Performance Assessment.............................................................................................. 27</p>
<p>4.2.4 Documentation ............................................................................................................. 28</p>
<p>Page | V</p>
<p>5 Project Outcomes ......................................................................................................................... 29</p>
<p>5.1 FIP Tester .............................................................................................................................. 29</p>
<p>5.1.1 The Testing Tool ............................................................................................................ 29</p>
<p>5.1.2 The GUI.......................................................................................................................... 30</p>
<p>5.1.3 Test Cases ...................................................................................................................... 30</p>
<p>5.1.4 Performance Assessment.............................................................................................. 30</p>
<p>5.1.5 Documentation ............................................................................................................. 34</p>
<p>5.2 TCS Testing Suite ................................................................................................................... 34</p>
<p>5.2.1 The Enhanced TCS Testing Suite ................................................................................... 34</p>
<p>5.2.2 The Log Parser ............................................................................................................... 36</p>
<p>5.2.3 Performance Assessment.............................................................................................. 36</p>
<p>6 Conclusion And Recommendations ............................................................................................. 39</p>
<p>6.1 FIP Tester .............................................................................................................................. 39</p>
<p>6.2 TCS Testing Suite ................................................................................................................... 39</p>
<p>6.3 Project Improvements .......................................................................................................... 40</p>
<p>7 References .................................................................................................................................... 41</p>
<p>8 Appendix A: Project Management Summary ............................................................................... 44</p>
<p>8.1 Project Timeline And Resources ........................................................................................... 44</p>
<p>8.1.1 Background Research Stage .......................................................................................... 44</p>
<p>8.1.2 Design Stage .................................................................................................................. 44</p>
<p>8.1.3 Implementation Stage ................................................................................................... 44</p>
<p>8.1.4 Documentation Stage ................................................................................................... 45</p>
<p>8.1.5 Project Timeline ............................................................................................................ 45</p>
<p>8.2 Risk Analysis .......................................................................................................................... 47</p>
<p>8.2.1 Operational Health And Safety Risks ............................................................................ 47</p>
<p>8.2.2 Project Scheduling Risks ................................................................................................ 48</p>
<p>8.2.3 Risk Matrix .................................................................................................................... 49</p>
<p>8.3 Project Deliverables Outcomes ............................................................................................. 50</p>
<p>8.4 Opportunities ........................................................................................................................ 50</p>
<p>9 Appendix B: FIP Messages ............................................................................................................ 51</p>
<p>10 Appendix C: TCS Message Protocols ............................................................................................ 52</p>
<p>10.1 TCS – ATOC Communication Protocol................................................................................... 52</p>
<p>10.2 TCS – RES Communication Protocol ...................................................................................... 52</p>
<p>10.3 TCS – VSS Communication Protocol ...................................................................................... 53</p>
<p>11 Appendix D: Test Bench Set-Up ................................................................................................... 54</p>
<p>12 Appendix E: FIP Tester Workflow Diagrams ................................................................................. 55</p>
<p>Page | VI</p>
<p>13 Appendix F: FIP Tester Tests ........................................................................................................ 57</p>
<p>14 Appendix G: TCS Testing Suite Architecture ................................................................................ 58</p>
<h2>L IST O F F IGURES</h2>
<p>Figure 1 AutoHaul® system breakdown .................................................................................................. 1</p>
<p>Figure 2 High level system architecture diagram of relevant sub-systems and interfaces .................... 3</p>
<p>Figure 3 FIP to IXL connections in the field ............................................................................................. 5</p>
<p>Figure 4 FIP to IXL connections in a test environment ........................................................................... 6</p>
<p>Figure 5 Example test from TCS integration test specification document [19] ...................................... 8</p>
<p>Figure 6 TCS Testing Suite behaviour ...................................................................................................... 9</p>
<p>Figure 7 Ranking of test case criteria as determined by Adlemo et al. [29] ......................................... 13</p>
<p>Figure 8 High level overview of FIP Tester tool .................................................................................... 19</p>
<p>Figure 9 Generic test case workflow diagram....................................................................................... 20</p>
<p>Figure 10 Flowchart of process to test new configurations management – See Figure 32 for the startup sequence testing .............................................................................................................................. 20</p>
<p>Figure 11 GUI design sketch .................................................................................................................. 23</p>
<p>Figure 12 High level TCS Testing Suite test engine design .................................................................... 25</p>
<p>Figure 13 Code showing a train being created before the enhancements .......................................... 26</p>
<p>Figure 14 High level log parser design .................................................................................................. 27</p>
<p>Figure 15 System architecture of FIP Tester ......................................................................................... 29</p>
<p>Figure 16 High level overview of designed FIP Tester .......................................................................... 29</p>
<p>Figure 17 GUI for the FIP Tester ............................................................................................................ 30</p>
<p>Figure 18 Confirmation pop-up for the FIP Tester ................................................................................ 30</p>
<p>Figure 19 'tcpdump' output .................................................................................................................. 31</p>
<p>Figure 20 Linux command ‘netcat’ output ............................................................................................ 31</p>
<p>Figure 21 Python terminal output of test case 1 (Start-up sequence) with correct replies as seen by the</p>
<p>FIP Simulator ......................................................................................................................................... 32</p>
<p>Figure 22 Python terminal output of test case 1 (Start-up sequence) with incorrect replies as seen by</p>
<p>the FIP Simulator ................................................................................................................................... 32</p>
<p>Figure 23 FIP Tester log file when incorrect message is received ........................................................ 32</p>
<p>Figure 24 FIP Tester log showing a passing test.................................................................................... 33</p>
<p>Figure 25 FIP Tester log showing a failing test ...................................................................................... 33</p>
<p>Figure 26 Screenshot from user guide .................................................................................................. 34</p>
<p>Figure 27 Code showing the helper function implemented to create multiple trains ......................... 35</p>
<p>Figure 28 Screenshot of the collated errors file ................................................................................... 36</p>
<p>Page | VII</p>
<p>Figure 29 Example of a test case used to verify test behaviour with pass/fail comments ................... 37</p>
<p>Figure 30 Project timeline ..................................................................................................................... 46</p>
<p>Figure 31 Test bench set-up in Brisbane ............................................................................................... 54</p>
<p>Figure 32 Flowchart of process to test initial testing sequences ......................................................... 55</p>
<p>Figure 33 Flowchart of process to test site-like transmission errors .................................................... 56</p>
<p>Figure 34 Start-up test code snippet .................................................................................................... 57</p>
<p>Figure 35 TCS Testing Suite architecture diagram [32] ......................................................................... 58</p>
<h2>L IST O F T ABLES</h2>
<p>Table 1 Table of abbreviations ............................................................................................................... IX</p>
<p>Table 2 Description of sub-systems ........................................................................................................ 4</p>
<p>Table 3 Polling cycle for FIP and IXL communications [15] ..................................................................... 6</p>
<p>Table 4 TCS interface details ................................................................................................................... 7</p>
<p>Table 5 Types of tests to implement autonomously ............................................................................ 14</p>
<p>Table 6 Interfaces covered in the scope ............................................................................................... 15</p>
<p>Table 7 Key deliverables list .................................................................................................................. 16</p>
<p>Table 8 Summary of project stages ....................................................................................................... 16</p>
<p>Table 9 System requirements for the FIP Tester .................................................................................. 18</p>
<p>Table 10 Test specifications for FIP test 1: Testing the initial start-up sequence ................................ 21</p>
<p>Table 11 Test specifications for FIP test 2: Testing with site-like conditions........................................ 21</p>
<p>Table 12 Test specifications for FIP test 3: Testing how new configurations are handled ................... 23</p>
<p>Table 13 Tests to verify the FIP Tester’s behaviour .............................................................................. 24</p>
<p>Table 14 Results from manually checking the logs in Figure 24 ........................................................... 33</p>
<p>Table 15 Results from manually checking the logs In Figure 25 ........................................................... 34</p>
<p>Table 16 Results of asking a tester to debug using the Log Parser ....................................................... 38</p>
<p>Table 17 OH&amp;S risks summary .............................................................................................................. 47</p>
<p>Table 18 Scheduling risks summary ...................................................................................................... 48</p>
<p>Table 19 Risk matrix .............................................................................................................................. 49</p>
<p>Table 20 Outcomes of key project deliverables .................................................................................... 50</p>
<p>Table 21 Project opportunities ............................................................................................................. 50</p>
<p>Table 22 FIP – IXL Message types and corresponding control characters [15] .................................... 51</p>
<p>Table 23 TCS – ATOC message protocol [4] .......................................................................................... 52</p>
<p>Table 24 TCS – VSS message protocol [12] ........................................................................................... 53</p>
<p>Page | VIII</p>
<p><em>Table 1 Table of abbreviations</em></p>
<table>
<thead>
<tr>
<th>Acronym Long Term</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>AMMI</strong></td>
<td>Automation Man Machine Interface</td>
</tr>
<tr>
<td><strong>AS</strong></td>
<td>Automation Server</td>
</tr>
<tr>
<td><strong>aTest</strong></td>
<td>A proprietary tool used at Hitachi to simulate ATOC connections</td>
</tr>
<tr>
<td><strong>ATO</strong></td>
<td>Automatic Train Operation</td>
</tr>
<tr>
<td><strong>ATOC</strong></td>
<td>Automation Train Operation Controller</td>
</tr>
<tr>
<td><strong>CTC</strong></td>
<td>Central Train Control</td>
</tr>
<tr>
<td><strong>FIP</strong></td>
<td>Field Interface Processor</td>
</tr>
<tr>
<td><strong>GUI</strong></td>
<td>Graphical User Interface</td>
</tr>
<tr>
<td><strong>HMI</strong></td>
<td>Human Machine Interface</td>
</tr>
<tr>
<td><strong>IXL</strong></td>
<td>Interlocking</td>
</tr>
<tr>
<td><strong>MISS</strong></td>
<td>Microlok Interlocking Simulation System</td>
</tr>
<tr>
<td><strong>RTIO</strong></td>
<td>Rio Tinto Iron Ore</td>
</tr>
<tr>
<td><strong>RES</strong></td>
<td>RTIO External Servers</td>
</tr>
<tr>
<td><strong>SIL</strong></td>
<td>Safety Integrity Level</td>
</tr>
<tr>
<td><strong>TCS</strong></td>
<td>Train Control Sub-system</td>
</tr>
<tr>
<td><strong>UDP</strong></td>
<td>User Datagram Protocol</td>
</tr>
<tr>
<td><strong>UI</strong></td>
<td>User Interface</td>
</tr>
<tr>
<td><strong>VM</strong></td>
<td>Virtual Machine</td>
</tr>
<tr>
<td><strong>VSS</strong></td>
<td>Vital Safety Server</td>
</tr>
</tbody>
</table>
<p>Page | IX</p>
<h2>1.0 I NTRODUCTION</h2>
<h5>1.1 P LACEMENT C ONTEXT</h5>
<p>Rio Tinto Iron Ore (RTIO) operates a heavy-haul freight railway network in Western Australia [1]. This</p>
<p>network moves iron ore from mines in the Pilbara region to coastal ports for shipping overseas. The</p>
<p>AutoHaul® project provides an Automated Train Operation (ATO) system to facilitate the driverless</p>
<p>movement of the trains on the mainline in RTIO’s network [2] [3]. As shown in Figure 1, AutoHaul®</p>
<p>uses three main sub-systems to control and monitor locomotives and ensure their safe movement in</p>
<p>the network. These sub-systems are the Trainborne System, the Control Centre and the Wayside</p>
<p>Systems. More details about these sub-systems are given in Section 2.0. Hitachi Rail STS (Hitachi) was</p>
<p>contracted to deliver and maintain this ATO system.</p>
<p><em>Figure 1 AutoHaul® system breakdown</em></p>
<p>Because the operations of trains in the AutoHaul® network are safety critical, any modifications made</p>
<p>to the AutoHaul® system require extensive testing before they are rolled out. Hitachi’s integration</p>
<p>team is responsible for performing system-wide testing. This includes testing the system as a whole</p>
<p>to ensure that all of the sub-systems are integrating properly with each other and all of the</p>
<p>communication interfaces between different sub-systems work as required. Presently, the integration</p>
<p>team uses manual testing for this purpose.</p>
<p>Due to the AutoHaul® project’s complexity, manual testing is a time consuming and repetitive process</p>
<p>that does not always identify the “edge case” issues in the system. As such, there is a potential to</p>
<p>automate these tests.</p>
<h5>1.2 P LACEMENT P URPOSE</h5>
<p>Because Hitachi regularly roll out upgrades and modifications for the AutoHaul® project, they are</p>
<p>investing in methods to reduce testing time by automating tests. As such, Hitachi wish to develop an</p>
<p>automated testing tool that can be used to:</p>
<ul>
<li>
<p>Automate time-consuming and repetitive tests,</p>
</li>
<li>
<p>Free up the tester’s time and allow them to perform different tests and therefore improve</p>
</li>
</ul>
<p>the thoroughness of the testing and</p>
<ul>
<li>Reduce the products costs earlier in the testing stage of the product’s lifecycle.</li>
</ul>
<p>Page | 1</p>
<h2>2.0 T ECHNICAL B ACKGROUND</h2>
<h5>2.1 T HE A UTO H AUL ® P ROJECT</h5>
<p>The AutoHaul® project introduced an Automated Train Operation (ATO) system so that trains are able</p>
<p>to move autonomously on the mainline of RTIO’s railway network in Western Australia [3].</p>
<p>As seen in Figure 1, the three sub-systems AutoHaul® uses to ensure the safe movement of trains are</p>
<p>the Trainborne System, the Control Centre and the Wayside Systems [3].</p>
<p>The Trainborne System is installed in each AutoHaul® locomotive and uses the Automatic Train</p>
<p>Operations Controller (ATOC) as its primary control system. The ATOC can be thought of as the train’s</p>
<p>driver. It interfaces to the locomotive’s equipment to perform tasks such as braking, accelerating and</p>
<p>collision detection. It also communicates with the Control Centre to receive instructions and transfer</p>
<p>relevant data [4].</p>
<p>Located in Perth, the Control Centre uses the Train Control Sub-system (TCS) to control the majority</p>
<p>of the AutoHaul® network [3]. It manages the train routing, mission planning and provides a user</p>
<p>interface for operators to use.</p>
<p>The Wayside Systems contains a range of devices that are placed on the side of the tracks at various</p>
<p>points in the network. Collectively, this system performs functions such as train tracking, interlocking</p>
<p>and controlling intersections on the track [5].</p>
<p>A high level system architecture containing the sub-systems and communication interfaces relevant</p>
<p>for this project is provided in Figure 2. A description of the sub-systems is provided in Table 2 and</p>
<p>more details are given about these interfaces in Section 2.2.</p>
<p>Page | 2</p>
<p><em>Figure 2 High level system architecture diagram of relevant sub-systems and interfaces</em></p>
<p>Page | 3</p>
<p><em>Table 2 Description of sub-systems</em></p>
<table>
<thead>
<tr>
<th>Sub-system Functionality</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>WAYSIDE SYSTEMS</strong></td>
<td><strong>WAYSIDE SYSTEMS</strong></td>
</tr>
<tr>
<td><strong>Wayside Equipment</strong></td>
<td>Equipment which is placed on the trackside and is used to monitor the<br>health and status of track-based assets [6].</td>
</tr>
<tr>
<td><strong>Interlocking (IXL)</strong></td>
<td>This component of signaling systems ensures that the railway behaves in a<br>safe manner and is fail-safe [7] [8]. IXL devices do this by:<br> <br>Performing vital functions such as route setting,<br> <br>Sending signaling information to the TCS and<br> <br>Receiving commands, such as clearing signals, from the TCS.<br>Hitachi’s Microlok II is used as the IXL devices.</td>
</tr>
<tr>
<td><strong>TRAINBORNE SYSTEM</strong></td>
<td><strong>TRAINBORNE SYSTEM</strong></td>
</tr>
<tr>
<td><strong>Locomotive</strong><br><strong>Equipment</strong></td>
<td>A collection of equipment and computer systems that are used to monitor<br>and perform train operations such as interfacing with the ATOC and<br>operating the throttle and brakes [3].<br></td>
</tr>
<tr>
<td><strong>Automatic Train</strong><br><strong>Operations</strong><br><strong>Controller (ATOC)</strong></td>
<td>The primary control system of the train. It communicates with other sub-<br>systems and controls all of the train’s operations [4].</td>
</tr>
<tr>
<td><strong>CONTROL CENTRE</strong></td>
<td><strong>CONTROL CENTRE</strong></td>
</tr>
<tr>
<td><strong>Train Control</strong><br><strong>Sub-system (TCS)</strong></td>
<td>The system used to monitor and control the railway network. It is used to<br>set routes, track train movement, manage alarms and perform monitoring<br>actions that were previously undertaken by drivers [9].</td>
</tr>
<tr>
<td><strong>Centralised Train</strong><br><strong>Control (CTC)</strong></td>
<td>A train control system that provides the network overview, shows<br>indications and allows route setting and train sheet management [10].</td>
</tr>
<tr>
<td><strong>Automation Man</strong><br><strong>Machine Interface</strong><br><strong>(AMMI)</strong></td>
<td>A user interface to all the trains and locomotives in the AutoHaul® system.<br>It allows users to access the CTC [11].</td>
</tr>
<tr>
<td><strong>Automation Server</strong><br><strong>(AS)</strong></td>
<td>A messaging service that acts as a gateway for the TCS and the rest of the<br>systems in the AutoHaul® system [5].</td>
</tr>
<tr>
<td><strong>Vital Safety Server</strong><br><strong>(VSS)</strong></td>
<td>Provides movement authorities to the train based on data from the<br>interlocking and level crossings [12]. The VSS also acts as a user interface<br>and allows users to set commands which are relayed to the rest of the<br>system.</td>
</tr>
<tr>
<td><strong>Field Interface</strong><br><strong>Processor (FIP)</strong></td>
<td>A device which connects to all of the IXLs in the field and facilitates the<br>exchange of information between the IXLs and the TCS [13].</td>
</tr>
<tr>
<td><strong>RTIO External</strong><br><strong>Systems</strong></td>
<td>A TIBCO Enterprise Message Service which acts as the interface between<br>the AutoHaul® and RTIO’s other systems [14]. RTIO External Systems are<br>used for functions such as producing an electronic train graph.</td>
</tr>
</tbody>
</table>
<p>Page | 4</p>
<h5>2.2 I NTERFACE D ETAILS A ND C URRENT T ESTING M ETHODS</h5>
<p>2.2.1 Interlocking – FIP Interface</p>
<p><em>2.2.1.1</em> <em>FIP Functionality</em></p>
<p>Each IXL device acts as a User Datagram Protocol (UDP) server which has an open socket on a specific</p>
<p>IP address and port. The FIP runs multiple UDP clients which connect to IXLs on the given IP addresses</p>
<p>and ports to enable bi-directional communication as shown in Figure 3.</p>
<p><em>Figure 3 FIP to IXL connections in the field</em></p>
<p>Please note that only 3 connections are shown in Figure 3, the AutoHaul® project actually uses many</p>
<p>more IXLs.</p>
<p><em>2.2.1.2</em> <em>Data Contents</em></p>
<p>The data sent between the FIP and the IXLs uses the Genisys protocol [15]. Each message contains a</p>
<p>control character, the address of the recipient/sender IXL, the interlocking data bytes, a security</p>
<p>checksum and a termination character. More details about the packet structure are provided in</p>
<p>Appendix B: FIP Messages.</p>
<p>The interlocking data bytes coming out of the Microlok IIs are at a Safety Integrity Level (SIL) 4 [16],</p>
<p>and are therefore considered to be highly reliable. The FIP is only responsible for collating this data</p>
<p>and passing it along to the TCS, where it is validated. As such, validating the interlocking data is outside</p>
<p>the scope of this project. Instead, the main focus of the testing is validating the FIP’s behaviour.</p>
<p>To request for data, the FIP uses a pre-defined polling cycle [15] as shown in Table 3. There are five</p>
<p>types of messages that can be sent from the FIP to the IXL. These are poll messages, control messages,</p>
<p>recall messages, execute messages and master acknowledge messages. Furthermore, there are three</p>
<p>types of messages that can be sent back to the FIP. These are indication messages, control check back</p>
<p>messages and slave acknowledgement messages. More details about the messages are provided in</p>
<p>Appendix B: FIP Messages.</p>
<p>Page | 5</p>
<p><em>Table 3 Polling cycle for FIP and IXL communications [15]</em></p>
<table>
<thead>
<tr>
<th>FIP to IXL Message Expected Reply</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FIP to IXL1</strong></td>
<td><strong>FIP to IXL2</strong></td>
<td><strong>FIP to IXL3</strong></td>
<td><strong>IXL to FIP</strong></td>
</tr>
<tr>
<td>Recall<br>message</td>
<td>Recall<br>message</td>
<td>Recall<br>message</td>
<td>Indication message from each IXL.</td>
</tr>
<tr>
<td>Control<br>message</td>
<td>Control<br>message</td>
<td>Control<br>message</td>
<td>Indication or slave acknowledge message from each<br>IXL.</td>
</tr>
<tr>
<td>Poll<br>message</td>
<td>Poll<br>message</td>
<td>Poll<br>message</td>
<td>Indication or slave acknowledge message from each<br>IXL.</td>
</tr>
<tr>
<td>Recall<br>message</td>
<td>Poll<br>message</td>
<td>Poll<br>message</td>
<td>Indication message from IXL1 and slave acknowledge<br>messages from other IXLs.</td>
</tr>
<tr>
<td>Poll<br>message</td>
<td>Recall<br>message</td>
<td>Poll<br>message</td>
<td>Indication message from IXL2 and slave acknowledge<br>messages from other IXLs.</td>
</tr>
<tr>
<td>Poll<br>message</td>
<td>Poll<br>message</td>
<td>Recall<br>message</td>
<td>Indication message from IXL3 and slave acknowledge<br>messages from other IXLs.</td>
</tr>
<tr>
<td>Recall<br>message</td>
<td>Poll<br>message</td>
<td>Poll<br>message</td>
<td>Indication message from IXL1 and slave acknowledge<br>messages from other IXLs.</td>
</tr>
</tbody>
</table>
<p><em>2.2.1.3</em> <em>Current Testing Methodology</em></p>
<p>In a test environment, setting up the physical connections between the IXLs and the FIP is not feasible</p>
<p>as there are too many hardware requirements. As such, a combination of a Virtual Serial Port Emulator</p>
<p>(VSPE) and a Microlok Interlocking Simulation System (MISS) is used. The FIP requires the incoming</p>
<p>data to come from UDP connections. However, due to being limited by its legacy software, MISS can</p>
<p>only communicate using Transmission Control Protocol (TCP) communications. Therefore, a Socket</p>
<p>Cat (SoCat) router is used to convert messages between TCP and UDP protocols. The testing set-up is</p>
<p>illustrated in Figure 4.</p>
<p><em>Figure 4 FIP to IXL connections in a test environment</em></p>
<p>Page | 6</p>
<p>Currently the FIP behaviour is tested using two main methods. These are:</p>
<p>1) Verifying that the IXL data received at the TCS is correct. It is assumed that if the data is correct
when it arrives at the TCS then the FIP must be behaving correctly.
2) Manually checking the log files that are stored in the FIP to verify the messages. This testing is</p>
<p>only conducted when an end-to-end change, such as adding a new IXL device, is made [17].</p>
<p>This project will focus on automating the manual checks that are conducted of the FIP’s behaviour.</p>
<p>2.2.2 TCS Interfaces</p>
<p>As described in Section 2.1, the TCS is the main control system of the AutoHaul® network and is</p>
<p>therefore tested frequently.</p>
<p><em>2.2.2.1</em> <em>Relevant Interfaces</em></p>
<p>There are three communication interfaces within the scope of this project. These are the TCS – ATOC</p>
<p>interface, the TCS – RES interface and TCS – VSS interface. The key elements of these interfaces are</p>
<p>summarised in Table 4. Additional details about the specific packet structures and message types of</p>
<p>each communication interface are provided in Appendix C: TCS Message Protocols.</p>
<p><em>Table 4 TCS interface details</em></p>
<table>
<thead>
<tr>
<th>Interface Details</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TCS – ATOC</strong></td>
<td> <br>The TCS – ATOC interface is a collection of communication channels between<br>the TCS and each ATOC system that is active in the AutoHaul® network.<br> <br>Locomotives use this interface to send locomotive status information to the<br>TCS and receive instructions and software updates from the TCS.<br> <br>The interface uses the TCS – ATOC Protocol [4] as the messaging protocol.<br> <br>A propriety tool, aTest, is used to simulate messages that the ATOC would<br>usually send autonomously. Users can send these messages via a User<br>Interface (UI) or through commands set in a Python script.</td>
</tr>
<tr>
<td><strong>TCS – RES</strong></td>
<td> This interface allows the RES to send planned train sheets and timetables to<br>the TCS. It also allows the TCS to send data regarding the actual train<br>movement back to the RES.<br> <br>Communication occurs via a TIBCO Message Service.<br> <br>In the existing test environment, ActiveMQ is used as the messaging service<br>and Python scripts can be used to simulate RTIO messages.<br> <br>The interface uses the TCS – RTIO Protocol [14] as the messaging protocol.</td>
</tr>
<tr>
<td><strong>TCS – VSS</strong></td>
<td> This is a bi-directional communication channel that allows the TCS to send<br>locomotive supervision data to the VSS and receive track information back<br>from the VSS.<br> <br>The interface uses the TCS – VSS Protocol [12] as the messaging protocol.<br> <br>A VM running an instance of a VSS is running on the test bench.</td>
</tr>
</tbody>
</table>
<p>Page | 7</p>
<p><em>2.2.2.2</em> <em>Testing Methodology</em></p>
<p>Currently, two methods are used to test the TCS interfaces. These are manual testing and the TCS</p>
<p>Testing Suite.</p>
<p><strong>Manual Testing</strong></p>
<p>This is the most commonly used form of testing at Hitachi. Testing the TCS involves manually</p>
<p>performing sequences, such as creating a train sheet and verifying that all of the sub-systems behave</p>
<p>as expected. This testing is done at a test bench which runs either a version, or a simulation of all sub
systems deployed in the AutoHaul® project. Appendix D: Test Bench Set-Up shows an image of this</p>
<p>test bench.</p>
<p>The expected behaviour of the system is based on the TCS’s functional and behavioural expectations</p>
<p>[18] and is tested with tests laid out in the TCS Integration Test Specification document [19]. Figure 5</p>
<p>shows one test from this document. The test outlines activities that must be done before the test can</p>
<p>begin (preconditions), which are the sequences to perform during the test and the expected results</p>
<p>to each step.</p>
<p><em>Figure 5 Example test from TCS integration test specification document [19]</em></p>
<p>All of the tests must be run independently to each other. For example, step 1 of Test 3.33 in Figure 5</p>
<p>requires the tester to log into the CTC’s Human Machine Interface (HMI) as the Asset Health Evaluator</p>
<p>(AHE). At the end of Test 3.33 the user must log out as the AHE even if the next test requires them to</p>
<p>login as the AHE again. This adds a lot of time to the testing procedure as the testers must set-up and</p>
<p>remove the test environment for each test.</p>
<p>Furthermore, the testing methodology is very structured and does not leave much time for “creative</p>
<p>testing”. Therefore, the testing does not cover all of the edge cases and unexpected situations that</p>
<p>might arise in the field.</p>
<p>An experienced tester at Hitachi will require approximately an hour to complete the integration</p>
<p>testing. This time estimate assumes that no bugs are found. In reality, testers find that they get half</p>
<p>way through the set of tests before a test fails. After fixing the cause of the failure, they must restart</p>
<p>the testing from the beginning.</p>
<p>Page | 8</p>
<p><strong>TCS Testing Suite</strong></p>
<p>A test environment has been created to run some of the more basic tests autonomously. The test</p>
<p>environment is installed on a VM and uses Python scripts to perform the actions listed below.</p>
<ul>
<li>
<p>Initialise connections with local versions of AutoHaul® servers such as the RES and VSS,</p>
</li>
<li>
<p>Connect to the ATOC emulator, aTest and emulate messages sending from ATOC,</p>
</li>
<li>
<p>Connect to Squish, a tool created by Froglogic to automate UI testing [20],</p>
</li>
<li>
<p>Use Squish to mimic interacting with the UIs and verify that the system is behaving as</p>
</li>
</ul>
<p>expected,</p>
<ul>
<li>Use a library of helper functions to facilitate communication between the TCS and all other</li>
</ul>
<p>AutoHaul® servers, ATOC and Squish and</p>
<ul>
<li>Run tests and log the activities in various log files depending on the action being performed.</li>
</ul>
<p>This behaviour is summarised in Figure 6. Additionally, the file structure and architecture of the TCS</p>
<p>Testing Suite is provided in Appendix G: TCS Testing Suite Architecture.</p>
<p>.</p>
<p><em>Figure 6 TCS Testing Suite behaviour</em></p>
<p>To use the test suite, a user must run a particular test script in a PyCharm environment. Once a test</p>
<p>script is run it behaves as follows:</p>
<ol>
<li>The AutoHaul® servers and simulators are started. This includes, beginning connections to the</li>
</ol>
<p>Automation Server, VSS, and RTIO External Servers.</p>
<ol>
<li>A start-up function is implemented. The start-up function sets up all of the pre-conditions (such</li>
</ol>
<p>as creating and registering a train) that are required for the tests.</p>
<ol>
<li>The tests are run. Generally, between 1 and 5 tests are contained within each script. The tests are</li>
</ol>
<p>typically implemented by:</p>
<p>a. Making an event occur in the system. For example, creating and registering a train.</p>
<p>b. Verifying that all of the messages related to that event have been received at their</p>
<p>destination. For example, verify that the CTC has received the details for a new train.</p>
<p>Page | 9</p>
<p>c. Using Squish to verify that the relevant user interface has been updated. For example,</p>
<p>making sure that the new train ID is visible to the user.</p>
<p>d. A shut down function is implemented. This function resets the testing environment so</p>
<p>that it is back to its original status. For example, if a train has been registered during the</p>
<p>tests, then the train is deleted.</p>
<p>e. The servers and simulators are shut down. This includes closing all active connections.</p>
<ol>
<li>A Python console displays the outcome of each test to the user.</li>
</ol>
<p>Like the manual testing, the test suite also uses the TCS Integration Test Specifications documentation</p>
<p>[19] for the test case design. However, not all of the required tests from the document have been</p>
<p>implemented.</p>
<p>The current implementation only includes tests which check the functionality of a particular sub
system and that sub-system’s User Interface (UI) currently exist. For example, a test can be run to</p>
<p>check that a train sheet is created and registered in the correct user’s view. But it cannot perform</p>
<p>more complicated tests such as performing load testing to ensure multiple trains have been</p>
<p>registered.</p>
<p>Furthermore, the test suite is a complex system that requires many active connections and uses many</p>
<p>log files to keep track of the events in a test. Therefore debugging a test to determine the root cause</p>
<p>of failure can be a time consuming task. For example, as a part of registering a new train, a TCS</p>
<p>message is sent to the RES and the RES needs to send back a confirmation. If there is an error in</p>
<p>connecting to the RES and therefore no confirmation is received by the TCS, then the following</p>
<p>messages would be recorded in the most relevant log files.</p>
<ul>
<li>
<p>The TCS and CTC logs would record that the train could not be registered,</p>
</li>
<li>
<p>The Automation Server logs would show that there was a timeout in waiting for the</p>
</li>
</ul>
<p>confirmation message and</p>
<ul>
<li>The Automation Server logs would record that a connection could not be established with the</li>
</ul>
<p>RES. However, this message would have been recorded at the start of the start-up activities</p>
<p>and would have been hidden by the newer activities.</p>
<p>It would be up to the user to recognise that the error is due to connection issues, not problems with</p>
<p>the AutoHaul’s ® code or the test’s logic.</p>
<p>This project focused on enhancing the capabilities of the TCS Testing Suite so that more complicated</p>
<p>tests could run. It also provided a method for users to quickly determine the causes of failure in a</p>
<p>system.</p>
<h5>2.3 L ITERATURE R EVIEW</h5>
<p>An increasing amount of businesses have begun to use automated testing tools to improve the</p>
<p>efficiency of their testing and lower their product life cycle costs. The existing literature that is relevant</p>
<p>to the project covers the design and performance testing of automated testing tools and frameworks.</p>
<p>Page | 10</p>
<p>2.3.1 Designing a Test Automation Framework</p>
<p>The framework required for automated testing involves defining support structure of the automation</p>
<p>testing suite and the logical interactions between components within the testing suite. The project</p>
<p>will require an automated testing framework to be developed from scratch for testing the FIP – IXL</p>
<p>interface.</p>
<p>A paper by Bajaj [21] determines that a well-designed framework is essential for the automated testing</p>
<p>suite to be reusable, maintainable and of high quality. The author categorises the types of frameworks</p>
<p>for test automation under four main categories; modular frameworks, data-driven frameworks, hybrid</p>
<p>frameworks and key-word driven frameworks. These categories are also supported by Umar and</p>
<p>Zhanfang in their paper [22].</p>
<p>Méndez-Porras et al. [23] recommend using a top-down approach to designing an automated testing</p>
<p>framework. The authors recommend that initially the testing framework’s objectives and</p>
<p>implementation requirements should be identified. Then a design should be created that uses these</p>
<p>requirements. The design should initially be a high-levelled overview and then provide more details</p>
<p>about each element in it. The authors found that creating detailed descriptions for the framework and</p>
<p>its elements allowed them to minimise the number of test cases needed to identify all the errors in</p>
<p>the system they were testing.</p>
<p>Additionally Wang et al. [24], hypothesises that because modern train control systems are becoming</p>
<p>more complicated, a purely model based testing approach will not sufficiently test the system. Instead,</p>
<p>they propose using a hybrid framework that is created in a virtual environment. The authors built a</p>
<p>testing platform using an online Model-Based Testing (MBT) platform and a railway simulator. Their</p>
<p>testing platform automatically generated and executed test cases.</p>
<p>To evaluate the testing platform, a case study was built using a real Communication Based Train</p>
<p>Controller (CBTC) system and testing was conducted for 12 hours. It was found that in most cases, the</p>
<p>hybrid MBT could detect errors more efficiently and in less time than traditional testing methods.</p>
<p>However, because the hybrid MBT created and executed test cases simultaneously, it could not detect</p>
<p>some errors such as minor delay errors.</p>
<p><strong>User Interface Design</strong></p>
<p>For this project a UI was developed to increase the useability of the testing tools. The UI was designed</p>
<p>using the approach recommended by Borisov et al. [25]. The authors present an approach to designing</p>
<p>a Test and Diagnosis User Interface (TDUI) which can be used to test ambient intelligent systems in</p>
<p>production environments. Their UI design process focused on ensuring that the UI met the</p>
<p>requirements needed for a safety-critical system, such as task conformance, providing feedback to</p>
<p>users based on user actions and reporting on process status. The authors designed a TDUI using a two</p>
<p>layer structure. The first layer was focused on the outward appearance of the UI and used strategies</p>
<p>such as creating guideline resources, style guides and having a good page layout to reduce errors in</p>
<p>interactions. The second layer was the main Interaction Logic Layer (ILL) used to connect the Graphical</p>
<p>User Interface (GUI) to the rest of the test environment. The paper found that using the two layer</p>
<p>approach, allowed for the designed TDUI to be adaptive to new changes and meet individual user</p>
<p>requirements.</p>
<p>Page | 11</p>
<p><strong>Relevant Standards</strong></p>
<p>Because the railway industry is highly regulated the relevant standards must be followed to implement</p>
<p>a testing framework. As the AutoHaul® project is based on the European Train Control System (ETCS),</p>
<p>it uses standards set by the European Committee for Electrotechnical Standardization (CENELEC).</p>
<p>For this project, the CENELEC standard EN50128:2011 for “Railway applications – Communication,</p>
<p>signalling and process systems – Software for railway control and protection systems” [18] is</p>
<p>applicable. Based on the information in the standard, the project must include the development of</p>
<p>test specifications, a test procedure, and the tests must produce a test report.</p>
<p>2.3.2 Evaluating Testing Tools</p>
<p>The literature surrounding evaluating the performance of testing tools is primarily focused on</p>
<p>evaluating tools which are commercially available and may be used in various applications. Although</p>
<p>this project requires the evaluation of a tool developed for a specific purpose, the general criteria that</p>
<p>other papers have used to evaluate tools will still be relevant to the project.</p>
<p>As per Bajaj [21], the effectiveness of a testing tool or testing framework for a project is dependent</p>
<p>on the technology stack which is being tested, the testing requirements, the skill sets of the users and</p>
<p>the project’s testing budget.</p>
<p>Bajaj provides a summarised view of different testing tools and how they meet the above listed</p>
<p>selection criteria. The paper recommends that the chosen testing tool should be tested with a proof</p>
<p>of concept as soon as it is adopted. This allows the users to test the compatibility of the testing tools</p>
<p>with the existing systems and ultimately allows the users to be more confident when choosing the</p>
<p>final testing tool.</p>
<p>Similarly, in a paper by Polo et al. [26], various testing tools are compared based on their maturity</p>
<p>levels and the expertise level required to use them. The authors provide a comparison of different</p>
<p>testing tools and provide recommendations for tools based on the intended use. Overall, Polo et al.</p>
<p>concluded that the best testing tool for a system is the one which has the highest maturity level and</p>
<p>requires the lowest expertise level.</p>
<p>In his paper Jönsson [27], ranks the performance of automated GUI testing tools based on their defect</p>
<p>detection, repeatability of tests and time requirements from users. Jönsson compares Squish,</p>
<p>TestComplete and manual testing. The paper focused on comparing Squish, a GUI testing tool</p>
<p>developed by Froglogic [20], TestComplete, a GUI testing tool by Smart Bear [28] and manual testing.</p>
<p>Jönsson found that all three methods identified the same amount of defects, but Squish performed</p>
<p>better in repeatability testing and required less time to set-up initially than TestComplete. This is</p>
<p>useful to the project as the current TCS Testing Suite utilises Squish to automate GUI tasks.</p>
<p>2.3.3 Designing Test Cases</p>
<p>A large part of evaluating how effective a test tool is in uncovering a system’s flaws includes</p>
<p>developing high quality test cases. The literature surround developing test cases is relevant to</p>
<p>developing tests for the FIP – IXL interface.</p>
<p>Page | 12</p>
<p>In a preliminary investigation created for the Ontology-based Software Test cAse Generation (OSTAG),</p>
<p>Adlemo et al. consolidate and rank 15 criteria for “good” test case performance [29]. The authors use</p>
<p>existing literature to identify important criteria for evaluating test case performance and then ask 13</p>
<p>software experts to rank the criteria on a scale of 0 (not relevant at all) to 10 (highly relevant). All of</p>
<p>these software experts were from low to medium sized Swedish companies. The resulting rankings</p>
<p>are shown in Figure 7. Based on the findings of Adlemo et al. the designed test cases should be</p>
<p>repeatable, accurate and correct.</p>
<p><em>Figure 7 Ranking of test case criteria as determined by Adlemo et al. [29]</em></p>
<p>In their paper, Freudenstein et al. [30] develop a tool to partially automate the requirement based</p>
<p>testing process [30]. Their tool, Specmate, captures testing requirements and uses them to generate</p>
<p>test procedures or test-scripts for use in Allianz Deutschland. Their algorithm for creating test</p>
<p>procedures is a three-step process. Firstly, Cause-Effect-Graphs (CEGs) model logical statements and</p>
<p>their relationships. Secondly, test specifications are developed from the CEG outputs. Finally, the test</p>
<p>procedure is implemented from the test specifications. The authors tested Specmate in Alliance</p>
<p>Deutschland and found that it helped reduce the efforts in the creation of test-procedures but</p>
<p>ultimately did not save time as the system took too long to setup. Because the testing requirements</p>
<p>for the FIP – IXL interface will not change, an automated test case generator, as suggested by</p>
<p>Freudenstein et al. [30] is not necessary. Instead, the three step approach recommended by the</p>
<p>authors can be used manually to derive a single set of test cases in this project.</p>
<p>2.3.4 Summary of Literature Review</p>
<p>This review addressed the existing literature used to design a testing framework, evaluate testing tools</p>
<p>and develop test cases. Based on the findings of the literature review, the following decisions were</p>
<p>made for the development of the project:</p>
<ul>
<li>Developing an automated testing framework using the top-down method recommended by</li>
</ul>
<p>Méndez-Porras et al. [23],</p>
<ul>
<li>
<p>Using a two layer structure, as recommended by Borisov et al. [25], when developing a GUI,</p>
</li>
<li>
<p>Ensuring that the project complies with the CENELEC standards EN50128:2011 [18],</p>
</li>
<li>
<p>Continuing to use Squish for TCS testing due to the findings of by Jönsson [27],</p>
</li>
<li>
<p>Utilising Freudenstein et al.’s three-step method to designing test cases [30] and</p>
</li>
<li>
<p>Ensuring the test cases meet the criteria’s for good test cases as described by Adlemo et al.</p>
</li>
</ul>
<p>[29].</p>
<p>Page | 13</p>
<h2>3.0 P ROJECT D ESCRIPTION</h2>
<h5>3.1 A IMS A ND O BJECTIVES</h5>
<p>The overarching purpose of the project was to design and develop an integrated testing framework to</p>
<p>support the automated testing of the AutoHaul® project. The testing concentrated on expanding the</p>
<p>current testing framework to include the tests described in Table 5.</p>
<p><em>Table 5 Types of tests to implement autonomously</em></p>
<table>
<thead>
<tr>
<th>Testing Type Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Site-like behavior testing</strong></td>
<td>Testing with inputs that the AutoHaul® system received from the site.</td>
</tr>
<tr>
<td><strong>Load testing</strong></td>
<td>Testing the behavior of the AutoHaul® system under normal and<br>extreme loads.</td>
</tr>
<tr>
<td><strong>Capacity testing</strong></td>
<td>Testing to validate that the AutoHaul® system can handle the amount<br>of traffic that it was designed to handle.</td>
</tr>
<tr>
<td><strong>Automated testing of</strong><br><strong>new configurations</strong></td>
<td>Tests which can be used to see how the AutoHaul® system will behave<br>when new configurations are programmed into it.</td>
</tr>
</tbody>
</table>
<p>As such, the main goals were:</p>
<ul>
<li>
<p>Reviewing the existing testing methods at Hitachi,</p>
</li>
<li>
<p>Successfully developing a testing tool that can perform the tests detailed in Table 5 and</p>
</li>
<li>
<p>Creating documentation that allows the designed testing tool to be used easily.</p>
</li>
</ul>
<h5>3.2 S COPE</h5>
<p>The scope of the project focused on the capabilities of the testing tool. Items and tasks that were</p>
<p>considered within the scope were:</p>
<ul>
<li>Investigating and choosing tools to automate the testing of systems via testing four specific</li>
</ul>
<p>communication interfaces. The interfaces, and a brief description of their purpose are given</p>
<p>in Table 6.</p>
<ul>
<li>
<p>Developing a test engine with the chosen testing tools that can perform all required tests.</p>
</li>
<li>
<p>Providing the documentation necessary for the tests to be used at Hitachi.</p>
</li>
</ul>
<p>The tasks that were considered out of scope for this project were:</p>
<ul>
<li>Validating the payload of the interlocking data that is sent to the Fiber Interface Processor</li>
</ul>
<p>(FIP) from the Microlok Interlocking System (IXL).</p>
<ul>
<li>Testing the communication mediums (such as wireless or fibre optics) that are used in the</li>
</ul>
<p>communication interfaces.</p>
<p>The IXL – FIP interface was included in the scope as there was no testing tool used at Hitachi to test</p>
<p>behaviour of this interface. All previous testing was focused on validating IXL data at the TCS and</p>
<p>assuming that the FIP behaviour was correct. As such, testing the FIP’s behaviour using the FIP – IXL</p>
<p>interface was the project’s main priority.</p>
<p>Page | 14</p>
<p>The three TCS interfaces were included in the scope as the TCS is the main system in the AutoHaul®</p>
<p>project and any changes made to the system generally impact it. Therefore, this project tested the</p>
<p>TCS’s behaviour by using the ATOC, RES and VSS interfaces. Only these three interfaces have been</p>
<p>chosen instead of other TCS interfaces, such as the TCS – FIP interface, because the other interfaces</p>
<p>either have testing tools that have been specifically designed for them already or have no supporting</p>
<p>functions in the TCS Testing Suite. As such, automating the testing for those interfaces would take</p>
<p>more time than was available for this project.</p>
<p>More about these interfaces and how they fit into the AutoHaul® project are provided in Section 2.0.</p>
<p><em>Table 6 Interfaces covered in the scope</em></p>
<table>
<thead>
<tr>
<th>Interface Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Train Control Sub-</strong><br><strong>system (TCS) –</strong><br><strong>Automatic Train</strong><br><strong>Operation Controller</strong><br><strong>(ATOC)</strong></td>
<td> <br>The TCS is a part of the Control Centre and is responsible for<br>controlling most of the operations of the railway network.<br> <br>ATOC is a part of the Trainborne System that acts as the main control<br>system inside each train.<br> <br>The TCS – ATOC interface is responsible for transmitting messages<br>between these systems and is vital for the safe movements of trains<br>in the network.</td>
</tr>
<tr>
<td><strong>TCS – RTIO External</strong><br><strong>Systems (RES)</strong></td>
<td> <br>The RES is a server which acts as a gateway between the TCS and<br>other RTIO’s operations.<br> <br>The TCS – RES interface is responsible for transmitting messages<br>between these systems.</td>
</tr>
<tr>
<td><strong>TCS – Vital Safety</strong><br><strong>Servers (VSS)</strong></td>
<td> The VSS utilises data from the network to determine if and where it<br>is safe for trains to move.<br> The TCS – VSS interface is responsible transmitting messages<br>between these systems.</td>
</tr>
<tr>
<td><strong>Microlok Interlocking</strong><br><strong>System (IXL) – Fiber</strong><br><strong>Interface Processor</strong><br><strong>(FIP)</strong></td>
<td> Various IXLs are placed next to tracks and perform vital functions<br>such as route settings. The FIP is a device which polls all of the IXLs,<br>collates their responses and sends the data onto the TCS.<br> The FIP – IXL interface is responsible for transmitting messages<br>between these systems.</td>
</tr>
</tbody>
</table>
<p>Page | 15</p>
<h5>3.3 P ROJECT D ELIVERABLES</h5>
<p>The key deliverables for the project are outlined in Table 7.</p>
<p><em>Table 7 Key deliverables list</em></p>
<table>
<thead>
<tr>
<th>Deliverable Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1. Software Solution</strong></td>
<td>A software package that meets the goals detailed in Section 3.1.</td>
</tr>
<tr>
<td><strong>2. Architecture</strong><br><strong>Design</strong></td>
<td>The design and architecture of the software solution.</td>
</tr>
<tr>
<td><strong>3. Test Procedure</strong></td>
<td>The procedure used to validate the design of the software solution.</td>
</tr>
<tr>
<td><strong>4. User Manual</strong></td>
<td>A guide which steps the user through how to configure and run tests and<br>interpret the output.</td>
</tr>
<tr>
<td><strong>5. Reflective</strong><br><strong>Journals</strong></td>
<td>5 journals which reflect on key learnings relating to Engineers Australia<br>stage 1 competencies during the placement.</td>
</tr>
<tr>
<td><strong>6. Project Proposal</strong></td>
<td>A document which outlines the context, project plan and risk assessment<br>of the project.</td>
</tr>
<tr>
<td><strong>7. Interim Report</strong></td>
<td>A report which presents the project’s progress and includes a discussion<br>of the results obtained.</td>
</tr>
<tr>
<td><strong>8. Oral Presentation</strong></td>
<td>A presentation which summaries the findings and recommendations of<br>the project.</td>
</tr>
<tr>
<td><strong>9. Final Report</strong></td>
<td>A report which details the findings and recommendations of the project.</td>
</tr>
</tbody>
</table>
<h5>3.4 P ROJECT M ANAGEMENT</h5>
<p>The project was split into four stages. A description of each stage is provided in Table 8. More details</p>
<p>about the stages and the project’s timeline are provided in Appendix A: Project Management</p>
<p>Summary.</p>
<p><em>Table 8 Summary of project stages</em></p>
<table>
<thead>
<tr>
<th>Stage Description Time spent<br>on stage</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Research</strong></td>
<td>This stage was used to conduct research in order to<br>understand the full context of the tests being performed and<br>the interfaces being tested.</td>
<td>6 weeks</td>
</tr>
<tr>
<td><strong>Design</strong></td>
<td>In this stage, the testing tools that would be used for testing<br>were chosen. A test engine was also designed.</td>
<td>4 weeks</td>
</tr>
<tr>
<td><strong>Implementation</strong></td>
<td>During this stage, functionality was programmed so that the<br>test engine could perform the required testing.</td>
<td>11 weeks</td>
</tr>
<tr>
<td><strong>Documentation</strong></td>
<td>In this stage, user guides and test specifications were written<br>to document the work done on the project and ensure that<br>Hitachi could continue to access it after the completion of the<br>project.</td>
<td>6 weeks</td>
</tr>
</tbody>
</table>
<p>Page | 16</p>
<h2>4 D ESIGN</h2>
<p>This section describes the technical approach used to develop testing tools for the FIP – IXL interface</p>
<p>and the TCS interfaces. As the two testing tools were different, two separate design strategies were</p>
<p>applied and are described below.</p>
<h5>4.1 FIP T ESTER</h5>
<p>The aim of testing the FIP – IXL interface was to check the behaviour of the FIP by ensuring that its</p>
<p>responses to IXL messages are correct. Hence, it was decided that a FIP testing tool (FIP Tester) would</p>
<p>be developed. This tool would simulate IXL devices in a fashion similar to how aTest (Hitachi’s</p>
<p>proprietary ATOC simulation tool) operates in the TCS testing.</p>
<p>4.1.1 Test Framework Design</p>
<p>The design process used to design this testing tool was based on the top-down methodology</p>
<p>recommended by Méndez-Porras et al. [31] in their paper. As recommended by Méndez-Porras, the</p>
<p>system requirements were defined, then a high level system was designed.</p>
<p><strong>System Requirements</strong></p>
<p>The requirements for the FIP Tester are provided in Table 9.</p>
<p>Page | 17</p>
<p><em>Table 9 System requirements for the FIP Tester</em></p>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Implementation Requirements</strong></td>
<td><strong>Implementation Requirements</strong></td>
</tr>
<tr>
<td>**1 **</td>
<td>All IXL simulations shall connect to the FIP in the same way that IXL devices do.</td>
</tr>
<tr>
<td>**2 **</td>
<td>IXL packets shall use the Genisys protocol.<br>Instead of actual interlocking data, the IXL simulator shall send “0101”.</td>
</tr>
<tr>
<td>**3 **</td>
<td>No changes shall be made to the FIP’s software to use this testing tool.</td>
</tr>
<tr>
<td>**4 **</td>
<td>The FIP Tester shall integrate with Hitachi’s existing systems.</td>
</tr>
<tr>
<td><strong>Test Case Requirements</strong></td>
<td><strong>Test Case Requirements</strong></td>
</tr>
<tr>
<td>**5 **</td>
<td>In order to run capacity and load testing, the FIP Tester shall simulate multiple IXL devices at<br>the same time.</td>
</tr>
<tr>
<td>**6 **</td>
<td>The FIP Tester shall test the behavior of the FIP in response to site-like transmission errors.<br>These errors include:<br> <br>Partially received messages,<br> <br>Corrupted messages,<br> <br>Duplicated messages,<br> <br>Lost messages and<br> <br>Messages time-out.</td>
</tr>
<tr>
<td>**7 **</td>
<td>The FIP Tester shall test the behaviour of the FIP in response to a new IXL connecting to it<br>after it has already been running.</td>
</tr>
<tr>
<td><strong>Software Requirements</strong></td>
<td><strong>Software Requirements</strong></td>
</tr>
<tr>
<td>**8 **</td>
<td>The FIP Tester shall only use open source software libraries.</td>
</tr>
<tr>
<td><strong>Documentation Requirements</strong></td>
<td><strong>Documentation Requirements</strong></td>
</tr>
<tr>
<td>**9 **</td>
<td>A document outlining the architecture of the software solution and the test procedure used<br>to validate its behavior shall be provided.</td>
</tr>
<tr>
<td><strong>10</strong></td>
<td>A user guide which provides instructions shall be provided. The user guide shall provide<br>instructions on:<br> <br>Setting up the FIP Tester,<br> <br>Running tests and<br> <br>Making modifications to the system</td>
</tr>
<tr>
<td><strong>Output Requirements</strong></td>
<td><strong>Output Requirements</strong></td>
</tr>
<tr>
<td><strong>11</strong></td>
<td>The user shall be presented with a pass/fail report at the end of the tests. If a test fails, the<br>report should include the cause of failure.</td>
</tr>
<tr>
<td><strong>12</strong></td>
<td>The FIP Tester shall log the testing activities to make it easier to debug issues.</td>
</tr>
</tbody>
</table>
<p><strong>High Level Design</strong></p>
<p>As per Méndez-Porras et al. [31], the next step in designing the framework was to develop a high level</p>
<p>system overview and then breakdown the functions of each element in the framework. As</p>
<p>summarised in Figure 8, the designed behaviour of the FIP tester is as follows.</p>
<ol>
<li>A GUI allows the user to enter test parameters. Example parameters include specifying the</li>
</ol>
<p>number of IXLs to create.</p>
<ol>
<li>The test engine runs the tests to check the functionality of the FIP. This test engine should</li>
</ol>
<p>be able to perform the following tasks.</p>
<p>a. Log all testing activities.</p>
<p>b. Initialise IXL simulators and connect them to the FIP based on user specified IP</p>
<p>addresses and port numbers.</p>
<p>c. Run tests to check that all the IXLs are connected and that communications are</p>
<p>following the polling cycle described in Table 3.</p>
<p>Page | 18</p>
<p>d. Test how the FIP behaves in each of the test cases.</p>
<p>e. Shut down the test environment to prevent it from affecting future tests.</p>
<ol>
<li>A pass/fail report is generated to inform the user about the results of the tests. In case a test</li>
</ol>
<p>fails, the report will include a reason for the failure.</p>
<p><em>Figure 8 High level overview of FIP Tester tool</em></p>
<p>Key decisions made when designing this framework include:</p>
<ul>
<li>
<p>Using Python as the programming language because of the availability of multiple open
source libraries that will be useful to the project,</p>
</li>
<li>
<p>Implementing the program in a Linux VM to increase adoptability in Hitachi.</p>
</li>
</ul>
<p>4.1.2 Test Case Design</p>
<p>There were three test cases for which tests were designed. These were:</p>
<p>1) Testing the initial start-up sequence,</p>
<p>2) Testing with site-like conditions and</p>
<p>3) Testing how new configurations are handled.</p>
<p>The generic process used for tests (1) and (2) are shown in Figure 9. Detailed workflow diagrams for</p>
<p>each test are provided in Appendix E: FIP Tester Workflow Diagrams. For testing site-like conditions,</p>
<p>the 6 types of communication errors that were simulated and their expected response are:</p>
<ul>
<li>
<p>25% packet loss,</p>
</li>
<li>
<p>50% packet loss,</p>
</li>
<li>
<p>75% packet loss,</p>
</li>
<li>
<p>Duplicate packets being sent,</p>
</li>
<li>
<p>Corrupted packets being sent and</p>
</li>
<li>
<p>100% packet loss.</p>
</li>
</ul>
<p>In all of these cases, the FIP is expected to disregard the response that is received from the IXL and</p>
<p>resend the request for the response.</p>
<p>Page | 19</p>
<p><em>Figure 9 Generic test case workflow diagram</em></p>
<p>The generic process used for test (3) is provided in Figure 10. For each of the three tests, a test</p>
<p>specification was created and is detailed in Table 10 to Table 12.</p>
<p><em>Figure 10 Flowchart of process to test new configurations management – See Figure 32 for the start-up sequence testing</em></p>
<p>Page | 20</p>
<p><strong>FIP Test 1: Testing the initial start-up sequence</strong></p>
<p>Description: Ensures that the FIP sends a recall, control, and poll message as per its polling cycle.</p>
<p>Pre-test actions:</p>
<p>1) Restart the FIP.</p>
<p>2) Ensure the VM hosting the FIP Tester has the correct networking settings.</p>
<p><em>Table 10 Test specifications for FIP test 1: Testing the initial start-up sequence</em></p>
<table>
<thead>
<tr>
<th>Step Description Expected Result<br>No</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>FIP Tester begins running</td>
<td>A GUI opens that asks the user to enter<br>test configurations.</td>
</tr>
<tr>
<td>**2 **</td>
<td>A recall message is received at each valid IXL</td>
<td>The FIP Tester logs show the received<br>message and display a message that shows<br>that the recall test is passed.</td>
</tr>
<tr>
<td>**3 **</td>
<td>An indication message is sent by each IXL.<br>The FIP should receive this message and send<br>a control message back to the IXL</td>
<td>The FIP Tester logs show the received<br>message and display a message that shows<br>that the control test is passed.</td>
</tr>
<tr>
<td>**4 **</td>
<td>A slave acknowledge message is sent by each<br>IXL. The FIP should receive this message and<br>send a poll message back to the IXL</td>
<td>The FIP Tester logs show the received<br>message and display a message that shows<br>that the poll test is passed.</td>
</tr>
<tr>
<td>**5 **</td>
<td>An indication message is sent by each IXL.<br>The FIP should receive this message and send<br>a recall message back to the IXL</td>
<td>The FIP Tester logs show the received<br>message and display a message that shows<br>that the test is complete.<br>Test ends.</td>
</tr>
</tbody>
</table>
<p><strong>FIP Test 2: Testing with site-like conditions</strong></p>
<p>Description: Ensures that the FIP can respond appropriately when there are communications errors.</p>
<p>Pre-test actions:</p>
<p>1) Restart the FIP.</p>
<p>2) Ensure the VM hosting the FIP Tester has the correct networking settings.</p>
<p>3)</p>
<p><em>Table 11 Test specifications for FIP test 2: Testing with site-like conditions</em></p>
<table>
<thead>
<tr>
<th>Step Description Expected Result<br>No</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>FIP Tester begins running</td>
<td>A GUI opens that asks the user to enter test<br>configurations.</td>
</tr>
<tr>
<td>**2 **</td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message<br>and display a message saying the first recall<br>message has been received.</td>
</tr>
<tr>
<td>**3 **</td>
<td>An indication message is created and<br>25% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.</td>
<td>The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 25% packet loss recall test has passed.</td>
</tr>
<tr>
<td>**4 **</td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.</td>
</tr>
</tbody>
</table>
<p>Page | 21</p>
<table>
<thead>
<tr>
<th>5</th>
<th>An indication message is created and<br>50% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.</th>
<th>The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 50% packet loss recall test has passed.</th>
</tr>
</thead>
<tbody>
<tr>
<td>**6 **</td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.</td>
</tr>
<tr>
<td>**7 **</td>
<td>An indication message is created and<br>75% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.</td>
<td>The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 75% packet loss recall test has passed.</td>
</tr>
<tr>
<td>**8 **</td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.</td>
</tr>
<tr>
<td>**9 **</td>
<td>An indication message is created. It is<br>duplicated and sent by each IXL to the<br>FIP. The FIP should receive this message,<br>realise that it is invalid and resend the<br>recall message.</td>
<td>The FIP Tester logs show the recall message<br>arriving again received message and display a<br>message that shows that the duplicate packet<br>recall test has passed.</td>
</tr>
<tr>
<td><strong>10</strong></td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.</td>
</tr>
<tr>
<td><strong>11</strong></td>
<td>An indication message is created. Its CRC<br>is changed to “0000” and it sent by each<br>IXL to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.</td>
<td>The FIP Tester logs show the recall message<br>arriving again received message and display a<br>message that shows that the corrupted packet<br>recall test has passed.</td>
</tr>
<tr>
<td><strong>12</strong></td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.</td>
</tr>
<tr>
<td><strong>13</strong></td>
<td>No reply is sent</td>
<td>The FIP will timeout while waiting for a reply<br>and resent a recall message.</td>
</tr>
<tr>
<td><strong>14</strong></td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message.<br> <br>The display will show that all control tests have<br>passed.</td>
</tr>
<tr>
<td><strong>15</strong></td>
<td>Steps 2 – 14 are repeated except a<br>control message is received instead of a<br>recall message.</td>
<td>The FIP Tester logs show the received message<br>and displays a message saying that part of the<br>testing is complete.<br> <br>The display will show that all control tests have<br>passed.</td>
</tr>
<tr>
<td><strong>16</strong></td>
<td>Steps 2 – 14 are repeated except a poll<br>message is received instead of a recall<br>message.</td>
<td>The FIP Tester logs show the received message<br>and display a message.<br> <br>The display will show that all poll tests have<br>passed.</td>
</tr>
<tr>
<td><strong>17</strong></td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The FIP Tester logs show the received message<br>and displays a message that shows that the test<br>is complete.<br> <br>Test ends.</td>
</tr>
</tbody>
</table>
<p>Page | 22</p>
<p><strong>FIP Test 3: Testing how new configurations are handled</strong></p>
<p>Description: Ensures that the FIP behaves correctly when a new IXL is initialised after the FIP is already</p>
<p>running.</p>
<p>Pre-test actions:</p>
<p>1) Restart the FIP.</p>
<p>2) Ensure the VM hosting the FIP Tester has the correct networking settings.</p>
<p>3) Run FIP Test 1 with 5 IXLs initialised.</p>
<p><em>Table 12 Test specifications for FIP test 3: Testing how new configurations are handled</em></p>
<table>
<thead>
<tr>
<th>Step No Description Expected Result</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>After FIP Test 1 has completed, allow<br>time for 10 recall messages to be<br>received at each IXL.</td>
<td>The logs show that FIP Test 1 has completed<br>and 10 recall messages have been sent and<br>replied to appropriately.</td>
</tr>
<tr>
<td>**2 **</td>
<td>A new IXL is initialised.</td>
<td>The start-up testing is repeated for all of the<br>connected IXLs.</td>
</tr>
<tr>
<td>**3 **</td>
<td>A recall message is received at each<br>valid IXL.</td>
<td>The logs show the received message and<br>display a message that shows that the test is<br>complete. Test ends.</td>
</tr>
</tbody>
</table>
<p>4.1.3 GUI Design</p>
<p>In order to make the system easier to use, a GUI was designed using a two layer structure as</p>
<p>recommended by Borisov et al. [25]. A sketch of the design is shown in Figure 11.</p>
<p><em>Figure 11 GUI design sketch</em></p>
<p>Key design decisions made based on the two-layer GUI design strategy [25] were:</p>
<ul>
<li>
<p>Having a simple layout,</p>
</li>
<li>
<p>Incorporating a “Help” button which opens the user guide,</p>
</li>
<li>
<p>Including a pop-up dialogue box which asks the user to confirm all the settings,</p>
</li>
<li>
<p>Allowing the user to control the complexity and type of testing that would run and</p>
</li>
<li>
<p>Mimicking the configurations requirements (Hosts file and configurations file) to the FIP’s</p>
</li>
</ul>
<p>configuration requirements in order to minimise tester effort.</p>
<p>Page | 23</p>
<p>4.1.4 Performance Assessment Design</p>
<p>Table 13 describes the tests that were performed to ensure that the FIP Tester was working as</p>
<p>expected.</p>
<p><em>Table 13 Tests to verify the FIP Tester’s behaviour</em></p>
<table>
<thead>
<tr>
<th>Test Test Method Description Purpose<br>Number</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>‘netcat’ and<br>‘tcpdump’ at<br>the FIP</td>
<td>The Linux commands ‘netcat’ was used to view<br>the incoming and outgoing traffic on a<br>particular IP and port in the FIP and the FIP<br>Tester.<br> <br>Additionally, the command ‘tcpdump’ was used<br>to ensure that all incoming and outgoing packet<br>contents were correct</td>
<td>Ensured that the<br>networking setup is<br>accurate.</td>
</tr>
<tr>
<td>**2 **</td>
<td>FIP Simulator</td>
<td>Created a UDP client that connects to the FIP<br>Tester on a particular IP address and port that<br>would<br>otherwise<br>have<br>been<br>used<br>to<br>communicate with the FIP.<br> <br>A user then created messages in the Python<br>terminal and sent them to the FIP Tester as if<br>the FIP was sending them. For the test cases<br>outlined in Table 10 to Table 12, the FIP’s<br>expected replies were sent via the FIP<br>Simulator.<br> <br>In addition to those test cases, the user also<br>introduced errors to the system and ensured<br>that the FIP Tester responds correctly to those<br>errors.</td>
<td>Ensured that the FIP<br>Tester’s behavior is<br>accurate.</td>
</tr>
<tr>
<td>**3 **</td>
<td>Manually<br>check FIP<br>Tester Logs</td>
<td>All of the FIP Tester’s activities during tests were<br>logged in the log files.<br> <br>A user manually checked the log files and<br>ensure that the correct sequence of events, as<br>defined by test cases shown in Table 10 to Table<br>12, are correct.</td>
<td>Ensured that the<br>tests were<br>repeatable and<br>correct.<br> <br>It was also used to<br>load test the FIP<br>Tester.</td>
</tr>
</tbody>
</table>
<p>4.1.5 Documentation</p>
<p>The documentation provided for the FIP – IXL Interface Testing Tool was based on the documentation</p>
<p>required by the CENELEC standard EN 50128:2011 [18]. It includes a user guide which provides details</p>
<p>for initiating and running tests and a set of test specifications.</p>
<p>Page | 24</p>
<h5>4.2 TCS T ESTING S UITE</h5>
<p>In order to achieve this project’s goals, it was decided that the work done on the existing TCS Testing</p>
<p>Suite would be built upon in the project. In terms of testing the TCS interfaces, the TCS Testing Suite</p>
<p>had two main limitations.</p>
<p>These were:</p>
<ul>
<li>The existing tests and helper functions did not have the capabilities necessary to run complex</li>
</ul>
<p>tests.</p>
<ul>
<li>The entire Testing Suite was complex and not user friendly. Therefore, it was difficult for a</li>
</ul>
<p>user to traverse through log files and identify a cause of failure.</p>
<p>Therefore, the enhancements made to the TCS Testing Suite were focused on these two limitations.</p>
<p><em>Figure 12 High level TCS Testing Suite test engine design</em></p>
<p>Figure 12 shows a high level design of the existing test engine used in the TCS Testing Suite.</p>
<p>Originally, the testing tool design involved modifying the test engine so that it uses a hybrid framework</p>
<p>as recommended by Wang et al. [24]. This test tool would iterate through log files to automatically</p>
<p>generate test cases and then run the tests using the Test Engine shown in Figure 12.</p>
<p>This test engine would have combined a Log Parser with the test engine, such that the overall testing</p>
<p>tool could automatically identify the causes of error and then execute the relevant tests from the TCS</p>
<p>Testing Suite. It would also have replaced Squish with an open-source GUI automation testing tool to</p>
<p>save future licencing costs. However, this design decision was rejected for the following reasons.</p>
<ul>
<li>
<p>Developing the tool would require more time than was available in the project,</p>
</li>
<li>
<p>Consultation with engineers in the testing team revealed that the TCS Testing Suite would be</p>
</li>
</ul>
<p>easier to maintain if the log parser and test engine were separate,</p>
<ul>
<li>The functionality required from the GUI automation testing tool was already implemented</li>
</ul>
<p>with Squish and</p>
<ul>
<li>Background research by Jönsson found that Squish is an effective tool for the required</li>
</ul>
<p>functionality [27].</p>
<p>Instead of modifying the test engine it was decided that all work done to enhance the TCS Testing</p>
<p>Suite would remain compatible with the existing test engine. Additionally, a Log Parser was designed</p>
<p>to reduce user effort when debugging tests. This section outlines the design process for both these</p>
<p>activities.</p>
<p>Page | 25</p>
<p>4.2.1 Increasing Testing Capabilities</p>
<p>To increase the testing capabilities, two main activities were conducted. These were to increase the</p>
<p>capabilities of the helper functions and existing tests and to add more tests.</p>
<p><strong>Increasing Capabilities of Helper Function and Existing Tests</strong></p>
<p>The TCS Testing Suite contains a library of helper functions and configurations that could be called</p>
<p>during tests. But this library was only set-up to allow testing with certain configurations. For example,</p>
<p>only a train with a specific Train ID could be created during tests because the configurations file and</p>
<p>helper functions were hard coded to implement functionality for that Train ID. Therefore, the</p>
<p>functions and tests were modified so that they could refer to a larger amount of trains.</p>
<p>As an example, Figure 14 shows a screenshot of the existing code used to register a train. This code</p>
<p>would generally be called at the start of each test to create and register a train in the system. It works</p>
<p>by:</p>
<ol>
<li>Constructing the train sheet and train steps messages required to register a train. The helper</li>
</ol>
<p>functions used to construct this message contain a list of parameters and corresponding values. If</p>
<p>a user provides a value for a parameter, then the default value is overwritten by the user</p>
<p>generated value.</p>
<ol>
<li>Creating and activating the train by using the “CreateAndActivate()” helper function. This function</li>
</ol>
<p>sends the train sheet and train steps message and then checks the Automation Server logs to</p>
<p>ensure that the TCS has been provided with updated information. For example, it will ensure that</p>
<p>a message has arrived that says that the train is active.</p>
<ol>
<li>Initialising the ATOC for that train.</li>
</ol>
<p><em>Figure 13 Code showing a train being created before the enhancements</em></p>
<p>Page | 26</p>
<p>In order to enhance this code so that it can be used to create multiple locomotives, or different</p>
<p>locomotives, the following actions were taken.</p>
<ul>
<li>
<p>Modifying the helper functions to remove references to specific configurations and</p>
</li>
<li>
<p>Creating a new helper function which can be called during tests.</p>
</li>
</ul>
<p><strong>Adding More Tests</strong></p>
<p>Several tests from the TCS Integration Test Specifications document [19] had not been included in the</p>
<p>Testing Suite because more functionality needed to be added to the Testing Suite’s helper functions</p>
<p>before they could be run.</p>
<p>Therefore, as a part of increasing the testing capabilities, several tests relevant to the TCS – ATOC, TCS</p>
<p>– VSS and TCS – RES interfaces were added to the system.</p>
<p>4.2.2 Log Parser</p>
<p><em>Figure 14 High level log parser design</em></p>
<p>The Log Parser was designed to help users identify errors in the system. It was developed in Python so</p>
<p>that it can be included in the TCS Testing Suite’s existing libraries package. As outlined in Figure 14, it</p>
<p>does this by:</p>
<ol>
<li>Taking in a file which contains the address of all relevant log files. The log files may be from</li>
</ol>
<p>the actual AutoHaul® system or the testing system.</p>
<ol>
<li>
<p>Collating the critical, fatal and connection errors present in each of the log files.</p>
</li>
<li>
<p>Outputting this information to the user in a compiled list.</p>
</li>
</ol>
<p>Users can then use this information to identify sections of the source code which must be changed to</p>
<p>fix the errors.</p>
<p>4.2.3 Performance Assessment</p>
<p><strong>Testing Enhancements</strong></p>
<p>The testing conducted for TCS interfaces must follow the guidelines set in the TCS Test Specifications</p>
<p>document [19]. Therefore, to verify that the enhancements made to the TCS system were working</p>
<p>properly, the implemented tests were run. The user was able to visually verify that all of the steps and</p>
<p>the expected outcomes of those steps were being performed.</p>
<p>Page | 27</p>
<p><strong>Log Parser</strong></p>
<p>Because the Log Parser only collates the errors already present in log files to save the user time while</p>
<p>debugging, the aim of the performance assessment was to ensure that it did not miss any errors and</p>
<p>that it was actually useful to the testers at Hitachi.</p>
<p>To perform this assessment, a simple test to check whether a train had been properly registered in</p>
<p>the CTC was run multiple times. Each time the test was run, a different error was introduced to the</p>
<p>system. A user then manually verified that all of the expected critical, fatal and connection errors</p>
<p>produced by the system in response to these errors were present in the log files. The errors introduced</p>
<p>to the system were:</p>
<ul>
<li>
<p>Disabling TCS communications by disconnecting the Automation Server,</p>
</li>
<li>
<p>Attempting to create a train with invalid parameters and</p>
</li>
<li>
<p>Commenting out configurations required to register a train.</p>
</li>
</ul>
<p>These errors were chosen because they resulted in multiple error messages appearing in multiple log</p>
<p>files as described in Section 0. They were also chosen because the changes made to the system could</p>
<p>be rectified easily and would not break the functionality of the overall TCS Testing Suite.</p>
<p>Additionally, a user who regularly uses the TCS Testing Suite for testing was given the compiled error</p>
<p>list from the three scenarios listed above. The user was then asked to identify the cause of failure</p>
<p>based on the error messages.</p>
<p>4.2.4 Documentation</p>
<p>Hitachi engineers are already familiar with the TCS Testing Suite and they have already undergone a</p>
<p>vigorous process to ensure that the tests in the TCS Integration Test Specifications document [19]</p>
<p>comply with CENELEC standards [18]. Therefore, the documentation provided for the TCS Testing Suite</p>
<p>was focused on ensuring that engineers know what functionality has been added to the TCS Testing</p>
<p>Suite and how they can use the Log Analyser.</p>
<p>Page | 28</p>
<h2>5 P ROJECT O UTCOMES</h2>
<h5>5.1 FIP T ESTER</h5>
<p>5.1.1 The Testing Tool</p>
<p>The designed FIP Tester has been implemented with the architecture shown in Figure 15. The</p>
<p>architecture was modelled after the TCS Testing Suite’s architecture, which is provided in</p>
<p>Appendix G: TCS Testing Suite Architecture. It was intentionally made to be simple as this allowed it</p>
<p>to be easy to navigate and maintain for users who are not familiar with the Python programming.</p>
<p><em>Figure 15 System architecture of FIP Tester</em></p>
<p>As illustrated in Figure 16 the FIP Tester is primarily controlled by a Test Controller. This controller is</p>
<p>responsible for:</p>
<ul>
<li>
<p>Initialising the GUI to get test configurations from the user,</p>
</li>
<li>
<p>Initialising the IXL simulators which connect to the FIP,</p>
</li>
<li>
<p>Performing all logging functions,</p>
</li>
<li>
<p>Running and ending tests and</p>
</li>
<li>
<p>Displaying the test outcomes to the user.</p>
</li>
</ul>
<p>In order to increase easy of maintenance and accessibility, the FIP Tester has been hosted on a VM</p>
<p>and uses the same networking set-up that exists between the SoCat and the FIP that is shown in Figure</p>
<ol>
<li>Therefore, the FIP’s software does not need to be changed in order to use the FIP Tester.</li>
</ol>
<p><em>Figure 16 High level overview of designed FIP Tester</em></p>
<p>Page | 29</p>
<p>5.1.2 The GUI</p>
<p>The GUI implemented to take user inputs and set test configurations is shown in Figure 17. The GUI</p>
<p>requires the user to enter the number of IXLs that the simulator will create, a text file containing the</p>
<p>IP addresses, a text file containing the port numbers and the tests which they want to run.</p>
<p><em>Figure 17 GUI for the FIP Tester</em></p>
<p>To reduce errors in interactions with users, a confirmation pop-up dialogue is created once the user</p>
<p>selects “Begin Test” This pop-up is shown in Figure 18.</p>
<p><em>Figure 18 Confirmation pop-up for the FIP Tester</em></p>
<p>5.1.3 Test Cases</p>
<p>Code was written to run through the three tests outlined in in Table 10, Table 11 and Table 12. A code</p>
<p>snippet from the start-up testing procedure is provided in Appendix F: FIP Tester Tests.</p>
<p>The main limitation of the implemented code is that because it was written to be easy to use and</p>
<p>maintain for users not familiar with Python programming, it is not memory efficient.</p>
<p>5.1.4 Performance Assessment</p>
<p><strong>Outcomes for Test 1 (Using the Linux commands ‘tcpdump’ and ‘netcat’ in the FIP)</strong></p>
<p>The Linux command ‘tcpdump’ was used to verify that the packet contents were correct. The output</p>
<p>from one of the connections is shown in Figure 19.</p>
<p>Page | 30</p>
<p><em>Figure 19 'tcpdump' output</em></p>
<p>One of the key problems solved by using ‘tcpdump’ was that the FIP logs and documentation implied</p>
<p>that individual elements of messages were surrounded with square brackets. For example, an</p>
<p>acknowledge message would be ”[f1][29][f6]”. However, using ‘tcpdump’ made it clear that this was</p>
<p>not the case and the message was actually “f129f6”.</p>
<p>Furthermore, the Linux command ‘netcat’ was used to verify that the messages were arriving at and</p>
<p>leaving from the correct IP address and port number. The output from one of these connections is</p>
<p>shown in Figure 20. This figure shows the initial recall message (“fd 2a c1 4f f6”) being sent from the</p>
<p>FIP to the IXL. Because no message is being sent back to the FIP, the recall message is being sent</p>
<p>repeatedly.</p>
<p><em>Figure 20 Linux command ‘netcat’ output</em></p>
<p>Using ‘netcat’ was highly effective in ensuring that the initial networking set-up was correct. A major</p>
<p>problem encountered early in the project was that messages would be seen arriving correctly by</p>
<p>‘tcpdump’ but would not register as having arrived in the FIP software and therefore not show up in</p>
<p>the FIP’s logs. Using ‘netcat’ it was found that the firewalls were preventing the data from actually</p>
<p>“reaching” its destination. The firewall rules were subsequently modified to allow the data to be</p>
<p>transmitted correctly.</p>
<p><strong>Outcomes for Test 2 (Using a FIP Simulator)</strong></p>
<p>A FIP Simulator was used to manually send messages that met the expected behaviour of the test</p>
<p>cases outlined in Table 10, Table 11 and Table 12. Figure 21 shows the replies sent by the FIP Tester</p>
<p>when all of the correct messages are sent from the FIP Simulator in the start-up testing sequence. The</p>
<p>logs in the FIP Tester look exactly like Figure 24.</p>
<p>Page | 31</p>
<p><em>Figure 21 Python terminal output of test case 1 (Start-up sequence) with correct replies as seen by the FIP Simulator</em></p>
<p>Figure 22 shows the replies from the FIP Tester when an incorrect message is sent from the FIP</p>
<p>Simulator. As seen in the Python terminal output, the reply sent by the FIP Simulator after receiving a</p>
<p>control message is incorrect. Therefore, the FIP Tester resends the control message. The</p>
<p>corresponding log file in the FIP Tester is shown in Figure 23.</p>
<p><em>Figure 22 Python terminal output of test case 1 (Start-up sequence) with incorrect replies as seen by the FIP Simulator</em></p>
<p><em>Figure 23 FIP Tester log file when incorrect message is received</em></p>
<p>Please note that for confidentiality purposes, the IP address and port number that are present in the</p>
<p>log file have been overwritten by the phrase “IP:Port” which is highlighted in blue. This also applies to</p>
<p>other figures in this report.</p>
<p><strong>Outcomes for Test 3 (Manually checking FIP Tester Logs)</strong></p>
<p>The logs for the FIP Tester were checked manually to ensure that the system’s behaviour matched the</p>
<p>expected behaviour that is outlined in Table 10, Table 11 and Table 12. An example of a passing test</p>
<p>is shown in Figure 24. This is a test to verify the start-up message sequence. The corresponding results</p>
<p>table is shown in Table 14.</p>
<p>Page | 32</p>
<p><em>Figure 24 FIP Tester log showing a passing test</em></p>
<p><em>Table 14 Results from manually checking the logs in Figure 24</em></p>
<table>
<thead>
<tr>
<th>Step Description Expected Result Pass/<br>No Fail</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>FIP Tester begins running.</td>
<td>A GUI opens that asks the user to<br>enter test configurations.</td>
<td>NA</td>
</tr>
<tr>
<td>**2 **</td>
<td>A recall message is received at each valid<br>IXL.</td>
<td>The logs show the received message<br>and display a message that shows<br>that the recall test is passed.</td>
<td>Pass</td>
</tr>
<tr>
<td>**3 **</td>
<td>An indication message is sent by each IXL.<br>The FIP should receive this message and<br>send a control message back to each IXL.</td>
<td>The logs show the received message<br>and display a message that shows<br>that the control test is passed.</td>
<td>Pass</td>
</tr>
<tr>
<td>**4 **</td>
<td>A slave acknowledge message is sent by<br>each IXL. The FIP should receive this<br>message and send a poll message back to<br>the IXL.</td>
<td>The logs show the received message<br>and display a message that shows<br>that the poll test is passed.</td>
<td>Pass</td>
</tr>
<tr>
<td>**5 **</td>
<td>An indication message is sent by each IXL.<br>The FIP should receive this message and<br>send a recall message back to each IXL.</td>
<td>The logs show the received message<br>and display a message that shows<br>that the test is complete.<br> <br>Test ends.</td>
<td>Pass</td>
</tr>
<tr>
<td><strong>Overall Outcome: Pass</strong></td>
<td><strong>Overall Outcome: Pass</strong></td>
<td><strong>Overall Outcome: Pass</strong></td>
<td><strong>Overall Outcome: Pass</strong></td>
</tr>
</tbody>
</table>
<p>Similarly, Figure 25 and Table 15 show a failing test. In this test a connection to the IP address and port</p>
<p>cannot be established and therefore no recall message is received by FIP Tester.</p>
<p><em>Figure 25 FIP Tester log showing a failing test</em></p>
<p>Page | 33</p>
<p><em>Table 15 Results from manually checking the logs In Figure 25</em></p>
<table>
<thead>
<tr>
<th>Step Description Expected Result Pass/<br>No Fail</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
</tr>
</thead>
<tbody>
<tr>
<td>**1 **</td>
<td>FIP Tester begins running</td>
<td>A GUI opens that asks the user to enter test<br>configurations.</td>
<td>NA</td>
</tr>
<tr>
<td>**2 **</td>
<td>A recall message is received<br>at each valid IXL.</td>
<td>The logs show the received message and display<br>a message that shows that the recall test is<br>passed.</td>
<td>Fail</td>
</tr>
<tr>
<td><strong>Overall Outcome: Fail because connection with FIP could not be established</strong></td>
<td><strong>Overall Outcome: Fail because connection with FIP could not be established</strong></td>
<td><strong>Overall Outcome: Fail because connection with FIP could not be established</strong></td>
<td><strong>Overall Outcome: Fail because connection with FIP could not be established</strong></td>
</tr>
</tbody>
</table>
<p>5.1.5 Documentation</p>
<p><em>Figure 26 Screenshot from user guide</em></p>
<p>Figure 26 shows a screenshot from the user guide. The user guide provides instructions for:</p>
<ul>
<li>
<p>Installing the FIP Tester,</p>
</li>
<li>
<p>Setting up the networking between the FIP Tester and the FIP and</p>
</li>
<li>
<p>Making modifications to the tests.</p>
</li>
</ul>
<h5>5.2 TCS T ESTING S UITE</h5>
<p>5.2.1 The Enhanced TCS Testing Suite</p>
<p>As described in Section 4.2.1, two main activities were conducted to enhance the TCS Testing Suite.</p>
<p>These were to increase the capabilities of the helper functions and existing tests and to implement</p>
<p>more tests from the TCS Test Specifications document [19]. This section describes the product from</p>
<p>performing these activities.</p>
<p>Page | 34</p>
<p><strong>Increasing the Capabilities of Helper Functions and Tests</strong></p>
<p>A number of changes were made to helper functions and tests in order to increase their capabilities</p>
<p>and make them more useful for testing. These changes included:</p>
<ul>
<li>Modifying all helper functions which were hardcoded to be specific to one Train ID to be</li>
</ul>
<p>compatible with multiple Train IDs and</p>
<ul>
<li>Modifying tests to run with any train configurations.</li>
</ul>
<p>As an example of these modifications, Figure 27 shows the enhanced version of code written during</p>
<p>the project to create and register multiple trains when given an initial Train ID and the number of</p>
<p>trains to create. It is based on the code shown in Figure 13. The changes made to the original code</p>
<p>were:</p>
<ul>
<li>Moving the train creation functionality into a helper function. This allows users to modify</li>
</ul>
<p>parameters easily.</p>
<ul>
<li>Modifying the default parameters in the “BuildTrainSheetMessage()” and</li>
</ul>
<p>“BuildTrainStepsMessage()” so that factors which limit the train configurations to one train</p>
<p>(such as train ID or train location) are no longer limiting.</p>
<ul>
<li>Creating ATOC helper functions, so that ATOC messages with non-default configurations can</li>
</ul>
<p>be sent.</p>
<p>Figure 27 has been annotated to show examples of these modifications.</p>
<p><em>Figure 27 Code showing the helper function implemented to create multiple trains</em></p>
<p>Page | 35</p>
<p>5.2.2 The Log Parser</p>
<p>In the TCS Testing Suite, each AutoHaul® sub-system has a folder which contains its log files. Each time</p>
<p>a test is run, the test runner migrates the most recent log to an archives folder and then creates a new</p>
<p>log file for the sub-system.</p>
<p>As its input, the Log Parser takes a file containing the addresses of the folders which contain these</p>
<p>logs. Each time the Log Parser is run, it iterates through the most recent log file for each sub-system</p>
<p>and finds the critical, fatal and connections errors present in the file.</p>
<p>The Log Parser then collates the error messages into a single file. The errors are sorted by their file of</p>
<p>origin and then by their time stamp. An example output is shown in Figure 28.</p>
<p><em>Figure 28 Screenshot of the collated errors file</em></p>
<p>To save memory, the compiled errors file gets overwritten each time the Log Parser is run. The user</p>
<p>has the option of modifying a parameter to save the previous version of the file if required.</p>
<p>A GUI was not designed for the Log Parser as the code is intended to be used by Hitachi testers who</p>
<p>are already familiar with the TCS Testing Suite and are proficient with Python coding. Hence, adding a</p>
<p>GUI would increase the time needed to use the Log Parser and therefore slow the testing efficiency.</p>
<p>A major limitation of the Log Parser is that it only searches for three types of errors. However, a lot of</p>
<p>errors, particularly those seen during deployment at site, are not registered as errors by the AutoHaul®</p>
<p>system and will not show up in the logs. Therefore, this tool is only useful as a quick debugging aid to</p>
<p>narrow down the causes of error.</p>
<p>5.2.3 Performance Assessment</p>
<p><strong>TCS Testing Enhancements</strong></p>
<p>To verify that the tests written to enhance the testing capabilities of the TCS Testing Suite were</p>
<p>working correctly, the user ran tests and visually observed the screen to ensure that the expected</p>
<p>events were occurring as per the TCS Integration Test Specification document [19].</p>
<p>Where events occurred in the background and could not be visually verified, print statements were</p>
<p>used to print the status of elements that needed to be checked. Figure 29 shows an example of a test</p>
<p>that was verified using this method. Testing outcomes and notes have been written in the pass/fail</p>
<p>column.</p>
<p>Page | 36</p>
<p><em>Figure 29 Example of a test case used to verify test behaviour with pass/fail comments</em></p>
<p>TCS – RES Interface</p>
<p>Due to uncontrollable factors, the server hosting the RES was disabled in May 2020. Therefore, the</p>
<p>TCS Testing Suite was not able to communicate with the RES and the code implemented was not able</p>
<p>to be verified. Once RES functionalities resume, Hitachi engineers will be able to test the code written</p>
<p>in this project.</p>
<p><strong>Log Parser</strong></p>
<p>To test the log parser, errors were intentionally introduced to the system and the combined log file</p>
<p>was checked to ensure that all the errors appeared in it.</p>
<p>A tester at Hitachi, who had written several of the tests in the TCS Testing Suite, was asked to identify</p>
<p>the cause of error based on the compiled error reports. Table 16 shows the results of this testing.</p>
<p>Page | 37</p>
<p><em>Table 16 Results of asking a tester to debug using the Log Parser</em></p>
<table>
<thead>
<tr>
<th>Test Type Was The Error Tester Comments<br>Identified?</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Disconnected the</strong><br><strong>Automation Server</strong></td>
<td>Yes</td>
<td>Other connection errors, such as time synchronization<br>issues between 2 sub-systems or incorrect port<br>configurations also produce the same error message as a<br>disconnected system in the logs.<br> <br>As such, the tester identified a disconnected Automation<br>Server as the most likely cause of the error but indicated<br>that they would have to do more testing to rule out<br>other potential causes.</td>
</tr>
<tr>
<td><strong>Invalid train</strong><br><strong>parameters</strong></td>
<td>Yes</td>
<td></td>
</tr>
<tr>
<td><strong>Removing steps</strong><br><strong>required to create</strong><br><strong>a train</strong></td>
<td>Yes</td>
<td>Identifying the error required the tester to go through<br>the code.</td>
</tr>
</tbody>
</table>
<p>Page | 38</p>
<h2>6 C ONCLUSION A ND R ECOMMENDATIONS</h2>
<p>In conclusion, this project designed and developed an integrated testing framework for automating</p>
<p>the testing of four communication interfaces used within the AutoHaul® project. The conclusions of</p>
<p>the project are outlined below.</p>
<h5>6.1 FIP T ESTER</h5>
<p>A testing tool called the FIP Tester was designed to simulate IXL and send IXL messages to the FIP in</p>
<p>order to test that the FIP behaves correctly when responding to these IXL messages. This testing tool</p>
<p>was designed to replace the manual testing done when an end-to-end change, such as adding or</p>
<p>removing an IXL device, was made to the AutoHaul® system. The tool is effective in performing this</p>
<p>task.</p>
<p>However, because the FIP Tester does not test the FIP’s behaviour as a response to TCS requests, it</p>
<p>has limited functionality at Hitachi. Therefore, future work could be done to create a TCS Simulator</p>
<p>within the FIP Tester. This could be used to verify that the FIP responds to TCS messages correctly.</p>
<p>Furthermore, the scope of the project did not include sending correct interlocking data to the FIP.</p>
<p>Therefore, the developed IXL Simulators inside the FIP Tester only send “dummy” interlocking data to</p>
<p>the FIP. In order to make the testing more useful for Hitachi, the existing Microlok Interlocking</p>
<p>Simulation System (MISS) testing tool could be integrated with the FIP Tester. Combined with a TCS</p>
<p>Simulator and integration with MISS, the FIP Tester would allow Hitachi to test the entire journey of</p>
<p>the data from the IXL devices to the FIP and finally to the TCS.</p>
<h5>6.2 TCS T ESTING S UITE</h5>
<p>The project covered the testing for three TCS communication interfaces. These were the TCS – ATOC</p>
<p>interface, the TCS – RES interface and the TCS – VSS interface.</p>
<p>Prior to the project, a TCS Testing Suite had been developed in Hitachi to automate basic tests. During</p>
<p>this project, the TCS Testing Suite’s functionality was enhanced so that it could autonomously perform</p>
<p>tests for these three interfaces. This was done by:</p>
<ul>
<li>
<p>Modifying the helper functions to allow more complex test scenarios to be created,</p>
</li>
<li>
<p>Modifying the existing tests to allow them to perform more complex tests and</p>
</li>
<li>
<p>Writing more tests based on Hitachi’s existing test specifications.</p>
</li>
</ul>
<p>The TCS Testing Suite is a complex software tool that is difficult to navigate and debugging. To address</p>
<p>this issue, a Log Parser was developed which scanned through various log files generated by a test and</p>
<p>collated the errors into a single file. This made it easier for the user to debug the issue.</p>
<p>The main limitation of the TCS Testing Suite is that both the helper functions and the tests must be</p>
<p>constantly updated as the AutoHaul® requirements change. Therefore, it requires a lot of maintenance</p>
<p>in-order for it to remain useful at Hitachi. To address this issue, work can be done to make the tests</p>
<p>and the helper functions more modular. This would significantly reduce the workload that arises when</p>
<p>the system has to be updated as only the helper function would need to be updated.</p>
<p>Page | 39</p>
<p>Additionally, future work could also include further enhancing the TCS Testing Suite so that it can</p>
<p>perform automated testing for all of the TCS interfaces that were excluded from the scope in this</p>
<p>project.</p>
<h5>6.3 P ROJECT I MPROVEMENTS</h5>
<p>This project could have been improved by better documentation and project management strategies.</p>
<p>In response to the COVID-19 pandemic, the order in which project activities were conducted was</p>
<p>changed in March 2020. At that stage, work had primarily focused on enhancing the TCS Testing Suite</p>
<p>and very little progress had been made on developing a FIP Tester. Due to the seemingly high</p>
<p>likelihood of server access being lost, the project’s focus was switched to the FIP Tester, as that was</p>
<p>the project’s priority. However, progress made on the TCS Testing Suite was not properly documented</p>
<p>when the project’s priority changed. Therefore, when the project switched back to enhancing the TCS</p>
<p>Testing Suite weeks later, a lot of knowledge had to be re-learnt.</p>
<p>Furthermore, Hitachi had implemented updates to the TCS Testing Suite and included new</p>
<p>components. Therefore, a lot of the previously written tests were outdated and work had to be re
implemented.</p>
<p>A lot of time could have been saved if the progress made initially had been documented better.</p>
<p>Page | 40</p>
<h2>7 R EFERENCES</h2>
<p>[1] K. Smith, “Rise of the machines Rio Tinto breaks new ground with AutoHaul,” <em>International</em></p>
<p><em>Railway Journal,</em> vol. 59, no. 8, pp. 14-18, 2019.</p>
<p>[2] Mining Media International, “Rio Tinto Achieves First Delivery of Iron Ore With World's Largest</p>
<p>Robot,” <em>Engineering and Mining Journal,</em> vol. 219, pp. 4-6, 2018.</p>
<p>[3] Andrew Stewart, “AutoHaul System Architecture and Design Specification,” Hitachi Rail STS,</p>
<p>Brisbane, 2016.</p>
<p>[4] Andrew Stewart, “TCS – ATOC Interface Control Document,” Hitachi Rail STS, Perth, 2017.</p>
<p>[5] Andrew Stewart, “Automation Server Subsystem Requirements Specification,” Hitachi Rail STS,</p>
<p>Brisbane, 2019.</p>
<p>[6] Anthony MacDonald, “TCS-Wayside Interface Control Description,” Hitachi Rail STS, Brisbane,</p>
<p>2013.</p>
<p>[7] Graeme Reid, “TCS - PRCCI Interface Control,” Hitachi Rail STS, Brisbane, 2020.</p>
<p>[8] T. Rowbotham, “Interlocking,” in <em>Introduction to Signalling</em>, London, Institution of Railway Signal</p>
<p>Engineers, 2000.</p>
<p>[9] Lionel Van Den Berg, “TCS Architecture And Design Specifications,” Hitachi Rail STS, Brisbane,</p>
<p>2019.</p>
<p>[10] Andrew Stewart, “CTC – Automation Interface Control Document,” Hitachi Rail STS, Perth, 2018.</p>
<p>[11] Samuel Dekker, “AUTOMATION MMI User Manual For Train Controllers,” Hitachi Rail STS, Perth,</p>
<p>2020.</p>
<p>[12] Andrew Stewart, “TCS - VSS Interface Control Document,” Hitachi Rail STS, Perth, 2016.</p>
<p>[13] Kent Yip, “Fiber Interface Processor,” Hitachi Rail STS, Perth, 2008.</p>
<p>[14] Andrew Stewart, “TCS – RTIO External Systems Interface Control Document,” Hitachi Rail STS,</p>
<p>Perth, 2017.</p>
<p>[15] Stephane Joubert, “Genisys Protocol Description,” Hitachi Rail STS, Perth, 2013.</p>
<p>[16] P. Wigger, “Experience with Safety Integrity Level (SIL) Allocation in Railway Applications,”</p>
<p>Institute for Software, Electronics, Railroad Technology (ISEB), Cologne, 2001.</p>
<p>[17] U. Silchanka, “FIP Test Manager,” Hitachi Rail STS, Perth, 2008.</p>
<p>Page | 41</p>
<p>[18] Cenelec, “Railway Applications - Communication Signalling and Processing Systems - Software</p>
<p>for Railway Control and Protection Systems,” Cenelec, Brussels, 2011.</p>
<p>[19] Michal Cedrych, “TCS Integration Test Specification,” Hitachi Rail STS, Brisbane, 2019.</p>
<p>[20] Froglogic, “Squish,” Froglogic, 2020. [Online]. Available: https://www.froglogic.com/squish/.</p>
<p>[Accessed 28 February 2020].</p>
<p>[21] H. Bajaj, “Choosing The Right Automation Tool and Framework Is Critical To Project Success,”</p>
<p>Infosys, Bengaluru, 2018.</p>
<p>[22] M. A. Umar and C. Zhanfang, “A Study of Automated Software Testing: Automation Tools and</p>
<p>Frameworks,” <em>International Journal of Computer Science Engineering (IJCSE),</em> vol. 8, pp. 217-225,</p>
<p>December 2019.</p>
<p>[23] A. Méndez-Porras, M. N. Hidalgo, J. M. García-Chamizo, M. Jenkins and A. M. Porras, “A Top
Down Design Approach for an Automated Testing Framework,” in <em>International Conference on</em>
<em>Ubiquitous Computing and Ambient Intelligence</em>, Springer, 2015.</p>
<p>[24] Y. Wang, L. Chen, D. Kirkwood, P. Fu, J. Lv and C. Roberts, “Hybrid Online Model-Based Testing</p>
<p>for Communication-Based Train Control Systems,” <em>IEEE Intelligent Transportation Systems</em>
<em>Magazine,</em> vol. 10, no. 3, pp. 35-47, 2018.</p>
<p>[25] N. Borisov, A. Kluge, W. Luther and B. Weyers, “User Interface Design for Test and Diagnosis</p>
<p>Software in Automotive Production Environments,” in <em>International Conference on Ubiquitous</em>
<em>Computing and Ambient Intelligence</em>, Springer, 2014.</p>
<p>[26] M. Polo, P. Reales, M. Piattini and C. Ebert, “Test Automation,” <em>IEEE Software,</em> vol. 30, no. 1, pp.</p>
<p>84-89, Jan 2013.</p>
<p>[27] T. Jönsson, “Efficiency determination of automated techniques for GUI testing,” School of</p>
<p>Engineering in Jönköping, Jönköping, 2014.</p>
<p>[28] Smart Bear, “Test Complete,” Smart Bear, 2020. [Online]. Available:
https://smartbear.com/product/testcomplete/overview/. [Accessed 3 June 2020].</p>
<p>[29] A. Adlemos, H. Tan and V. Tarasov, “Test case quality as perceived in Sweden,” in <em>2018 ACM/IEEE</em></p>
<p><em>5th International Workshop on Requirements Engineering and Testing</em>, ACM, 2018.</p>
<p>[30] D. Freudenstein, J. Radduenz, M. Junker and S. Eder, “Automated test-design from</p>
<p>requirements: the Specmate tool,” in <em>5th International Workshop on Requirements Engineering</em>
<em>and Testin</em>, ACM, 2018.</p>
<p>[31] A. Méndez-Porras, M. N. Hidalgo, J. M. García-Chamizo, M. Jenkins and A. M. Porras, “A Top
Down Design Approach for an Automated Testing Framework,” in <em>International Conference on</em>
<em>Ubiquitous Computing and Ambient Intelligence</em>, Cham, 2015.</p>
<p>[32] B. Mountford, “Testing Procedure For TCS Test Suite,” Hitachi Rail STS, Perth, 2020.</p>
<p>Page | 42</p>
<p>[33] Andrew Stewart, “VICS Interface Control Document,” Hitachi Rail STS, Perth, 2016.</p>
<p>[34] European Committee for Electrotechnical Standardization, “Railway applications –</p>
<p>Communication, signalling and process systems – Software for railway control and protection
systems,” CENELEC, Brussels, 2011.</p>
<p>Page | 43</p>
<h2>8 A PPENDIX A: P ROJECT M ANAGEMENT S UMMARY</h2>
<h5>8.1 P ROJECT T IMELINE A ND R ESOURCES</h5>
<p>The overall project has been broken down into four stages – background research, design,</p>
<p>implementation and documentation. More details about each stage and the resources required for</p>
<p>that stage are provided in Sections 8.1.1 to 8.1.4. The project timeline is provided in Section 8.1.5.</p>
<p>8.1.1 Background Research Stage</p>
<p><strong>Time Period:</strong> Week 1 – Week 6 (6 weeks).</p>
<p>In order to understand the full context of the tests that need to be automated, background research</p>
<p>was conducted. This research focused on:</p>
<ul>
<li>
<p>How railway systems in general operate.</p>
</li>
<li>
<p>The architecture, design specifications and testing protocols of the AutoHaul® project.</p>
</li>
</ul>
<p>8.1.2 Design Stage</p>
<p><strong>Time Period:</strong> Week 5 – Week 8 (4 weeks).</p>
<p>The design stage of the project was completed by using the following steps.</p>
<ol>
<li>
<p>Understanding the existing systems and their advantages and weaknesses.</p>
</li>
<li>
<p>Researching alternative technologies to replace or supplement the existing tools. These</p>
</li>
</ol>
<p>technologies were required to be open-source and capable of running on an air-gapped</p>
<p>system.</p>
<ol>
<li>
<p>Designing test engines for testing the TCS interfaces and FIP communications.</p>
</li>
<li>
<p>Reviewing the test engine with the project supervisor.</p>
</li>
</ol>
<p>8.1.3 Implementation Stage</p>
<p><strong>Time Period:</strong> Week 8 – Week 19 (11 weeks).</p>
<p>The implementation stage was the longest stage in the project. It focused on implementing a test</p>
<p>engine and then verifying that the test engine was reliable. The implementation stage was completed</p>
<p>by using the following steps.</p>
<ol>
<li>
<p>Obtaining initial test data for the TCS and the FIP.</p>
</li>
<li>
<p>Testing basic scenarios for TCS and the FIP. These scenarios were simple and easy to</p>
</li>
</ol>
<p>implement, for example creating a train sheet in the system. The purpose of these tests was</p>
<p>to validate the design of the test engine.</p>
<ol>
<li>
<p>Revising the test engine as needed.</p>
</li>
<li>
<p>Create a testing protocol for the FIP testing.</p>
</li>
<li>
<p>Coding tests for the FIP. This included verifying that the tests yield correct results.</p>
</li>
<li>
<p>Coding the tests from the pre-existing TCS Integration testing protocol. This included verifying</p>
</li>
</ol>
<p>that the coded tests are performing correctly.</p>
<p>Page | 44</p>
<p>Initially, the TCS testing was to be carried out first as it required more input from other engineers to</p>
<p>set-up. However, due to the COVID-19 pandemic, the focus shifted to completing the FIP testing as it</p>
<p>was the project’s priority.</p>
<p>8.1.4 Documentation Stage</p>
<p><strong>Time Period:</strong> Intermittent during Week 4 – Week 23 (19 weeks).</p>
<p>The documentation stage focused on presenting the final solution and all of its relevant details. This</p>
<p>included creating documentation at Hitachi, to be used by engineers working on the AutoHaul® project</p>
<p>and documentation for university. The tasks and the time spent on each documentation activity are</p>
<p>shown below.</p>
<p>For Hitachi (Week 19 – Week 20 (2 weeks)):</p>
<ul>
<li>
<p>The software solution’s design specifications (Week 19),</p>
</li>
<li>
<p>The software solution’s test procedure (Week 19) and</p>
</li>
<li>
<p>User guide (Week 20).</p>
</li>
</ul>
<p>Furthermore, for university assessment, the following items are still required to be completed:</p>
<ul>
<li>
<p>Monthly reflection journals (0.5 days per journal),</p>
</li>
<li>
<p>Project Proposal (Week 7 – Week 8),</p>
</li>
<li>
<p>Interim report (Week 14 – Week 15),</p>
</li>
<li>
<p>Oral presentation (Week 20 – Week 21) and</p>
</li>
<li>
<p>Final report (Week 19 – Week 23).</p>
</li>
</ul>
<p>The original timeline that was submitted in the Project Proposal was modified so that the final report</p>
<p>was allocated an extra two weeks of time. This was done to ensure that there would be ample time</p>
<p>for the document to undergo a review at Hitachi to ensure that no confidential material is being</p>
<p>published.</p>
<p>8.1.5 Project Timeline</p>
<p>The expected project timeline is presented below as a Gantt chart in Figure 30. The tasks to be</p>
<p>completed are sorted by the stages described in Sections 8.1.1 to 8.1.4. The completion dates for the</p>
<p>tasks in the chart acted as milestones for the project.</p>
<p>Page | 45</p>
<p><em>Figure 30 Project timeline</em></p>
<p>Page | 46</p>
<h5>8.2 R ISK A NALYSIS</h5>
<p>The risks for this project were classified as, Operational Health &amp; Safety (OH&amp;S) risks and project</p>
<p>scheduling risks. The risks and relevant mitigation actions are detailed below. The risks have been</p>
<p>assigned a residual risk level based on the risk matrix in Section 8.2.3.</p>
<p>Since the Project Proposal, the only major change to the risk assessment was to accommodate for</p>
<p>risks posed by the COVID-19 pandemic.</p>
<p>8.2.1 Operational Health And Safety Risks</p>
<p>The work being conducted in this project was done in the Hitachi office which was a low risk</p>
<p>environment.</p>
<p><em>Table 17 OH&amp;S risks summary</em></p>
<table>
<thead>
<tr>
<th>Hazardous Consequences Residual Preventative Tasks Mitigation<br>Activity Risk Level Methods</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
<th>Col5</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Potential</strong><br><strong>exposure to</strong><br><strong>COVID-19 and</strong><br><strong>other viruses</strong></td>
<td>Getting sick</td>
<td>High</td>
<td> Practice the recommended<br>preventative tasks such as<br>maintaining social distancing<br>and good hygiene practices.<br> Use the same office desk and<br>equipment.</td>
<td> Alert office<br>members<br>immediately.<br> <br> If required,<br>parts of<br>testing the<br>TCS<br>interfaces will<br>be removed<br>from the<br>scope of the<br>project.</td>
</tr>
<tr>
<td><strong>Using testing</strong><br><strong>equipment,</strong><br><strong>which has</strong><br><strong>multiple large</strong><br><strong>monitors, for</strong><br><strong>extended</strong><br><strong>periods</strong></td>
<td>Aches and<br>cramps on<br>neck and back<br>muscles</td>
<td>Medium</td>
<td> Take regular breaks and<br>perform stretches.<br> Stand where possible rather<br>than tilting neck upwards.</td>
<td> Rest well and<br>perform non-<br>test bed<br>related tasks<br>for a few<br>days.</td>
</tr>
<tr>
<td><strong>Computer</strong><br><strong>usage for</strong><br><strong>extended</strong><br><strong>time periods</strong></td>
<td>Eye strain</td>
<td>Low</td>
<td> Ensure lighting and screen<br>positioning is optimal.<br> Take regular breaks and<br>perform eye exercises</td>
<td> Use<br>lubricating<br>eye drops<br>daily and take<br>rest.</td>
</tr>
<tr>
<td><strong>Typing for</strong><br><strong>extended</strong><br><strong>periods of</strong><br><strong>time</strong></td>
<td>Repetitive<br>strain injuries<br>to wrist</td>
<td>Low</td>
<td> Use an ergonomic keyboard<br>and perform wrist exercises at<br>regular intervals during the<br>day.</td>
<td> Rest well and<br>use wrist<br>supporting.</td>
</tr>
<tr>
<td><strong>Slips, trips</strong><br><strong>and falls</strong></td>
<td>Skin injuries or<br>broken bones</td>
<td>Low</td>
<td> Remain observant to<br>surroundings when moving<br>around the office.<br></td>
<td> Apply first aid<br>from the<br>office and<br>contact the<br>HSE officer.</td>
</tr>
</tbody>
</table>
<p>Page | 47</p>
<p>8.2.2 Project Scheduling Risks</p>
<p>These are risks which may cause the project to not be delivered in its ideal form.</p>
<p><em>Table 18 Scheduling risks summary</em></p>
<table>
<thead>
<tr>
<th>Hazardous Consequences Residual Preventative Tasks Mitigation Methods<br>Activity Risk Level</th>
<th>Col2</th>
<th>Col3</th>
<th>Col4</th>
<th>Col5</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>The office is</strong><br><strong>shut down due</strong><br><strong>to COVID-19</strong></td>
<td>Cannot access<br>VMs on<br>servers at<br>Hitachi and<br>therefore<br>sections of<br>work cannot<br>be completed<br>on time or to<br>required<br>standards.</td>
<td>High</td>
<td> Practice<br>recommended<br>prevention<br>practices to reduce<br>the spread of<br>COVID-19.</td>
<td> Install VMWare<br>Workstation on a<br>laptop and export<br>VMs from the server<br>onto it.<br> <br> If required, parts of<br>testing the TCS<br>interfaces will be<br>removed from the<br>scope of the project.</td>
</tr>
<tr>
<td><strong>Sickness</strong></td>
<td>Delays to the<br>project.</td>
<td>High</td>
<td> Practice<br>recommended<br>prevention<br>practices to avoid<br>COVID-19 and<br>other illnesses.</td>
<td> Notify supervisor and<br>work efficiently on<br>return.<br> <br> Determine sections<br>of the project to<br>priorities completing<br>if required.</td>
</tr>
<tr>
<td><strong>Delayed access</strong><br><strong>to required</strong><br><strong>resources</strong></td>
<td>Sections of<br>work cannot<br>be started on<br>time.</td>
<td>Medium</td>
<td> Plan ahead and<br>request access to<br>resources in<br>advance to<br>requiring to use<br>them.</td>
<td> Focus on different<br>sections of the<br>project while waiting<br>for particular<br>resources.</td>
</tr>
<tr>
<td><strong>Data losses</strong></td>
<td>Work will<br>need to be<br>repeated.</td>
<td>Medium</td>
<td> Make weekly<br>backup and<br>document<br>progress made.</td>
<td> Revert to the last<br>backup and continue<br>work.</td>
</tr>
<tr>
<td><strong>Procrastination</strong><br><strong>or mental</strong><br><strong>slumps</strong></td>
<td>Delays to the<br>project.</td>
<td>Low</td>
<td> Set weekly goals<br>and use<br>accountability<br>tools to track<br>progress.</td>
<td> Work efficiently at<br>other times.</td>
</tr>
</tbody>
</table>
<p>Page | 48</p>
<p>8.2.3 Risk Matrix</p>
<p><em>Table 19 Risk matrix</em></p>
<table>
<thead>
<tr>
<th>Col1</th>
<th>Col2</th>
<th>Col3</th>
<th>CONSEQUENCE</th>
<th>Col5</th>
<th>Col6</th>
<th>Col7</th>
<th>Col8</th>
</tr>
</thead>
<tbody>
<tr>
<td><br> <br> <br> <br> <br></td>
<td><br> <br> <br> <br> <br></td>
<td><br> <br> <br> <br> <br></td>
<td>Negligible (1)</td>
<td>Minor (2)</td>
<td>Moderate (3)</td>
<td>Significant (4)</td>
<td>Catastrophic (5)</td>
</tr>
<tr>
<td></td>
<td>OH&amp;S</td>
<td>OH&amp;S</td>
<td>First aid only</td>
<td>Medical treatment<br>required</td>
<td>Prolonged<br>hospitalisation<br>required</td>
<td>Permanent injury</td>
<td>Fatality</td>
</tr>
<tr>
<td></td>
<td>Scheduling</td>
<td>Scheduling</td>
<td>No significant<br>delays to the<br>project</td>
<td>Minor delays with<br>no lasting impact<br>to the project</td>
<td>Delays requiring<br>significant effort<br>to recover</td>
<td>Sections of the<br>project will not be<br>completed</td>
<td>Project cannot be<br>completed</td>
</tr>
<tr>
<td><strong>LIKELIHOOD</strong></td>
<td>Certain (5)</td>
<td>Frequent occurrence<br>(once a week)</td>
<td>Medium (11)</td>
<td>High (16)</td>
<td>High (20)</td>
<td>Extreme (23)</td>
<td>Extreme (25)</td>
</tr>
<tr>
<td><strong>LIKELIHOOD</strong></td>
<td>Likely (4)</td>
<td>Likely to occur     (once<br>a month)</td>
<td>Low (7)</td>
<td>Medium (12)</td>
<td>High (17)</td>
<td>Extreme (21)</td>
<td>Extreme (24)</td>
</tr>
<tr>
<td><strong>LIKELIHOOD</strong></td>
<td>Possible (3)</td>
<td>Possible to occur (once<br>every 6 months)</td>
<td>Low (4)</td>
<td>Medium (8)</td>
<td>Medium (13)</td>
<td>High (18)</td>
<td>Extreme (22)</td>
</tr>
<tr>
<td><strong>LIKELIHOOD</strong></td>
<td>Unlikely (2)</td>
<td>Unlikely to occur (once<br>a year)</td>
<td>Low (2)</td>
<td>Low (5)</td>
<td>Medium (9)</td>
<td>Medium (14)</td>
<td>High (19)</td>
</tr>
<tr>
<td><strong>LIKELIHOOD</strong></td>
<td>Rare (1)</td>
<td>Practically impossible<br>(once in 10 years)</td>
<td>Low (1)</td>
<td>Low (3)</td>
<td>Low (6)</td>
<td>Medium (10)</td>
<td>High (15)</td>
</tr>
<tr>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
<td><strong>RESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT</strong></td>
</tr>
<tr>
<td><strong>Low Risk</strong></td>
<td><strong>Low Risk</strong></td>
<td><strong>Medium Risk</strong></td>
<td><strong>Medium Risk</strong></td>
<td><strong>High Risk</strong></td>
<td><strong>High Risk</strong></td>
<td><strong>Extreme Risk</strong></td>
<td><strong>Extreme Risk</strong></td>
</tr>
<tr>
<td>1 to 7</td>
<td>1 to 7</td>
<td>7 to 14</td>
<td>7 to 14</td>
<td>15 to 20</td>
<td>15 to 20</td>
<td>20 to 25</td>
<td>20 to 25</td>
</tr>
<tr>
<td>No authority needed</td>
<td>No authority needed</td>
<td>Team leader</td>
<td>Team leader</td>
<td>Academic/ Hitachi Supervisor</td>
<td>Academic/ Hitachi Supervisor</td>
<td>Course Coordinator</td>
<td>Course Coordinator</td>
</tr>
</tbody>
</table>
<p>Page | 49</p>
<h5>8.3 P ROJECT D ELIVERABLES O UTCOMES</h5>
<p>As outlined in Table 7, this project had 9 deliverables. The outcomes of these deliverables are outlined</p>
<p>in Table 20.</p>
<p><em>Table 20 Outcomes of key</em> <em>project deliverables</em></p>
<table>
<thead>
<tr>
<th>Deliverable Outcomes</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1. Software Solution</strong></td>
<td>Two separate software solutions were developed. The first built on the<br>existing TCS Testing Suite and the second developed a Test Suite for<br>testing the FIP’s behavior via the FIP – IXL interface.</td>
</tr>
<tr>
<td><strong>2. Architecture Design</strong></td>
<td>The architecture design for both software solutions has been provided<br>in this report.</td>
</tr>
<tr>
<td><strong>3. Test Procedure</strong></td>
<td>A document containing the test procedure has been submitted to<br>Hitachi.</td>
</tr>
<tr>
<td><strong>4. User Manual</strong></td>
<td>A user manual has been submitted to Hitachi.</td>
</tr>
<tr>
<td><strong>5. Reflective Journals</strong><br><strong>6. Project Proposal</strong><br><strong>7. Interim Report</strong></td>
<td>These deliverables were submitted to UQ at various stages of the<br>semester. Figure 30 contains the full details of the submission times.</td>
</tr>
<tr>
<td><strong>8. Oral Presentation</strong></td>
<td>An oral presentation which summarised the project was delivered on the<br>11th of June 2020.</td>
</tr>
<tr>
<td><strong>9. Final Report</strong></td>
<td>This report is the final report.</td>
</tr>
</tbody>
</table>
<h5>8.4 O PPORTUNITIES</h5>
<p>The major project opportunities of this project are provided in Table 21.</p>
<p><em>Table 21 Project opportunities</em></p>
<table>
<thead>
<tr>
<th>Opportunity Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Further expanding the</strong><br><strong>TCS Testing Suite</strong></td>
<td>As a part of this project, the existing TCS Testing Suite was modified to<br>make it easier to perform more complicated operations, such as<br>registering multiple locomotives. Therefore, it will be possible to add<br>additional tests which can be used to test other TCS functionality and use<br>other TCS interfaces.</td>
</tr>
<tr>
<td><strong>Testing</strong><br><strong>the</strong><br><strong>FIP’s</strong><br><strong>behavior via a TCS –</strong><br><strong>FIP interface.</strong></td>
<td>This project focused on testing the FIP’s behavior by focusing on the<br>communications between IXL devices. Now that this test suite exists, it is<br>relatively easy to enhance this test suite so that it includes a TCS simulator<br>as well. This will make it easier to verify that the FIP is behaving correctly<br>to TCS requests and allowing the FIP’s behavior to be validated from the<br>TCS side of operations.</td>
</tr>
</tbody>
</table>
<p>Page | 50</p>
<h2>9 A PPENDIX B: FIP M ESSAGES</h2>
<p><strong>Genisys Protocol Message Structure</strong></p>
<p>Messages sent over the Genisys protocol have five components. All elements are sent as hexadecimal</p>
<p>characters. The five components of messages are [15]:</p>
<ol>
<li>
<p>Control character – a character which denotes the type of message being sent.</p>
</li>
<li>
<p>Station address – The address of the station where the IXL is located as a hexadecimal value.</p>
</li>
</ol>
<p>A message sent from an IXL would contain its own station number and a message sent from</p>
<p>the FIP would contain the station number of the target IXL.</p>
<ol>
<li>Data bytes – The interlocking data being sent for the TCS to use. Depending on the message</li>
</ol>
<p>type, data bytes may not be sent.</p>
<ol>
<li>Security checksum – A two byte Cyclic Redundancy Check (CRC) of all message bytes, up to</li>
</ol>
<p>but not including, the security checksum. Depending on the message type, a security</p>
<p>checksum may not be sent.</p>
<ol>
<li>Termination character – The character “f6”.</li>
</ol>
<p>Table 22 provides more details about each message type.</p>
<p><em>Table 22 FIP – IXL Message types and corresponding control characters [15]</em></p>
<table>
<thead>
<tr>
<th>Message Type Control Description<br>Character</th>
<th>Col2</th>
<th>Col3</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FIP to IXL Messages</strong></td>
<td><strong>FIP to IXL Messages</strong></td>
<td><strong>FIP to IXL Messages</strong></td>
</tr>
<tr>
<td>Master Acknowledge Message</td>
<td>fa</td>
<td>Used to acknowledge a message and to act as a poll<br>message.</td>
</tr>
<tr>
<td>Poll Message</td>
<td>fb</td>
<td>Used to ask the IXL if it has any new indications it<br>wishes to report. It receives an indication message if<br>the IXL does wish to send updated data or it receives<br>a slave acknowledgement.</td>
</tr>
<tr>
<td>Control Command</td>
<td>fc</td>
<td>Used to send controls.</td>
</tr>
<tr>
<td>Recall Indication Command</td>
<td>fd</td>
<td>Requests the IXL to send all its indications to the FIP.</td>
</tr>
<tr>
<td>Execute Controls Command</td>
<td>fe</td>
<td>Causes the IXL to write the controls to its database.</td>
</tr>
<tr>
<td>**IXL to FIP Messages **</td>
<td>**IXL to FIP Messages **</td>
<td>**IXL to FIP Messages **</td>
</tr>
<tr>
<td>Slave Acknowledge Message</td>
<td>f1</td>
<td>Sent as a response when no other response is<br>needed.</td>
</tr>
<tr>
<td>Indication<br>Data<br>Response<br>Message</td>
<td>f2</td>
<td>Used to send data to the FIP.</td>
</tr>
<tr>
<td>Control Checkback Message</td>
<td>f3</td>
<td>Used to verify the controls from the FIP when in<br>checkback control mode.</td>
</tr>
</tbody>
</table>
<p>As an example, a slave acknowledge message from station 80 would be sent as “f150f6”.</p>
<p>Page | 51</p>
<h2>10 A PPENDIX C: TCS M ESSAGE P ROTOCOLS</h2>
<h5>– 10.1 TCS ATOC C OMMUNICATION P ROTOCOL</h5>
<p>The message structure for all TCS – ATOC messages is described in Table 23.</p>
<p><em>Table 23 TCS – ATOC message protocol [4]</em></p>
<table>
<thead>
<tr>
<th>Field Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sequence number</strong></td>
<td>A number used to order the messages</td>
</tr>
<tr>
<td><strong>Time stamp</strong></td>
<td>Message time stamp</td>
</tr>
<tr>
<td><strong>Protocol version</strong></td>
<td>An identifier for the protocol version</td>
</tr>
<tr>
<td><strong>Source ID</strong></td>
<td>An identifier for the sender of the message</td>
</tr>
<tr>
<td><strong>Destination ID</strong></td>
<td>An identifier for the receiver of the message</td>
</tr>
<tr>
<td><strong>Length</strong></td>
<td>The length of the data being sent. Varies according to the<br>data.</td>
</tr>
<tr>
<td><strong>Data</strong></td>
<td>The data being communicated between the sub-systems. Is<br>variable in length.</td>
</tr>
<tr>
<td><strong>Cyclic redundancy checks (CRC)</strong></td>
<td>Uses CRC-32</td>
</tr>
</tbody>
</table>
<h5>– 10.2 TCS RES C OMMUNICATION P ROTOCOL</h5>
<p>TCS – RTIO External Systems messages all comply with the XML schema. The element and attribute
names depends on the type of message being sent. In general, the elements include:</p>
<ul>
<li>
<p>Message type,</p>
</li>
<li>
<p>Time stamp and</p>
</li>
<li>
<p>Message details.</p>
</li>
</ul>
<p>There are 12 types of messages that can be sent from the TCS to RTIO External Systems. These are:</p>
<ul>
<li>
<p>Train Sheet,</p>
</li>
<li>
<p>Train Step,</p>
</li>
<li>
<p>Track Block,</p>
</li>
<li>
<p>Fleeting,</p>
</li>
<li>
<p>Track Notification,</p>
</li>
<li>
<p>Train Notification,</p>
</li>
<li>
<p>Train Position,</p>
</li>
<li>
<p>HiRail Track Machine Position,</p>
</li>
<li>
<p>Point Tag,</p>
</li>
<li>
<p>Turnout Status,</p>
</li>
<li>
<p>Temporary Speed Restrictions and</p>
</li>
<li>
<p>Train Speed Restrictions.</p>
</li>
</ul>
<p>Additionally, there are 3 messages that RTIO External Systems might send to the TCS. These are:</p>
<ul>
<li>
<p>Refresh,</p>
</li>
<li>
<p>Execute Schedule and</p>
</li>
<li>
<p>Notification.</p>
</li>
</ul>
<p>Page | 52</p>
<h5>– 10.3 TCS VSS C OMMUNICATION P ROTOCOL</h5>
<p>The message structure for all TCS – VSS messages is described in Table 24.</p>
<p><em>Table 24 TCS – VSS message protocol [12]</em></p>
<table>
<thead>
<tr>
<th>Field Description</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Source ID</strong></td>
<td>An identifier for the sender of the message.</td>
</tr>
<tr>
<td><strong>Destination ID</strong></td>
<td>An identifier for the receiver of the message.</td>
</tr>
<tr>
<td><strong>Protocol version</strong></td>
<td>An identifier for the protocol version.</td>
</tr>
<tr>
<td><strong>VSS Status</strong></td>
<td>Can be Master or Standby.</td>
</tr>
<tr>
<td><strong>Time stamp</strong></td>
<td>Message time stamp.</td>
</tr>
<tr>
<td><strong>Length</strong></td>
<td>The length of the data being sent. Varies according to the<br>data.</td>
</tr>
<tr>
<td><strong>Data</strong></td>
<td>The data being communicated between the sub-systems. Is<br>variable in length.</td>
</tr>
<tr>
<td><strong>Cyclic redundancy checks (CRC)</strong></td>
<td>Uses CRC-32.</td>
</tr>
</tbody>
</table>
<p>Page | 53</p>
<h2>11 A PPENDIX D: T EST B ENCH S ET -U P</h2>
<p><em>Figure 31 Test bench set-up in Brisbane</em></p>
<p>Page | 54</p>
<h2>12 A PPENDIX E: FIP T ESTER W ORKFLOW D IAGRAMS</h2>
<p><em>Figure 32 Flowchart of process to test initial testing</em></p>
<p><em>sequences</em></p>
<p>Page | 55</p>
<p><em>Figure 33 Flowchart of process to test site-like</em></p>
<p><em>transmission errors</em></p>
<p>Page | 56</p>
<h2>13 A PPENDIX F: FIP T ESTER T ESTS</h2>
<p><em>Figure 34 Start-up test code snippet</em></p>
<p>Page | 57</p>
<h2>14 A PPENDIX G: TCS T ESTING S UITE A RCHITECTURE</h2>
<p><em>Figure 35 TCS Testing Suite architecture diagram [32]</em></p>
<p>Page | 58</p>
    </body>
    </html>
    