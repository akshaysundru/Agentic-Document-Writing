{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91d8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from ollama import Client\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ccda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract all text from a PDF using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f126cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf('Uniersity Queensland AHE Thesis.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a4b5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Faculty of Engineering, Architecture and Information Technology \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTHE UNIVERSITY OF QUEENSLAND \\n \\n \\nEnhanced Rail Test Automation \\n \\n \\n \\nStudent Name:  Isha, JOSHI \\n \\nCourse Code:  ENGG7290 \\n \\nSupervisor:  Graeme Smith – Associate Professor \\nSchool of Information Technology and Electrical Engineering \\n \\nSubmission date:  25th June 2020 \\n \\n \\n\\nPage | III  \\n \\nEXECUTIVE SUMMARY \\nHitachi Rail STS (Hitachi) has been contracted to deliver and maintain an Automated Train Operation \\n(ATO) system for Rio Tinto Iron Ore as a part of their AutoHaul® project. This ATO system facilitates \\nthe driverless movement of trains in a railway network in Western Australia. Because of its safety \\ncritical nature, any modifications made to the AutoHaul® system require extensive testing before they \\ncan be rolled out. Presently, this testing is performed manually and uses a lot of testing time and \\nresources. Hitachi has commissioned the design and development of an automated testing tool which \\ncan be used to reduce the testing time and improve testing efficiency. The testing would focus on the \\ncommunication interfaces between various AutoHaul® sub-systems.  \\nThe primary aim of this project was to design and develop an integrated testing framework to support \\nthe automated testing of the AutoHaul® project. The testing focused on testing the behaviour of the \\nFiber Interface Processor (FIP) and the Train Control Sub-system (TCS) by testing the communication \\ninterfaces between these sub-systems and other AutoHaul® sub-systems.  \\nThis report provides an overview of the AutoHaul® system and details about the specific interfaces \\nthat testing is being automated for. A review of literature relevant to designing an automated testing \\nframework is also present. \\nThis report also delivers a summary of the design and outcomes of the tool used to test the FIP’s \\nbehaviour. This tool, called the FIP Tester, simulates the behaviour of the Interlocking (IXL) devices \\nthat connect to the FIP. It verifies that the FIP is responding correctly to messages sent by the IXL \\nsimulators in order to ensure that the FIP is behaving correctly. Furthermore, the report provides a \\nsummary of the testing activities conducted to validate the behaviour of the FIP Tester.  \\nSimilarly, this report provides a summary of the design and project outcomes of enhancing a pre-\\nexisting tool, called the TCS Testing Suite. Prior to the project, this tool was developed at Hitachi to \\nbegin to automate the TCS testing. It contains the testing framework required to automate the testing \\nprocedure that is conducted by Hitachi.  \\nDuring this project, the TCS Testing Suite’s capabilities were enhanced to make it capable of running \\nmore complex tests. Additionally, a Log Parser which iterates through various log files and consolidates \\nall of the errors into a single file was developed to aid users in troubleshooting while using the TCS \\nTesting Suite. The report also provides a summary of the testing activities performed to validate the \\nbehaviour of the TCS Testing Suite enhancements and the Log Parser.  \\nOverall, the developed FIP Tester and the enhanced TCS Testing Suite can be reliably used to replace \\na lot of the manual testing activities conducted at Hitachi.  \\n \\n \\n \\n\\nPage | IV  \\n \\nACKNOWLEDGEMENTS \\nI would like to sincerely thank a number of people who have helped me throughout this project.  \\nFirstly, I would like to thank my supervisor Associated Professor Graeme Smith for the guidance and \\nadvice that was provided throughout the semester which helped shape this project.  \\nI would also like to thank the EAIT Employability team and Dr Christopher Leonardi at the University \\nof Queensland for all of their help in organising my placement at Hitachi Rail STS. \\nI would also like to thank Hitachi Rail STS, the host company, for allowing me the opportunity to work \\non this project and learn about the railway industry and the AutoHaul® project. In particular I would \\nlike to express my gratitude to my supervisory team at Hitachi, Dr Anthony MacDonald, Lionel Van \\nDen Berg and most of all Michal Cedrych for all of the support and advice I was given throughout the \\ncourse of the project. I would also like to thank Ujas Soni, Andrew Mijat and Benjamin Mountford at \\nHitachi for all of the technical advice and support they gave me throughout the project.  \\n \\n \\n\\nPage | V  \\n \\nTABLE OF CONTENTS \\nExecutive Summary ................................................................................................................................ III \\nAcknowledgements ................................................................................................................................ IV \\nList Of Figures ........................................................................................................................................ VII \\nList Of Tables ........................................................................................................................................ VIII \\n1.0 \\nIntroduction ................................................................................................................................ 1 \\n1.1 \\nPlacement Context .................................................................................................................. 1 \\n1.2 \\nPlacement Purpose ................................................................................................................. 1 \\n2.0 \\nTechnical Background ................................................................................................................. 2 \\n2.1 \\nThe AutoHaul® Project ............................................................................................................ 2 \\n2.2 \\nInterface Details And Current Testing Methods ..................................................................... 5 \\n2.2.1 \\nInterlocking – FIP Interface ............................................................................................. 5 \\n2.2.2 \\nTCS Interfaces .................................................................................................................. 7 \\n2.3 \\nLiterature Review .................................................................................................................. 10 \\n2.3.1 \\nDesigning a Test Automation Framework ..................................................................... 11 \\n2.3.2 \\nEvaluating Testing Tools ................................................................................................ 12 \\n2.3.3 \\nDesigning Test Cases ..................................................................................................... 12 \\n2.3.4 \\nSummary of Literature Review ..................................................................................... 13 \\n3.0 \\nProject Description .................................................................................................................... 14 \\n3.1 \\nAims And Objectives ............................................................................................................. 14 \\n3.2 \\nScope ..................................................................................................................................... 14 \\n3.3 \\nProject Deliverables .............................................................................................................. 16 \\n3.4 \\nProject Management ............................................................................................................ 16 \\n4 \\nDesign ........................................................................................................................................... 17 \\n4.1 \\nFIP Tester .............................................................................................................................. 17 \\n4.1.1 \\nTest Framework Design ................................................................................................. 17 \\n4.1.2 \\nTest Case Design ........................................................................................................... 19 \\n4.1.3 \\nGUI Design ..................................................................................................................... 23 \\n4.1.4 \\nPerformance Assessment Design .................................................................................. 24 \\n4.1.5 \\nDocumentation ............................................................................................................. 24 \\n4.2 \\nTCS Testing Suite ................................................................................................................... 25 \\n4.2.1 \\nIncreasing Testing Capabilities ...................................................................................... 26 \\n4.2.2 \\nLog Parser ...................................................................................................................... 27 \\n4.2.3 \\nPerformance Assessment.............................................................................................. 27 \\n4.2.4 \\nDocumentation ............................................................................................................. 28 \\n\\nPage | VI  \\n \\n5 \\nProject Outcomes ......................................................................................................................... 29 \\n5.1 \\nFIP Tester .............................................................................................................................. 29 \\n5.1.1 \\nThe Testing Tool ............................................................................................................ 29 \\n5.1.2 \\nThe GUI.......................................................................................................................... 30 \\n5.1.3 \\nTest Cases ...................................................................................................................... 30 \\n5.1.4 \\nPerformance Assessment.............................................................................................. 30 \\n5.1.5 \\nDocumentation ............................................................................................................. 34 \\n5.2 \\nTCS Testing Suite ................................................................................................................... 34 \\n5.2.1 \\nThe Enhanced TCS Testing Suite ................................................................................... 34 \\n5.2.2 \\nThe Log Parser ............................................................................................................... 36 \\n5.2.3 \\nPerformance Assessment.............................................................................................. 36 \\n6 \\nConclusion And Recommendations ............................................................................................. 39 \\n6.1 \\nFIP Tester .............................................................................................................................. 39 \\n6.2 \\nTCS Testing Suite ................................................................................................................... 39 \\n6.3 \\nProject Improvements .......................................................................................................... 40 \\n7 \\nReferences .................................................................................................................................... 41 \\n8 \\nAppendix A: Project Management Summary ............................................................................... 44 \\n8.1 \\nProject Timeline And Resources ........................................................................................... 44 \\n8.1.1 \\nBackground Research Stage .......................................................................................... 44 \\n8.1.2 \\nDesign Stage .................................................................................................................. 44 \\n8.1.3 \\nImplementation Stage ................................................................................................... 44 \\n8.1.4 \\nDocumentation Stage ................................................................................................... 45 \\n8.1.5 \\nProject Timeline ............................................................................................................ 45 \\n8.2 \\nRisk Analysis .......................................................................................................................... 47 \\n8.2.1 \\nOperational Health And Safety Risks ............................................................................ 47 \\n8.2.2 \\nProject Scheduling Risks ................................................................................................ 48 \\n8.2.3 \\nRisk Matrix .................................................................................................................... 49 \\n8.3 \\nProject Deliverables Outcomes ............................................................................................. 50 \\n8.4 \\nOpportunities ........................................................................................................................ 50 \\n9 \\nAppendix B: FIP Messages ............................................................................................................ 51 \\n10 \\nAppendix C: TCS Message Protocols ............................................................................................ 52 \\n10.1 \\nTCS – ATOC Communication Protocol................................................................................... 52 \\n10.2 \\nTCS – RES Communication Protocol ...................................................................................... 52 \\n10.3 \\nTCS – VSS Communication Protocol ...................................................................................... 53 \\n11 \\nAppendix D: Test Bench Set-Up ................................................................................................... 54 \\n12 \\nAppendix E: FIP Tester Workflow Diagrams ................................................................................. 55 \\n\\nPage | VII  \\n \\n13 \\nAppendix F: FIP Tester Tests ........................................................................................................ 57 \\n14 \\nAppendix G: TCS Testing Suite Architecture ................................................................................ 58 \\n \\nLIST OF FIGURES \\nFigure 1 AutoHaul® system breakdown .................................................................................................. 1 \\nFigure 2 High level system architecture diagram of relevant sub-systems and interfaces .................... 3 \\nFigure 3 FIP to IXL connections in the field ............................................................................................. 5 \\nFigure 4 FIP to IXL connections in a test environment ........................................................................... 6 \\nFigure 5 Example test from TCS integration test specification document [19] ...................................... 8 \\nFigure 6 TCS Testing Suite behaviour ...................................................................................................... 9 \\nFigure 7 Ranking of test case criteria as determined by Adlemo et al. [29] ......................................... 13 \\nFigure 8 High level overview of FIP Tester tool .................................................................................... 19 \\nFigure 9 Generic test case workflow diagram....................................................................................... 20 \\nFigure 10 Flowchart of process to test new configurations management – See Figure 32 for the start-\\nup sequence testing .............................................................................................................................. 20 \\nFigure 11 GUI design sketch .................................................................................................................. 23 \\nFigure 12 High level TCS Testing Suite test engine design .................................................................... 25 \\nFigure 13 Code showing a train being created before the enhancements .......................................... 26 \\nFigure 14 High level log parser design .................................................................................................. 27 \\nFigure 15 System architecture of FIP Tester ......................................................................................... 29 \\nFigure 16 High level overview of designed FIP Tester .......................................................................... 29 \\nFigure 17 GUI for the FIP Tester ............................................................................................................ 30 \\nFigure 18 Confirmation pop-up for the FIP Tester ................................................................................ 30 \\nFigure 19 'tcpdump' output .................................................................................................................. 31 \\nFigure 20 Linux command ‘netcat’ output ............................................................................................ 31 \\nFigure 21 Python terminal output of test case 1 (Start-up sequence) with correct replies as seen by the \\nFIP Simulator ......................................................................................................................................... 32 \\nFigure 22 Python terminal output of test case 1 (Start-up sequence) with incorrect replies as seen by \\nthe FIP Simulator ................................................................................................................................... 32 \\nFigure 23 FIP Tester log file when incorrect message is received ........................................................ 32 \\nFigure 24 FIP Tester log showing a passing test.................................................................................... 33 \\nFigure 25 FIP Tester log showing a failing test ...................................................................................... 33 \\nFigure 26 Screenshot from user guide .................................................................................................. 34 \\nFigure 27 Code showing the helper function implemented to create multiple trains ......................... 35 \\nFigure 28 Screenshot of the collated errors file ................................................................................... 36 \\n\\nPage | VIII  \\n \\nFigure 29 Example of a test case used to verify test behaviour with pass/fail comments ................... 37 \\nFigure 30 Project timeline ..................................................................................................................... 46 \\nFigure 31 Test bench set-up in Brisbane ............................................................................................... 54 \\nFigure 32 Flowchart of process to test initial testing  sequences ......................................................... 55 \\nFigure 33 Flowchart of process to test site-like transmission errors .................................................... 56 \\nFigure 34 Start-up test code snippet .................................................................................................... 57 \\nFigure 35 TCS Testing Suite architecture diagram [32] ......................................................................... 58 \\n \\nLIST OF TABLES \\nTable 1 Table of abbreviations ............................................................................................................... IX \\nTable 2 Description of sub-systems ........................................................................................................ 4 \\nTable 3 Polling cycle for FIP and IXL communications [15] ..................................................................... 6 \\nTable 4 TCS interface details ................................................................................................................... 7 \\nTable 5 Types of tests to implement autonomously ............................................................................ 14 \\nTable 6 Interfaces covered in the scope ............................................................................................... 15 \\nTable 7 Key deliverables list .................................................................................................................. 16 \\nTable 8 Summary of project stages ....................................................................................................... 16 \\nTable 9 System requirements for the FIP Tester .................................................................................. 18 \\nTable 10 Test specifications for FIP test 1: Testing the initial start-up sequence ................................ 21 \\nTable 11 Test specifications for FIP test 2: Testing with site-like conditions........................................ 21 \\nTable 12 Test specifications for FIP test 3: Testing how new configurations are handled ................... 23 \\nTable 13 Tests to verify the FIP Tester’s behaviour .............................................................................. 24 \\nTable 14 Results from manually checking the logs in Figure 24 ........................................................... 33 \\nTable 15 Results from manually checking the logs In Figure 25 ........................................................... 34 \\nTable 16 Results of asking a tester to debug using the Log Parser ....................................................... 38 \\nTable 17 OH&S risks summary .............................................................................................................. 47 \\nTable 18 Scheduling risks summary ...................................................................................................... 48 \\nTable 19 Risk matrix .............................................................................................................................. 49 \\nTable 20 Outcomes of key project deliverables .................................................................................... 50 \\nTable 21 Project opportunities ............................................................................................................. 50 \\nTable 22 FIP – IXL Message types and corresponding control characters [15] .................................... 51 \\nTable 23 TCS – ATOC message protocol [4] .......................................................................................... 52 \\nTable 24 TCS – VSS message protocol [12] ........................................................................................... 53 \\n \\n\\nPage | IX  \\n \\nTable 1 Table of abbreviations \\nAcronym \\nLong Term \\nAMMI \\nAutomation Man Machine Interface \\nAS \\nAutomation Server \\naTest \\nA proprietary tool used at Hitachi to simulate ATOC connections \\nATO \\nAutomatic Train Operation \\nATOC \\nAutomation Train Operation Controller \\nCTC \\nCentral Train Control \\nFIP \\nField Interface Processor \\nGUI \\nGraphical User Interface \\nHMI \\nHuman Machine Interface \\nIXL \\nInterlocking \\nMISS \\nMicrolok Interlocking Simulation System \\nRTIO \\nRio Tinto Iron Ore \\nRES \\nRTIO External Servers \\nSIL \\nSafety Integrity Level \\nTCS \\nTrain Control Sub-system \\nUDP \\nUser Datagram Protocol \\nUI \\nUser Interface \\nVM \\nVirtual Machine \\nVSS \\nVital Safety Server \\n\\n \\nPage | 1  \\n \\n1.0 INTRODUCTION \\n1.1 PLACEMENT CONTEXT \\nRio Tinto Iron Ore (RTIO) operates a heavy-haul freight railway network in Western Australia [1]. This \\nnetwork moves iron ore from mines in the Pilbara region to coastal ports for shipping overseas. The \\nAutoHaul® project provides an Automated Train Operation (ATO) system to facilitate the driverless \\nmovement of the trains on the mainline in RTIO’s network [2] [3]. As shown in Figure 1, AutoHaul® \\nuses three main sub-systems to control and monitor locomotives and ensure their safe movement in \\nthe network. These sub-systems are the Trainborne System, the Control Centre and the Wayside \\nSystems. More details about these sub-systems are given in Section 2.0. Hitachi Rail STS (Hitachi) was \\ncontracted to deliver and maintain this ATO system. \\n \\nFigure 1 AutoHaul® system breakdown \\nBecause the operations of trains in the AutoHaul® network are safety critical, any modifications made \\nto the AutoHaul® system require extensive testing before they are rolled out. Hitachi’s integration \\nteam is responsible for performing system-wide testing. This includes testing the system as a whole \\nto ensure that all of the sub-systems are integrating properly with each other and all of the \\ncommunication interfaces between different sub-systems work as required. Presently, the integration \\nteam uses manual testing for this purpose.  \\nDue to the AutoHaul® project’s complexity, manual testing is a time consuming and repetitive process \\nthat does not always identify the “edge case” issues in the system. As such, there is a potential to \\nautomate these tests.  \\n1.2 PLACEMENT PURPOSE \\nBecause Hitachi regularly roll out upgrades and modifications for the AutoHaul® project, they are \\ninvesting in methods to reduce testing time by automating tests.  As such, Hitachi wish to develop an \\nautomated testing tool that can be used to: \\n\\uf0b7 \\nAutomate time-consuming and repetitive tests,  \\n\\uf0b7 \\nFree up the tester’s time and allow them to perform different tests and therefore improve \\nthe thoroughness of the testing and \\n\\uf0b7 \\nReduce the products costs earlier in the testing stage of the product’s lifecycle. \\n\\n \\nPage | 2  \\n \\n2.0 TECHNICAL BACKGROUND \\n2.1 THE AUTOHAUL® PROJECT \\nThe AutoHaul® project introduced an Automated Train Operation (ATO) system so that trains are able \\nto move autonomously on the mainline of RTIO’s railway network in Western Australia [3]. \\nAs seen in Figure 1, the three sub-systems AutoHaul® uses to ensure the safe movement of trains are \\nthe Trainborne System, the Control Centre and the Wayside Systems [3].  \\nThe Trainborne System is installed in each AutoHaul® locomotive and uses the Automatic Train \\nOperations Controller (ATOC) as its primary control system. The ATOC can be thought of as the train’s \\ndriver. It interfaces to the locomotive’s equipment to perform tasks such as braking, accelerating and \\ncollision detection. It also communicates with the Control Centre to receive instructions and transfer \\nrelevant data [4]. \\nLocated in Perth, the Control Centre uses the Train Control Sub-system (TCS) to control the majority \\nof the AutoHaul® network [3]. It manages the train routing, mission planning and provides a user \\ninterface for operators to use.  \\nThe Wayside Systems contains a range of devices that are placed on the side of the tracks at various \\npoints in the network. Collectively, this system performs functions such as train tracking, interlocking \\nand controlling intersections on the track [5].   \\nA high level system architecture containing the sub-systems and communication interfaces relevant \\nfor this project is provided in Figure 2. A description of the sub-systems is provided in Table 2 and \\nmore details are given about these interfaces in Section 2.2. \\n\\n \\nPage | 3  \\n \\n \\nFigure 2 High level system architecture diagram of relevant sub-systems and interfaces \\n\\n \\nPage | 4  \\n \\nTable 2 Description of sub-systems \\nSub-system \\nFunctionality \\nWAYSIDE SYSTEMS \\nWayside Equipment \\nEquipment which is placed on the trackside and is used to monitor the \\nhealth and status of track-based assets [6]. \\nInterlocking (IXL) \\nThis component of signaling systems ensures that the railway behaves in a \\nsafe manner and is fail-safe [7] [8]. IXL devices do  this by: \\n\\uf0b7 \\nPerforming vital functions such as route setting, \\n\\uf0b7 \\nSending signaling information to the TCS and \\n\\uf0b7 \\nReceiving commands, such as clearing signals, from the TCS. \\nHitachi’s Microlok II is used as the IXL devices.   \\nTRAINBORNE SYSTEM \\nLocomotive \\nEquipment \\nA collection of equipment and computer systems that are used to monitor \\nand perform train operations such as interfacing with the ATOC and \\noperating the throttle and brakes [3].  \\n \\nAutomatic Train \\nOperations \\nController (ATOC) \\nThe primary control system of the train. It communicates with other sub-\\nsystems and controls all of the train’s operations [4].  \\nCONTROL CENTRE \\nTrain Control  \\nSub-system (TCS) \\nThe system used to monitor and control the railway network. It is used to \\nset routes, track train movement, manage alarms and perform monitoring \\nactions that were previously undertaken by drivers [9]. \\nCentralised Train \\nControl (CTC) \\nA train control system that provides the network overview, shows \\nindications and allows route setting and train sheet management [10]. \\nAutomation Man \\nMachine Interface \\n(AMMI) \\nA user interface to all the trains and locomotives in the AutoHaul® system. \\nIt allows users to access the CTC [11].  \\nAutomation Server \\n(AS) \\nA messaging service that acts as a gateway for the TCS and the rest of the \\nsystems in the AutoHaul® system [5].  \\nVital Safety Server \\n(VSS) \\nProvides movement authorities to the train based on data from the \\ninterlocking and level crossings [12]. The VSS also acts as a user interface \\nand allows users to set commands which are relayed to the rest of the \\nsystem.  \\nField Interface \\nProcessor (FIP) \\nA device which connects to all of the IXLs in the field and facilitates the \\nexchange of information between the IXLs and the TCS [13].  \\nRTIO External \\nSystems \\nA TIBCO Enterprise Message Service which acts as the interface between \\nthe AutoHaul® and RTIO’s other systems [14]. RTIO External Systems are \\nused for functions such as producing an electronic train graph.  \\n \\n\\n \\nPage | 5  \\n \\n2.2  INTERFACE DETAILS AND CURRENT TESTING METHODS \\n2.2.1 \\nInterlocking – FIP Interface \\n2.2.1.1 \\nFIP Functionality \\nEach IXL device acts as a User Datagram Protocol (UDP) server which has an open socket on a specific \\nIP address and port. The FIP runs multiple UDP clients which connect to IXLs on the given IP addresses \\nand ports to enable bi-directional communication as shown in Figure 3.  \\n \\nFigure 3 FIP to IXL connections in the field \\nPlease note that only 3 connections are shown in Figure 3, the AutoHaul® project actually uses many \\nmore IXLs. \\n2.2.1.2 \\nData Contents \\nThe data sent between the FIP and the IXLs uses the Genisys protocol [15]. Each message contains a \\ncontrol character, the address of the recipient/sender IXL, the interlocking data bytes, a security \\nchecksum and a termination character. More details about the packet structure are provided in \\nAppendix B: FIP Messages.  \\nThe interlocking data bytes coming out of the Microlok IIs are at a Safety Integrity Level (SIL) 4 [16], \\nand are therefore considered to be highly reliable. The FIP is only responsible for collating this data \\nand passing it along to the TCS, where it is validated. As such, validating the interlocking data is outside \\nthe scope of this project. Instead, the main focus of the testing is validating the FIP’s behaviour. \\nTo request for data, the FIP uses a pre-defined polling cycle [15] as shown in Table 3. There are five \\ntypes of messages that can be sent from the FIP to the IXL. These are poll messages, control messages, \\nrecall messages, execute messages and master acknowledge messages. Furthermore, there are three \\ntypes of messages that can be sent back to the FIP. These are indication messages, control check back \\nmessages and slave acknowledgement messages. More details about the messages are provided in \\nAppendix B: FIP Messages. \\n \\n \\n\\n \\nPage | 6  \\n \\nTable 3 Polling cycle for FIP and IXL communications [15] \\nFIP to IXL Message \\nExpected Reply \\nFIP to IXL1 \\nFIP to IXL2 \\nFIP to IXL3 \\nIXL to FIP \\nRecall \\nmessage \\nRecall \\nmessage \\nRecall \\nmessage \\nIndication message from each IXL. \\nControl \\nmessage \\nControl \\nmessage \\nControl \\nmessage \\nIndication or slave acknowledge message from each \\nIXL. \\nPoll \\nmessage \\nPoll \\nmessage \\nPoll \\nmessage \\nIndication or slave acknowledge message from each \\nIXL. \\nRecall \\nmessage \\nPoll \\nmessage \\nPoll \\nmessage \\nIndication message from IXL1 and slave acknowledge \\nmessages from other IXLs. \\nPoll \\nmessage \\nRecall \\nmessage \\nPoll \\nmessage \\nIndication message from IXL2 and slave acknowledge \\nmessages from other IXLs. \\nPoll \\nmessage \\nPoll \\nmessage \\nRecall \\nmessage \\nIndication message from IXL3 and slave acknowledge \\nmessages from other IXLs. \\nRecall \\nmessage \\nPoll \\nmessage \\nPoll \\nmessage \\nIndication message from IXL1 and slave acknowledge \\nmessages from other IXLs. \\n2.2.1.3 \\nCurrent Testing Methodology \\nIn a test environment, setting up the physical connections between the IXLs and the FIP is not feasible \\nas there are too many hardware requirements. As such, a combination of a Virtual Serial Port Emulator \\n(VSPE) and a Microlok Interlocking Simulation System (MISS) is used. The FIP requires the incoming \\ndata to come from UDP connections. However, due to being limited by its legacy software, MISS can \\nonly communicate using Transmission Control Protocol (TCP) communications. Therefore, a Socket \\nCat (SoCat) router is used to convert messages between TCP and UDP protocols. The testing set-up is \\nillustrated in Figure 4.  \\n \\nFigure 4 FIP to IXL connections in a test environment \\n \\n \\n\\n \\nPage | 7  \\n \\nCurrently the FIP behaviour is tested using two main methods. These are: \\n1) \\nVerifying that the IXL data received at the TCS is correct. It is assumed that if the data is correct \\nwhen it arrives at the TCS then the FIP must be behaving correctly. \\n2) \\nManually checking the log files that are stored in the FIP to verify the messages. This testing is \\nonly conducted when an end-to-end change, such as adding a new IXL device, is made [17].  \\nThis project will focus on automating the manual checks that are conducted of the FIP’s behaviour. \\n2.2.2 \\nTCS Interfaces \\nAs described in Section 2.1, the TCS is the main control system of the AutoHaul® network and is \\ntherefore tested frequently.  \\n2.2.2.1 \\nRelevant Interfaces \\nThere are three communication interfaces within the scope of this project. These are the TCS – ATOC \\ninterface, the TCS – RES interface and TCS – VSS interface. The key elements of these interfaces are \\nsummarised in Table 4. Additional details about the specific packet structures and message types of \\neach communication interface are provided in Appendix C: TCS Message Protocols. \\nTable 4 TCS interface details \\nInterface \\nDetails \\nTCS – ATOC \\n\\uf0b7 \\nThe TCS – ATOC interface is a collection of communication channels between \\nthe TCS and each ATOC system that is active in the AutoHaul® network.  \\n\\uf0b7 \\nLocomotives use this interface to send locomotive status information to the \\nTCS and receive instructions and software updates from the TCS.  \\n\\uf0b7 \\nThe interface uses the TCS – ATOC Protocol [4] as the messaging protocol. \\n\\uf0b7 \\nA propriety tool, aTest, is used to simulate messages that the ATOC would \\nusually send autonomously. Users can send these messages via a User \\nInterface (UI) or through commands set in a Python script.    \\nTCS – RES \\n\\uf0b7 This interface allows the RES to send planned train sheets and timetables to \\nthe TCS. It also allows the TCS to send data regarding the actual train \\nmovement back to the RES.  \\n\\uf0b7 \\nCommunication occurs via a TIBCO Message Service. \\n\\uf0b7 \\nIn the existing test environment, ActiveMQ is used as the messaging service \\nand Python scripts can be used to simulate RTIO messages.  \\n\\uf0b7 \\nThe interface uses the TCS – RTIO Protocol [14] as the messaging protocol.  \\nTCS – VSS  \\n\\uf0b7 This is a bi-directional communication channel that allows the TCS to send \\nlocomotive supervision data to the VSS and receive track information back \\nfrom the VSS.  \\n\\uf0b7 \\nThe interface uses the TCS – VSS Protocol [12] as the messaging protocol.  \\n\\uf0b7 \\nA VM running an instance of a VSS is running on the test bench.  \\n \\n \\n \\n\\n \\nPage | 8  \\n \\n2.2.2.2 \\nTesting Methodology \\nCurrently, two methods are used to test the TCS interfaces. These are manual testing and the TCS \\nTesting Suite.  \\nManual Testing \\nThis is the most commonly used form of testing at Hitachi. Testing the TCS involves manually \\nperforming sequences, such as creating a train sheet and verifying that all of the sub-systems behave \\nas expected. This testing is done at a test bench which runs either a version, or a simulation of all sub-\\nsystems deployed in the AutoHaul® project. Appendix D: Test Bench Set-Up  shows an image of this \\ntest bench.  \\nThe expected behaviour of the system is based on the TCS’s functional and behavioural expectations \\n[18] and is tested with tests laid out in the TCS Integration Test Specification document [19]. Figure 5 \\nshows one test from this document. The test outlines activities that must be done before the test can \\nbegin (preconditions), which are the sequences to perform during the test and the expected results \\nto each step. \\n \\nFigure 5 Example test from TCS integration test specification document [19] \\nAll of the tests must be run independently to each other. For example, step 1 of Test 3.33 in Figure 5 \\nrequires the tester to log into the CTC’s Human Machine Interface (HMI) as the Asset Health Evaluator \\n(AHE). At the end of Test 3.33 the user must log out as the AHE even if the next test requires them to \\nlogin as the AHE again. This adds a lot of time to the testing procedure as the testers must set-up and \\nremove the test environment for each test. \\nFurthermore, the testing methodology is very structured and does not leave much time for “creative \\ntesting”. Therefore, the testing does not cover all of the edge cases and unexpected situations that \\nmight arise in the field.  \\nAn experienced tester at Hitachi will require approximately an hour to complete the integration \\ntesting. This time estimate assumes that no bugs are found. In reality, testers find that they get half \\nway through the set of tests before a test fails. After fixing the cause of the failure, they must restart \\nthe testing from the beginning.  \\n\\n \\nPage | 9  \\n \\nTCS Testing Suite \\nA test environment has been created to run some of the more basic tests autonomously. The test \\nenvironment is installed on a VM and uses Python scripts to perform the actions listed below. \\n\\uf0b7 \\nInitialise connections with local versions of AutoHaul® servers such as the RES and VSS, \\n\\uf0b7 \\nConnect to the ATOC emulator, aTest and emulate messages sending from ATOC,  \\n\\uf0b7 \\nConnect to Squish, a tool created by Froglogic to automate UI testing [20], \\n\\uf0b7 \\nUse Squish to mimic interacting with the UIs and verify that the system is behaving as \\nexpected,  \\n\\uf0b7 \\nUse a library of helper functions to facilitate communication between the TCS and all other \\nAutoHaul® servers, ATOC and Squish and \\n\\uf0b7 \\nRun tests and log the activities in various log files depending on the action being performed.  \\nThis behaviour is summarised in Figure 6. Additionally, the file structure and architecture of the TCS \\nTesting Suite is provided in Appendix G: TCS Testing Suite Architecture. \\n. \\n \\nFigure 6 TCS Testing Suite behaviour \\nTo use the test suite, a user must run a particular test script in a PyCharm environment. Once a test \\nscript is run it behaves as follows: \\n1. The AutoHaul® servers and simulators are started. This includes, beginning connections to the \\nAutomation Server, VSS, and RTIO External Servers. \\n2. A start-up function is implemented. The start-up function sets up all of the pre-conditions (such \\nas creating and registering a train) that are required for the tests. \\n3. The tests are run. Generally, between 1 and 5 tests are contained within each script. The tests are \\ntypically implemented by: \\na. Making an event occur in the system. For example, creating and registering a train. \\nb. Verifying that all of the messages related to that event have been received at their \\ndestination. For example, verify that the CTC has received the details for a new train.  \\n\\n \\nPage | 10  \\n \\nc. Using Squish to verify that the relevant user interface has been updated. For example, \\nmaking sure that the new train ID is visible to the user.   \\nd. A shut down function is implemented. This function resets the testing environment so \\nthat it is back to its original status. For example, if a train has been registered during the \\ntests, then the train is deleted.  \\ne. The servers and simulators are shut down. This includes closing all active connections.  \\n4. A Python console displays the outcome of each test to the user.  \\nLike the manual testing, the test suite also uses the TCS Integration Test Specifications documentation \\n[19] for the test case design. However, not all of the required tests from the document have been \\nimplemented.  \\nThe current implementation only includes tests which check the functionality of a particular sub-\\nsystem and that sub-system’s User Interface (UI) currently exist. For example, a test can be run to \\ncheck that a train sheet is created and registered in the correct user’s view. But it cannot perform \\nmore complicated tests such as performing load testing to ensure multiple trains have been \\nregistered. \\nFurthermore, the test suite is a complex system that requires many active connections and uses many \\nlog files to keep track of the events in a test. Therefore debugging a test to determine the root cause \\nof failure can be a time consuming task. For example, as a part of registering a new train, a TCS \\nmessage is sent to the RES and the RES needs to send back a confirmation. If there is an error in \\nconnecting to the RES and therefore no confirmation is received by the TCS, then the following \\nmessages would be recorded in the most relevant log files. \\n\\uf0b7 \\nThe TCS and CTC logs would record that the train could not be registered, \\n\\uf0b7 \\nThe Automation Server logs would show that there was a timeout in waiting for the \\nconfirmation message and \\n\\uf0b7 \\nThe Automation Server logs would record that a connection could not be established with the \\nRES. However, this message would have been recorded at the start of the start-up activities \\nand would have been hidden by the newer activities.   \\nIt would be up to the user to recognise that the error is due to connection issues, not problems with \\nthe AutoHaul’s ® code or the test’s logic.   \\nThis project focused on enhancing the capabilities of the TCS Testing Suite so that more complicated \\ntests could run. It also provided a method for users to quickly determine the causes of failure in a \\nsystem.  \\n2.3 LITERATURE REVIEW \\nAn increasing amount of businesses have begun to use automated testing tools to improve the \\nefficiency of their testing and lower their product life cycle costs. The existing literature that is relevant \\nto the project covers the design and performance testing of automated testing tools and frameworks.  \\n \\n\\n \\nPage | 11  \\n \\n2.3.1 \\nDesigning a Test Automation Framework \\nThe framework required for automated testing involves defining support structure of the automation \\ntesting suite and the logical interactions between components within the testing suite. The project \\nwill require an automated testing framework to be developed from scratch for testing the FIP – IXL \\ninterface.  \\nA paper by Bajaj [21] determines that a well-designed framework is essential for the automated testing \\nsuite to be reusable, maintainable and of high quality. The author categorises the types of frameworks \\nfor test automation under four main categories; modular frameworks, data-driven frameworks, hybrid \\nframeworks and key-word driven frameworks. These categories are also supported by Umar and \\nZhanfang in their paper [22].  \\nMéndez-Porras et al. [23] recommend using a top-down approach to designing an automated testing \\nframework. The authors recommend that initially the testing framework’s objectives and \\nimplementation requirements should be identified. Then a design should be created that uses these \\nrequirements. The design should initially be a high-levelled overview and then provide more details \\nabout each element in it. The authors found that creating detailed descriptions for the framework and \\nits elements allowed them to minimise the number of test cases needed to identify all the errors in \\nthe system they were testing.  \\nAdditionally Wang et al. [24], hypothesises that because modern train control systems are becoming \\nmore complicated, a purely model based testing approach will not sufficiently test the system. Instead, \\nthey propose using a hybrid framework that is created in a virtual environment. The authors built a \\ntesting platform using an online Model-Based Testing (MBT) platform and a railway simulator. Their \\ntesting platform automatically generated and executed test cases.  \\nTo evaluate the testing platform, a case study was built using a real Communication Based Train \\nController (CBTC) system and testing was conducted for 12 hours. It was found that in most cases, the \\nhybrid MBT could detect errors more efficiently and in less time than traditional testing methods. \\nHowever, because the hybrid MBT created and executed test cases simultaneously, it could not detect \\nsome errors such as minor delay errors. \\nUser Interface Design \\nFor this project a UI was developed to increase the useability of the testing tools. The UI was designed \\nusing the approach recommended by Borisov et al. [25]. The authors present an approach to designing \\na Test and Diagnosis User Interface (TDUI) which can be used to test ambient intelligent systems in \\nproduction environments. Their UI design process focused on ensuring that the UI met the \\nrequirements needed for a safety-critical system, such as task conformance, providing feedback to \\nusers based on user actions and reporting on process status. The authors designed a TDUI using a two \\nlayer structure. The first layer was focused on the outward appearance of the UI and used strategies \\nsuch as creating guideline resources, style guides and having a good page layout to reduce errors in \\ninteractions. The second layer was the main Interaction Logic Layer (ILL) used to connect the Graphical \\nUser Interface (GUI) to the rest of the test environment. The paper found that using the two layer \\napproach, allowed for the designed TDUI to be adaptive to new changes and meet individual user \\nrequirements.  \\n \\n\\n \\nPage | 12  \\n \\nRelevant Standards \\nBecause the railway industry is highly regulated the relevant standards must be followed to implement \\na testing framework. As the AutoHaul® project is based on the European Train Control System (ETCS), \\nit uses standards set by the European Committee for Electrotechnical Standardization (CENELEC).  \\nFor this project, the CENELEC standard EN50128:2011 for “Railway applications – Communication, \\nsignalling and process systems – Software for railway control and protection systems” [18] is \\napplicable. Based on the information in the standard, the project must include the development of \\ntest specifications, a test procedure, and the tests must produce a test report.  \\n2.3.2 \\nEvaluating Testing Tools \\nThe literature surrounding evaluating the performance of testing tools is primarily focused on \\nevaluating tools which are commercially available and may be used in various applications. Although \\nthis project requires the evaluation of a tool developed for a specific purpose, the general criteria that \\nother papers have used to evaluate tools will still be relevant to the project.  \\nAs per Bajaj [21], the effectiveness of a testing tool or testing framework for a project is dependent \\non the technology stack which is being tested, the testing requirements, the skill sets of the users and \\nthe project’s testing budget.  \\nBajaj provides a summarised view of different testing tools and how they meet the above listed \\nselection criteria. The paper recommends that the chosen testing tool should be tested with a proof \\nof concept as soon as it is adopted. This allows the users to test the compatibility of the testing tools \\nwith the existing systems and ultimately allows the users to be more confident when choosing the \\nfinal testing tool.  \\nSimilarly, in a paper by Polo et al. [26], various testing tools are compared based on their maturity \\nlevels and the expertise level required to use them. The authors provide a comparison of different \\ntesting tools and provide recommendations for tools based on the intended use. Overall, Polo et al. \\nconcluded that the best testing tool for a system is the one which has the highest maturity level and \\nrequires the lowest expertise level.  \\nIn his paper Jönsson [27], ranks the performance of automated GUI testing tools based on their defect \\ndetection, repeatability of tests and time requirements from users. Jönsson compares Squish, \\nTestComplete and manual testing. The paper focused on comparing Squish, a GUI testing tool \\ndeveloped by Froglogic [20], TestComplete, a GUI testing tool by Smart Bear [28] and manual testing. \\nJönsson found that all three methods identified the same amount of defects, but Squish performed \\nbetter in repeatability testing and required less time to set-up initially than TestComplete. This is \\nuseful to the project as the current TCS Testing Suite utilises Squish to automate GUI tasks.  \\n2.3.3 \\nDesigning Test Cases \\nA large part of evaluating how effective a test tool is in uncovering a system’s flaws includes \\ndeveloping high quality test cases. The literature surround developing test cases is relevant to \\ndeveloping tests for the FIP – IXL interface.  \\n \\n\\n \\nPage | 13  \\n \\nIn a preliminary investigation created for the Ontology-based Software Test cAse Generation (OSTAG), \\nAdlemo et al. consolidate and rank 15 criteria for “good” test case performance [29]. The authors use \\nexisting literature to identify important criteria for evaluating test case performance and then ask 13 \\nsoftware experts to rank the criteria on a scale of 0 (not relevant at all) to 10 (highly relevant). All of \\nthese software experts were from low to medium sized Swedish companies. The resulting rankings \\nare shown in Figure 7. Based on the findings of Adlemo et al. the designed test cases should be \\nrepeatable, accurate and correct.  \\n \\nFigure 7 Ranking of test case criteria as determined by Adlemo et al. [29] \\nIn their paper, Freudenstein et al. [30] develop a tool to partially automate the requirement based \\ntesting process [30]. Their tool, Specmate, captures testing requirements and uses them to generate \\ntest procedures or test-scripts for use in Allianz Deutschland. Their algorithm for creating test \\nprocedures is a three-step process. Firstly, Cause-Effect-Graphs (CEGs) model logical statements and \\ntheir relationships. Secondly, test specifications are developed from the CEG outputs. Finally, the test \\nprocedure is implemented from the test specifications. The authors tested Specmate in Alliance \\nDeutschland and found that it helped reduce the efforts in the creation of test-procedures but \\nultimately did not save time as the system took too long to setup. Because the testing requirements \\nfor the FIP – IXL interface will not change, an automated test case generator, as suggested by \\nFreudenstein et al. [30] is not necessary. Instead, the three step approach recommended by the \\nauthors can be used manually to derive a single set of test cases in this project.  \\n2.3.4 \\nSummary of Literature Review \\nThis review addressed the existing literature used to design a testing framework, evaluate testing tools \\nand develop test cases. Based on the findings of the literature review, the following decisions were \\nmade for the development of the project: \\n\\uf0b7 \\nDeveloping an automated testing framework using the top-down method recommended by \\nMéndez-Porras et al. [23], \\n\\uf0b7 \\nUsing a two layer structure, as recommended by Borisov et al. [25], when developing a GUI,  \\n\\uf0b7 \\nEnsuring that the project complies with the CENELEC standards EN50128:2011 [18], \\n\\uf0b7 \\nContinuing to use Squish for TCS testing due to the findings of by Jönsson [27],  \\n\\uf0b7 \\nUtilising Freudenstein et al.’s three-step method to designing test cases [30] and \\n\\uf0b7 \\nEnsuring the test cases meet the criteria’s for good test cases as described by Adlemo et al. \\n[29]. \\n\\n \\nPage | 14  \\n \\n3.0 PROJECT DESCRIPTION \\n3.1 AIMS AND OBJECTIVES \\nThe overarching purpose of the project was to design and develop an integrated testing framework to \\nsupport the automated testing of the AutoHaul® project. The testing concentrated on expanding the \\ncurrent testing framework to include the tests described in Table 5.  \\nTable 5 Types of tests to implement autonomously \\nTesting Type \\nDescription \\nSite-like behavior testing \\nTesting with inputs that the AutoHaul® system received from the site.   \\nLoad testing \\nTesting the behavior of the AutoHaul® system under normal and \\nextreme loads.  \\nCapacity testing \\nTesting to validate that the AutoHaul® system can handle the amount \\nof traffic that it was designed to handle.  \\nAutomated testing of \\nnew configurations \\nTests which can be used to see how the AutoHaul® system will behave \\nwhen new configurations are programmed into it.  \\nAs such, the main goals were: \\n\\uf0b7 \\nReviewing the existing testing methods at Hitachi, \\n\\uf0b7 \\nSuccessfully developing a testing tool that can perform the tests detailed in Table 5 and \\n\\uf0b7 \\nCreating documentation that allows the designed testing tool to be used easily.  \\n3.2 SCOPE \\nThe scope of the project focused on the capabilities of the testing tool. Items and tasks that were \\nconsidered within the scope were: \\n\\uf0b7 \\nInvestigating and choosing tools to automate the testing of systems via testing four specific \\ncommunication interfaces. The interfaces, and a brief description of their purpose are given \\nin Table 6.  \\n\\uf0b7 \\nDeveloping a test engine with the chosen testing tools that can perform all required tests. \\n\\uf0b7 \\nProviding the documentation necessary for the tests to be used at Hitachi. \\nThe tasks that were considered out of scope for this project were: \\n\\uf0b7 \\nValidating the payload of the interlocking data that is sent to the Fiber Interface Processor \\n(FIP) from the Microlok Interlocking System (IXL). \\n\\uf0b7 \\nTesting the communication mediums (such as wireless or fibre optics) that are used in the \\ncommunication interfaces. \\nThe IXL – FIP interface was included in the scope as there was no testing tool used at Hitachi to test \\nbehaviour of this interface. All previous testing was focused on validating IXL data at the TCS and \\nassuming that the FIP behaviour was correct. As such, testing the FIP’s behaviour using the FIP – IXL \\ninterface was the project’s main priority.   \\n \\n\\n \\nPage | 15  \\n \\nThe three TCS interfaces were included in the scope as the TCS is the main system in the AutoHaul® \\nproject and any changes made to the system generally impact it. Therefore, this project tested the \\nTCS’s behaviour by using the ATOC, RES and VSS interfaces. Only these three interfaces have been \\nchosen instead of other TCS interfaces, such as the TCS – FIP interface, because the other interfaces \\neither have testing tools that have been specifically designed for them already or have no supporting \\nfunctions in the TCS Testing Suite. As such, automating the testing for those interfaces would take \\nmore time than was available for this project.  \\nMore about these interfaces and how they fit into the AutoHaul® project are provided in Section 2.0. \\nTable 6 Interfaces covered in the scope \\nInterface \\nDescription \\nTrain Control Sub-\\nsystem (TCS) – \\nAutomatic Train \\nOperation Controller \\n(ATOC) \\n\\uf0b7 \\nThe TCS is a part of the Control Centre and is responsible for \\ncontrolling most of the operations of the railway network.   \\n\\uf0b7 \\nATOC is a part of the Trainborne System that acts as the main control \\nsystem inside each train.  \\n\\uf0b7 \\nThe TCS – ATOC interface is responsible for transmitting messages \\nbetween these systems and is vital for the safe movements of trains \\nin the network.  \\nTCS – RTIO External \\nSystems (RES) \\n\\uf0b7 \\nThe RES is a server which acts as a gateway between the TCS and \\nother RTIO’s operations.  \\n\\uf0b7 \\nThe TCS – RES interface is responsible for transmitting messages \\nbetween these systems. \\nTCS – Vital Safety \\nServers (VSS) \\n\\uf0b7 The VSS utilises data from the network to determine if and where it \\nis safe for trains to move.  \\n\\uf0b7 The TCS – VSS interface is responsible transmitting messages \\nbetween these systems. \\nMicrolok Interlocking \\nSystem (IXL) – Fiber \\nInterface Processor \\n(FIP) \\n\\uf0b7 Various IXLs are placed next to tracks and perform vital functions \\nsuch as route settings. The FIP is a device which polls all of the IXLs, \\ncollates their responses and sends the data onto the TCS.  \\n\\uf0b7 The FIP – IXL interface is responsible for transmitting messages \\nbetween these systems.  \\n \\n \\n \\n\\n \\nPage | 16  \\n \\n3.3 PROJECT DELIVERABLES \\nThe key deliverables for the project are outlined in Table 7. \\nTable 7 Key deliverables list \\nDeliverable \\nDescription \\n1. Software Solution \\nA software package that meets the goals detailed in Section 3.1. \\n2. Architecture \\nDesign \\nThe design and architecture of the software solution.  \\n3. Test Procedure \\nThe procedure used to validate the design of the software solution.  \\n4. User Manual \\nA guide which steps the user through how to configure and run tests and \\ninterpret the output. \\n5. Reflective \\nJournals \\n5 journals which reflect on key learnings relating to Engineers Australia \\nstage 1 competencies during the placement.  \\n6. Project Proposal  \\nA document which outlines the context, project plan and risk assessment \\nof the project.  \\n7. Interim Report \\nA report which presents the project’s progress and includes a discussion \\nof the results obtained.  \\n8. Oral Presentation \\nA presentation which summaries the findings and recommendations of \\nthe project. \\n9. Final Report \\nA report which details the findings and recommendations of the project.  \\n3.4 PROJECT MANAGEMENT \\nThe project was split into four stages. A description of each stage is provided in Table 8. More details \\nabout the stages and the project’s timeline are provided in Appendix A: Project Management \\nSummary. \\nTable 8 Summary of project stages \\nStage \\nDescription \\nTime spent \\non stage \\nResearch \\nThis stage was used to conduct research in order to \\nunderstand the full context of the tests being performed and \\nthe interfaces being tested.  \\n6 weeks \\nDesign \\nIn this stage, the testing tools that would be used for testing \\nwere chosen. A test engine was also designed. \\n4 weeks \\nImplementation \\nDuring this stage, functionality was programmed so that the \\ntest engine could perform the required testing.  \\n11 weeks \\nDocumentation \\nIn this stage, user guides and test specifications were written \\nto document the work done on the project and ensure that \\nHitachi could continue to access it after the completion of the \\nproject.  \\n6 weeks \\n\\n \\nPage | 17  \\n \\n4 DESIGN \\nThis section describes the technical approach used to develop testing tools for the FIP – IXL interface \\nand the TCS interfaces. As the two testing tools were different, two separate design strategies were \\napplied and are described below.  \\n4.1 FIP TESTER \\nThe aim of testing the FIP – IXL interface was to check the behaviour of the FIP by ensuring that its \\nresponses to IXL messages are correct. Hence, it was decided that a FIP testing tool (FIP Tester) would \\nbe developed. This tool would simulate IXL devices in a fashion similar to how aTest (Hitachi’s \\nproprietary ATOC simulation tool) operates in the TCS testing.  \\n4.1.1 \\nTest Framework Design \\nThe design process used to design this testing tool was based on the top-down methodology \\nrecommended by Méndez-Porras et al. [31] in their paper. As recommended by Méndez-Porras, the \\nsystem requirements were defined, then a high level system was designed.  \\nSystem Requirements \\nThe requirements for the FIP Tester are provided in Table 9. \\n \\n \\n\\n \\nPage | 18  \\n \\nTable 9 System requirements for the FIP Tester \\nRequirement \\nImplementation Requirements \\n1 \\nAll IXL simulations shall connect to the FIP in the same way that IXL devices do. \\n2 \\nIXL packets shall use the Genisys protocol.  \\nInstead of actual interlocking data, the IXL simulator shall send “0101”.  \\n3 \\nNo changes shall be made to the FIP’s software to use this testing tool. \\n4 \\nThe FIP Tester shall integrate with Hitachi’s existing systems. \\nTest Case Requirements \\n5 \\nIn order to run capacity and load testing, the FIP Tester shall simulate multiple IXL devices at \\nthe same time. \\n6 \\nThe FIP Tester shall test the behavior of the FIP in response to site-like transmission errors. \\nThese errors include: \\n\\uf0b7 \\nPartially received messages, \\n\\uf0b7 \\nCorrupted messages, \\n\\uf0b7 \\nDuplicated messages, \\n\\uf0b7 \\nLost messages and \\n\\uf0b7 \\nMessages time-out. \\n7 \\nThe FIP Tester shall test the behaviour of the FIP in response to a new IXL connecting to it \\nafter it has already been running. \\nSoftware Requirements \\n8 \\nThe FIP Tester shall only use open source software libraries. \\nDocumentation Requirements \\n9 \\nA document outlining the architecture of the software solution and the test procedure used \\nto validate its behavior shall be provided. \\n10 \\nA user guide which provides instructions shall be provided. The user guide shall provide \\ninstructions on: \\n\\uf0b7 \\nSetting up the FIP Tester, \\n\\uf0b7 \\nRunning tests and \\n\\uf0b7 \\nMaking modifications to the system \\nOutput Requirements \\n11 \\nThe user shall be presented with a pass/fail report at the end of the tests. If a test fails, the \\nreport should include the cause of failure. \\n12 \\nThe FIP Tester shall log the testing activities to make it easier to debug issues.  \\nHigh Level Design \\nAs per Méndez-Porras et al. [31], the next step in designing the framework was to develop a high level \\nsystem overview and then breakdown the functions of each element in the framework. As \\nsummarised in Figure 8, the designed behaviour of the FIP tester is as follows.  \\n1. A GUI allows the user to enter test parameters. Example parameters include specifying the \\nnumber of IXLs to create.  \\n2. The test engine runs the tests to check the functionality of the FIP. This test engine should \\nbe able to perform the following tasks. \\na. Log all testing activities. \\nb. Initialise IXL simulators and connect them to the FIP based on user specified IP \\naddresses and port numbers.  \\nc. Run tests to check that all the IXLs are connected and that communications are \\nfollowing the polling cycle described in Table 3. \\n\\n \\nPage | 19  \\n \\nd. Test how the FIP behaves in each of the test cases.  \\ne. Shut down the test environment to prevent it from affecting future tests. \\n3. A pass/fail report is generated to inform the user about the results of the tests. In case a test \\nfails, the report will include a reason for the failure.   \\n \\nFigure 8 High level overview of FIP Tester tool \\nKey decisions made when designing this framework include: \\n\\uf0b7 \\nUsing Python as the programming language because of the availability of multiple open-\\nsource libraries that will be useful to the project, \\n\\uf0b7 \\nImplementing the program in a Linux VM to increase adoptability in Hitachi. \\n4.1.2 \\nTest Case Design \\nThere were three test cases for which tests were designed. These were:  \\n1) Testing the initial start-up sequence,  \\n2) Testing with site-like conditions and \\n3) Testing how new configurations are handled.  \\nThe generic process used for tests (1) and (2) are shown in Figure 9. Detailed workflow diagrams for \\neach test are provided in Appendix E: FIP Tester Workflow Diagrams. For testing site-like conditions, \\nthe 6 types of communication errors that were simulated and their expected response are:  \\n\\uf0b7 \\n25% packet loss,  \\n\\uf0b7 \\n50% packet loss,  \\n\\uf0b7 \\n75% packet loss,  \\n\\uf0b7 \\nDuplicate packets being sent,  \\n\\uf0b7 \\nCorrupted packets being sent and \\n\\uf0b7 \\n100% packet loss.  \\nIn all of these cases, the FIP is expected to disregard the response that is received from the IXL and \\nresend the request for the response.  \\n\\n \\nPage | 20  \\n \\n \\nFigure 9 Generic test case workflow diagram \\nThe generic process used for test (3) is provided in Figure 10. For each of the three tests, a test \\nspecification was created and is detailed in Table 10 to Table 12. \\n \\nFigure 10 Flowchart of process to test new configurations management – See Figure 32 for the start-up sequence testing \\n \\n\\n \\nPage | 21  \\n \\nFIP Test 1: Testing the initial start-up sequence \\nDescription: Ensures that the FIP sends a recall, control, and poll message as per its polling cycle.  \\nPre-test actions:  \\n1) Restart the FIP. \\n2) Ensure the VM hosting the FIP Tester has the correct networking settings. \\nTable 10 Test specifications for FIP test 1: Testing the initial start-up sequence \\nStep \\nNo \\nDescription \\nExpected Result \\n1 \\nFIP Tester begins running \\nA GUI opens that asks the user to enter \\ntest configurations. \\n2 \\nA recall message is received at each valid IXL \\nThe FIP Tester logs show the received \\nmessage and display a message that shows \\nthat the recall test is passed. \\n3 \\nAn indication message is sent by each IXL. \\nThe FIP should receive this message and send \\na control message back to the IXL \\nThe FIP Tester logs show the received \\nmessage and display a message that shows \\nthat the control test is passed. \\n4 \\nA slave acknowledge message is sent by each \\nIXL. The FIP should receive this message and \\nsend a poll message back to the IXL \\nThe FIP Tester logs show the received \\nmessage and display a message that shows \\nthat the poll test is passed. \\n5 \\nAn indication message is sent by each IXL. \\nThe FIP should receive this message and send \\na recall message back to the IXL \\nThe FIP Tester logs show the received \\nmessage and display a message that shows \\nthat the test is complete. \\nTest ends. \\nFIP Test 2: Testing with site-like conditions \\nDescription: Ensures that the FIP can respond appropriately when there are communications errors.  \\nPre-test actions:  \\n1) Restart the FIP. \\n2) Ensure the VM hosting the FIP Tester has the correct networking settings. \\n3)  \\nTable 11 Test specifications for FIP test 2: Testing with site-like conditions \\nStep \\nNo \\nDescription \\nExpected Result \\n1 \\nFIP Tester begins running \\nA GUI opens that asks the user to enter test \\nconfigurations. \\n2 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message \\nand display a message saying the first recall \\nmessage has been received. \\n3 \\nAn indication message is created and \\n25% of the message is sent by each IXL \\nto the FIP. The FIP should receive this \\nmessage, realise that it is invalid and \\nresend the recall message.  \\nThe FIP Tester logs show the recall message \\narriving and displays a message that shows that \\nthe 25% packet loss recall test has passed. \\n4 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n\\n \\nPage | 22  \\n \\n5 \\nAn indication message is created and \\n50% of the message is sent by each IXL \\nto the FIP. The FIP should receive this \\nmessage, realise that it is invalid and \\nresend the recall message.  \\nThe FIP Tester logs show the recall message \\narriving and displays a message that shows that \\nthe 50% packet loss recall test has passed. \\n6 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n7 \\nAn indication message is created and \\n75% of the message is sent by each IXL \\nto the FIP. The FIP should receive this \\nmessage, realise that it is invalid and \\nresend the recall message.  \\nThe FIP Tester logs show the recall message \\narriving and displays a message that shows that \\nthe 75% packet loss recall test has passed. \\n8 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n9 \\nAn indication message is created. It is \\nduplicated and sent by each IXL to the \\nFIP. The FIP should receive this message, \\nrealise that it is invalid and resend the \\nrecall message.  \\nThe FIP Tester logs show the recall message \\narriving again received message and display a \\nmessage that shows that the duplicate packet \\nrecall test has passed. \\n10 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n11 \\nAn indication message is created. Its CRC \\nis changed to “0000” and it sent by each \\nIXL to the FIP. The FIP should receive this \\nmessage, realise that it is invalid and \\nresend the recall message.  \\nThe FIP Tester logs show the recall message \\narriving again received message and display a \\nmessage that shows that the corrupted packet \\nrecall test has passed. \\n12 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n13 \\nNo reply is sent \\nThe FIP will timeout while waiting for a reply \\nand resent a recall message.  \\n14 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message. \\n \\nThe display will show that all control tests have \\npassed. \\n15 \\nSteps 2 – 14 are repeated except a \\ncontrol message is received instead of a \\nrecall message.  \\nThe FIP Tester logs show the received message \\nand displays a message saying that part of the \\ntesting is complete. \\n \\nThe display will show that all control tests have \\npassed. \\n16 \\nSteps 2 – 14 are repeated except a poll \\nmessage is received instead of a recall \\nmessage.  \\nThe FIP Tester logs show the received message \\nand display a message. \\n \\nThe display will show that all poll tests have \\npassed. \\n17 \\nA recall message is received at each valid \\nIXL. \\nThe FIP Tester logs show the received message \\nand displays a message that shows that the test \\nis complete. \\n \\nTest ends. \\n \\n\\n \\nPage | 23  \\n \\nFIP Test 3: Testing how new configurations are handled \\nDescription: Ensures that the FIP behaves correctly when a new IXL is initialised after the FIP is already \\nrunning. \\nPre-test actions:  \\n1) Restart the FIP. \\n2) Ensure the VM hosting the FIP Tester has the correct networking settings. \\n3) Run FIP Test 1 with 5 IXLs initialised. \\nTable 12 Test specifications for FIP test 3: Testing how new configurations are handled \\nStep No \\nDescription \\nExpected Result \\n1 \\nAfter FIP Test 1 has completed, allow \\ntime for 10 recall messages to be \\nreceived at each IXL.  \\nThe logs show that FIP Test 1 has completed \\nand 10 recall messages have been sent and \\nreplied to appropriately.  \\n2 \\nA new IXL is initialised. \\nThe start-up testing is repeated for all of the \\nconnected IXLs. \\n3 \\nA recall message is received at each \\nvalid IXL. \\nThe logs show the received message and \\ndisplay a message that shows that the test is \\ncomplete. Test ends. \\n4.1.3 \\nGUI Design \\nIn order to make the system easier to use, a GUI was designed using a two layer structure as \\nrecommended by Borisov et al. [25]. A sketch of the design is shown in Figure 11.  \\n \\nFigure 11 GUI design sketch \\nKey design decisions made based on the two-layer GUI design strategy [25] were: \\n\\uf0b7 \\nHaving a simple layout, \\n\\uf0b7 \\nIncorporating a “Help” button which opens the user guide, \\n\\uf0b7 \\nIncluding a pop-up dialogue box which asks the user to confirm all the settings, \\n\\uf0b7 \\nAllowing the user to control the complexity and type of testing that would run and \\n\\uf0b7 \\nMimicking the configurations requirements (Hosts file and configurations file) to the FIP’s \\nconfiguration requirements in order to minimise tester effort.  \\n\\n \\nPage | 24  \\n \\n4.1.4 \\nPerformance Assessment Design \\nTable 13 describes the tests that were performed to ensure that the FIP Tester was working as \\nexpected.  \\nTable 13 Tests to verify the FIP Tester’s behaviour \\nTest \\nNumber \\nTest Method \\nDescription \\nPurpose \\n1 \\n‘netcat’ and \\n‘tcpdump’ at \\nthe FIP \\nThe Linux commands ‘netcat’ was used to view \\nthe incoming and outgoing traffic on a \\nparticular IP and port in the FIP and the FIP \\nTester. \\n \\nAdditionally, the command ‘tcpdump’ was used \\nto ensure that all incoming and outgoing packet \\ncontents were correct \\nEnsured that the \\nnetworking setup is \\naccurate.  \\n2 \\nFIP Simulator \\nCreated a UDP client that connects to the FIP \\nTester on a particular IP address and port that \\nwould \\notherwise \\nhave \\nbeen \\nused \\nto \\ncommunicate with the FIP. \\n \\nA user then created messages in the Python \\nterminal and sent them to the FIP Tester as if \\nthe FIP was sending them. For the test cases \\noutlined in Table 10 to Table 12, the FIP’s \\nexpected replies were sent via the FIP \\nSimulator.  \\n \\nIn addition to those test cases, the user also \\nintroduced errors to the system and ensured \\nthat the FIP Tester responds correctly to those \\nerrors.   \\nEnsured that the FIP \\nTester’s behavior is \\naccurate.  \\n3 \\nManually \\ncheck FIP \\nTester Logs \\nAll of the FIP Tester’s activities during tests were \\nlogged in the log files.  \\n \\nA user manually checked the log files and \\nensure that the correct sequence of events, as \\ndefined by test cases shown in Table 10 to Table \\n12, are correct. \\nEnsured that the \\ntests were \\nrepeatable and \\ncorrect. \\n \\nIt was also used to \\nload test the FIP \\nTester.   \\n4.1.5 \\nDocumentation \\nThe documentation provided for the FIP – IXL Interface Testing Tool was based on the documentation \\nrequired by the CENELEC standard EN 50128:2011 [18]. It includes a user guide which provides details \\nfor initiating and running tests and a set of test specifications. \\n \\n\\n \\nPage | 25  \\n \\n4.2 TCS TESTING SUITE \\nIn order to achieve this project’s goals, it was decided that the work done on the existing TCS Testing \\nSuite would be built upon in the project. In terms of testing the TCS interfaces, the TCS Testing Suite \\nhad two main limitations.  \\nThese were: \\n\\uf0b7 \\nThe existing tests and helper functions did not have the capabilities necessary to run complex \\ntests. \\n\\uf0b7 \\nThe entire Testing Suite was complex and not user friendly. Therefore, it was difficult for a \\nuser to traverse through log files and identify a cause of failure.  \\nTherefore, the enhancements made to the TCS Testing Suite were focused on these two limitations.  \\n \\nFigure 12 High level TCS Testing Suite test engine design \\nFigure 12 shows a high level design of the existing test engine used in the TCS Testing Suite.  \\nOriginally, the testing tool design involved modifying the test engine so that it uses a hybrid framework \\nas recommended by Wang et al. [24]. This test tool would iterate through log files to automatically \\ngenerate test cases and then run the tests using the Test Engine shown in Figure 12.   \\nThis test engine would have combined a Log Parser with the test engine, such that the overall testing \\ntool could automatically identify the causes of error and then execute the relevant tests from the TCS \\nTesting Suite. It would also have replaced Squish with an open-source GUI automation testing tool to \\nsave future licencing costs. However, this design decision was rejected for the following reasons.  \\n\\uf0b7 \\nDeveloping the tool would require more time than was available in the project,  \\n\\uf0b7 \\nConsultation with engineers in the testing team revealed that the TCS Testing Suite would be \\neasier to maintain if the log parser and test engine were separate,  \\n\\uf0b7 \\nThe functionality required from the GUI automation testing tool was already implemented \\nwith Squish and  \\n\\uf0b7 \\nBackground research by Jönsson found that Squish is an effective tool for the required \\nfunctionality [27].  \\nInstead of modifying the test engine it was decided that all work done to enhance the TCS Testing \\nSuite would remain compatible with the existing test engine. Additionally, a Log Parser was designed \\nto reduce user effort when debugging tests. This section outlines the design process for both these \\nactivities. \\n\\n \\nPage | 26  \\n \\n4.2.1 \\nIncreasing Testing Capabilities \\nTo increase the testing capabilities, two main activities were conducted. These were to increase the \\ncapabilities of the helper functions and existing tests and to add more tests.  \\nIncreasing Capabilities of Helper Function and Existing Tests \\nThe TCS Testing Suite contains a library of helper functions and configurations that could be called \\nduring tests. But this library was only set-up to allow testing with certain configurations. For example, \\nonly a train with a specific Train ID could be created during tests because the configurations file and \\nhelper functions were hard coded to implement functionality for that Train ID. Therefore, the \\nfunctions and tests were modified so that they could refer to a larger amount of trains.  \\nAs an example, Figure 14 shows a screenshot of the existing code used to register a train. This code \\nwould generally be called at the start of each test to create and register a train in the system. It works \\nby: \\n1. Constructing the train sheet and train steps messages required to register a train. The helper \\nfunctions used to construct this message contain a list of parameters and corresponding values. If \\na user provides a value for a parameter, then the default value is overwritten by the user \\ngenerated value.  \\n2. Creating and activating the train by using the “CreateAndActivate()” helper function. This function \\nsends the train sheet and train steps message and then checks the Automation Server logs to \\nensure that the TCS has been provided with updated information. For example, it will ensure that \\na message has arrived that says that the train is active.  \\n3. Initialising the ATOC for that train.  \\n \\nFigure 13 Code showing a train being created before the enhancements \\nMessage construction \\nTrain sheet activation \\nATOC initialisation \\n\\n \\nPage | 27  \\n \\n \\nIn order to enhance this code so that it can be used to create multiple locomotives, or different \\nlocomotives, the following actions were taken. \\n\\uf0b7 \\nModifying the helper functions to remove references to specific configurations and \\n\\uf0b7 \\nCreating a new helper function which can be called during tests.  \\nAdding More Tests \\nSeveral tests from the TCS Integration Test Specifications document [19] had not been included in the \\nTesting Suite because more functionality needed to be added to the Testing Suite’s helper functions \\nbefore they could be run.  \\nTherefore, as a part of increasing the testing capabilities, several tests relevant to the TCS – ATOC, TCS \\n– VSS and TCS – RES interfaces were added to the system.  \\n4.2.2 \\nLog Parser \\n \\nFigure 14 High level log parser design \\nThe Log Parser was designed to help users identify errors in the system. It was developed in Python so \\nthat it can be included in the TCS Testing Suite’s existing libraries package. As outlined in Figure 14, it \\ndoes this by:  \\n1. Taking in a file which contains the address of all relevant log files. The log files may be from \\nthe actual AutoHaul® system or the testing system.  \\n2. Collating the critical, fatal and connection errors present in each of the log files.  \\n3. Outputting this information to the user in a compiled list.  \\nUsers can then use this information to identify sections of the source code which must be changed to \\nfix the errors. \\n4.2.3 \\nPerformance Assessment \\nTesting Enhancements \\nThe testing conducted for TCS interfaces must follow the guidelines set in the TCS Test Specifications \\ndocument [19]. Therefore, to verify that the enhancements made to the TCS system were working \\nproperly, the implemented tests were run. The user was able to visually verify that all of the steps and \\nthe expected outcomes of those steps were being performed.  \\n \\n\\n \\nPage | 28  \\n \\nLog Parser \\nBecause the Log Parser only collates the errors already present in log files to save the user time while \\ndebugging, the aim of the performance assessment was to ensure that it did not miss any errors and \\nthat it was actually useful to the testers at Hitachi.  \\nTo perform this assessment, a simple test to check whether a train had been properly registered in \\nthe CTC was run multiple times. Each time the test was run, a different error was introduced to the \\nsystem. A user then manually verified that all of the expected critical, fatal and connection errors \\nproduced by the system in response to these errors were present in the log files. The errors introduced \\nto the system were: \\n\\uf0b7 \\nDisabling TCS communications by disconnecting the Automation Server,  \\n\\uf0b7 \\nAttempting to create a train with invalid parameters and \\n\\uf0b7 \\nCommenting out configurations required to register a train. \\nThese errors were chosen because they resulted in multiple error messages appearing in multiple log \\nfiles as described in Section 0. They were also chosen because the changes made to the system could \\nbe rectified easily and would not break the functionality of the overall TCS Testing Suite. \\nAdditionally, a user who regularly uses the TCS Testing Suite for testing was given the compiled error \\nlist from the three scenarios listed above. The user was then asked to identify the cause of failure \\nbased on the error messages.  \\n4.2.4 \\nDocumentation \\nHitachi engineers are already familiar with the TCS Testing Suite and they have already undergone a \\nvigorous process to ensure that the tests in the TCS Integration Test Specifications document [19] \\ncomply with CENELEC standards [18]. Therefore, the documentation provided for the TCS Testing Suite \\nwas focused on ensuring that engineers know what functionality has been added to the TCS Testing \\nSuite and how they can use the Log Analyser.  \\n \\n \\n\\n \\nPage | 29  \\n \\n5 PROJECT OUTCOMES \\n5.1 FIP TESTER \\n5.1.1 \\nThe Testing Tool \\nThe designed FIP Tester has been implemented with the architecture shown in Figure 15. The \\narchitecture was modelled after the TCS Testing Suite’s architecture, which is provided in \\nAppendix G: TCS Testing Suite Architecture. It was intentionally made to be simple as this allowed it \\nto be easy to navigate and maintain for users who are not familiar with the Python programming. \\n \\nFigure 15 System architecture of FIP Tester \\nAs illustrated in Figure 16 the FIP Tester is primarily controlled by a Test Controller. This controller is \\nresponsible for: \\n\\uf0b7 \\nInitialising the GUI to get test configurations from the user,  \\n\\uf0b7 \\nInitialising the IXL simulators which connect to the FIP, \\n\\uf0b7 \\nPerforming all logging functions,  \\n\\uf0b7 \\nRunning and ending tests and \\n\\uf0b7 \\nDisplaying the test outcomes to the user.  \\nIn order to increase easy of maintenance and accessibility, the FIP Tester has been hosted on a VM \\nand uses the same networking set-up that exists between the SoCat and the FIP that is shown in Figure \\n4. Therefore, the FIP’s software does not need to be changed in order to use the FIP Tester. \\n \\nFigure 16 High level overview of designed FIP Tester \\n\\n \\nPage | 30  \\n \\n5.1.2 \\nThe GUI \\nThe GUI implemented to take user inputs and set test configurations is shown in Figure 17. The GUI \\nrequires the user to enter the number of IXLs that the simulator will create, a text file containing the \\nIP addresses, a text file containing the port numbers and the tests which they want to run.  \\n \\nFigure 17 GUI for the FIP Tester \\nTo reduce errors in interactions with users, a confirmation pop-up dialogue is created once the user \\nselects “Begin Test” This pop-up is shown in Figure 18. \\n \\nFigure 18 Confirmation pop-up for the FIP Tester \\n5.1.3 \\nTest Cases \\nCode was written to run through the three tests outlined in in Table 10, Table 11 and Table 12. A code \\nsnippet from the start-up testing procedure is provided in Appendix F: FIP Tester Tests.  \\nThe main limitation of the implemented code is that because it was written to be easy to use and \\nmaintain for users not familiar with Python programming, it is not memory efficient. \\n5.1.4 \\nPerformance Assessment \\nOutcomes for Test 1 (Using the Linux commands ‘tcpdump’ and ‘netcat’ in the FIP) \\nThe Linux command ‘tcpdump’ was used to verify that the packet contents were correct. The output \\nfrom one of the connections is shown in Figure 19. \\n\\n \\nPage | 31  \\n \\n \\nFigure 19 'tcpdump' output \\nOne of the key problems solved by using ‘tcpdump’ was that the FIP logs and documentation implied \\nthat individual elements of messages were surrounded with square brackets. For example, an \\nacknowledge message would be ”[f1][29][f6]”. However, using ‘tcpdump’ made it clear that this was \\nnot the case and the message was actually “f129f6”.  \\nFurthermore, the Linux command ‘netcat’ was used to verify that the messages were arriving at and \\nleaving from the correct IP address and port number. The output from one of these connections is \\nshown in Figure 20. This figure shows the initial recall message (“fd 2a c1 4f f6”) being sent from the \\nFIP to the IXL. Because no message is being sent back to the FIP, the recall message is being sent \\nrepeatedly.  \\n \\nFigure 20 Linux command ‘netcat’ output \\nUsing ‘netcat’ was highly effective in ensuring that the initial networking set-up was correct. A major \\nproblem encountered early in the project was that messages would be seen arriving correctly by \\n‘tcpdump’ but would not register as having arrived in the FIP software and therefore not show up in \\nthe FIP’s logs. Using ‘netcat’ it was found that the firewalls were preventing the data from actually \\n“reaching” its destination. The firewall rules were subsequently modified to allow the data to be \\ntransmitted correctly.  \\nOutcomes for Test 2 (Using a FIP Simulator) \\nA FIP Simulator was used to manually send messages that met the expected behaviour of the test \\ncases outlined in Table 10, Table 11 and Table 12. Figure 21 shows the replies sent by the FIP Tester \\nwhen all of the correct messages are sent from the FIP Simulator in the start-up testing sequence. The \\nlogs in the FIP Tester look exactly like Figure 24.  \\n\\n \\nPage | 32  \\n \\n \\nFigure 21 Python terminal output of test case 1 (Start-up sequence) with correct replies as seen by the FIP Simulator \\nFigure 22 shows the replies from the FIP Tester when an incorrect message is sent from the FIP \\nSimulator. As seen in the Python terminal output, the reply sent by the FIP Simulator after receiving a \\ncontrol message is incorrect. Therefore, the FIP Tester resends the control message. The \\ncorresponding log file in the FIP Tester is shown in Figure 23. \\n \\nFigure 22 Python terminal output of test case 1 (Start-up sequence) with incorrect replies as seen by the FIP Simulator \\n \\nFigure 23 FIP Tester log file when incorrect message is received \\nPlease note that for confidentiality purposes, the IP address and port number  that are present in the \\nlog file have been overwritten by the phrase “IP:Port” which is highlighted in blue. This also applies to \\nother figures in this report.  \\nOutcomes for Test 3 (Manually checking FIP Tester Logs) \\nThe logs for the FIP Tester were checked manually to ensure that the system’s behaviour matched the \\nexpected behaviour that is outlined in Table 10, Table 11 and Table 12. An example of a passing test \\nis shown in Figure 24. This is a test to verify the start-up message sequence. The corresponding results \\ntable is shown in Table 14.  \\n\\n \\nPage | 33  \\n \\n \\nFigure 24 FIP Tester log showing a passing test \\nTable 14 Results from manually checking the logs in Figure 24 \\nStep \\nNo \\nDescription \\nExpected Result \\nPass/ \\nFail \\n1 \\nFIP Tester begins running. \\nA GUI opens that asks the user to \\nenter test configurations. \\nNA \\n2 \\nA recall message is received at each valid \\nIXL. \\nThe logs show the received message \\nand display a message that shows \\nthat the recall test is passed. \\nPass \\n3 \\nAn indication message is sent by each IXL. \\nThe FIP should receive this message and \\nsend a control message back to each IXL. \\nThe logs show the received message \\nand display a message that shows \\nthat the control test is passed. \\nPass \\n4 \\nA slave acknowledge message is sent by \\neach IXL. The FIP should receive this \\nmessage and send a poll message back to \\nthe IXL. \\nThe logs show the received message \\nand display a message that shows \\nthat the poll test is passed. \\nPass \\n5 \\nAn indication message is sent by each IXL. \\nThe FIP should receive this message and \\nsend a recall message back to each IXL. \\nThe logs show the received message \\nand display a message that shows \\nthat the test is complete. \\n \\nTest ends. \\nPass \\nOverall Outcome: Pass \\n \\nSimilarly, Figure 25 and Table 15 show a failing test. In this test a connection to the IP address and port \\ncannot be established and therefore no recall message is received by FIP Tester. \\n \\n \\nFigure 25 FIP Tester log showing a failing test \\n \\n \\n\\n \\nPage | 34  \\n \\nTable 15 Results from manually checking the logs In Figure 25 \\nStep \\nNo \\nDescription \\nExpected Result \\nPass/ \\nFail \\n1 \\nFIP Tester begins running \\nA GUI opens that asks the user to enter test \\nconfigurations. \\nNA \\n2 \\nA recall message is received \\nat each valid IXL. \\nThe logs show the received message and display \\na message that shows that the recall test is \\npassed. \\nFail \\nOverall Outcome: Fail because connection with FIP could not be established \\n5.1.5 \\nDocumentation \\n \\nFigure 26 Screenshot from user guide \\nFigure 26 shows a screenshot from the user guide. The user guide provides instructions for: \\n\\uf0b7 \\nInstalling the FIP Tester,  \\n\\uf0b7 \\nSetting up the networking between the FIP Tester and the FIP and \\n\\uf0b7 \\nMaking modifications to the tests. \\n5.2 TCS TESTING SUITE  \\n5.2.1 \\nThe Enhanced TCS Testing Suite \\nAs described in Section 4.2.1, two main activities were conducted to enhance the TCS Testing Suite. \\nThese were to increase the capabilities of the helper functions and existing tests and to implement \\nmore tests from the TCS Test Specifications document [19]. This section describes the product from \\nperforming these activities.  \\n\\n \\nPage | 35  \\n \\nIncreasing the Capabilities of Helper Functions and Tests \\nA number of changes were made to helper functions and tests in order to increase their capabilities \\nand make them more useful for testing. These changes included: \\n\\uf0b7 \\nModifying all helper functions which were hardcoded to be specific to one Train ID to be \\ncompatible with multiple Train IDs and \\n\\uf0b7 \\nModifying tests to run with any train configurations.  \\nAs an example of these modifications, Figure 27 shows the enhanced version of code written during \\nthe project to create and register multiple trains when given an initial Train ID and the number of \\ntrains to create. It is based on the code shown in Figure 13. The changes made to the original code \\nwere: \\n\\uf0b7 \\nMoving the train creation functionality into a helper function. This allows users to modify \\nparameters easily. \\n\\uf0b7 \\nModifying the default parameters in the “BuildTrainSheetMessage()” and \\n“BuildTrainStepsMessage()” so that factors which limit the train configurations to one train \\n(such as train ID or train location) are no longer limiting. \\n\\uf0b7 \\nCreating ATOC helper functions, so that ATOC messages with non-default configurations can \\nbe sent. \\nFigure 27 has been annotated to show examples of these modifications.  \\n \\nFigure 27 Code showing the helper function implemented to create multiple trains \\n\\n \\nPage | 36  \\n \\n5.2.2 \\nThe Log Parser \\nIn the TCS Testing Suite, each AutoHaul® sub-system has a folder which contains its log files. Each time \\na test is run, the test runner migrates the most recent log to an archives folder and then creates a new \\nlog file for the sub-system.  \\nAs its input, the Log Parser takes a file containing the addresses of the folders which contain these \\nlogs.  Each time the Log Parser is run, it iterates through the most recent log file for each sub-system \\nand finds the critical, fatal and connections errors present in the file.  \\nThe Log Parser then collates the error messages into a single file. The errors are sorted by their file of \\norigin and then by their time stamp. An example output is shown in Figure 28.  \\n \\nFigure 28 Screenshot of the collated errors file \\nTo save memory, the compiled errors file gets overwritten each time the Log Parser is run. The user \\nhas the option of modifying a parameter to save the previous version of the file if required.  \\nA GUI was not designed for the Log Parser as the code is intended to be used by Hitachi testers who \\nare already familiar with the TCS Testing Suite and are proficient with Python coding. Hence, adding a \\nGUI would increase the time needed to use the Log Parser and therefore slow the testing efficiency.  \\nA major limitation of the Log Parser is that it only searches for three types of errors. However, a lot of \\nerrors, particularly those seen during deployment at site, are not registered as errors by the AutoHaul® \\nsystem and will not show up in the logs. Therefore, this tool is only useful as a quick debugging aid to \\nnarrow down the causes of error.  \\n5.2.3 \\nPerformance Assessment \\nTCS Testing Enhancements \\nTo verify that the tests written to enhance the testing capabilities of the TCS Testing Suite were \\nworking correctly, the user ran tests and visually observed the screen to ensure that the expected \\nevents were occurring as per the TCS Integration Test Specification document [19]. \\nWhere events occurred in the background and could not be visually verified, print statements were \\nused to print the status of elements that needed to be checked. Figure 29 shows an example of a test \\nthat was verified using this method. Testing outcomes and notes have been written in the pass/fail \\ncolumn. \\n\\n \\nPage | 37  \\n \\n \\nFigure 29 Example of a test case used to verify test behaviour with pass/fail comments \\nTCS – RES Interface \\nDue to uncontrollable factors, the server hosting the RES was disabled in May 2020. Therefore, the \\nTCS Testing Suite was not able to communicate with the RES and the code implemented was not able \\nto be verified. Once RES functionalities resume, Hitachi engineers will be able to test the code written \\nin this project.  \\nLog Parser \\nTo test the log parser, errors were intentionally introduced to the system and the combined log file \\nwas checked to ensure that all the errors appeared in it.  \\nA tester at Hitachi, who had written several of the tests in the TCS Testing Suite, was asked to identify \\nthe cause of error based on the compiled error reports. Table 16 shows the results of this testing. \\n \\n \\nPass \\nPass - Used \\nprint \\nstatement to \\nsee the \\nmessage \\nPass \\n\\n \\nPage | 38  \\n \\n \\nTable 16 Results of asking a tester to debug using the Log Parser \\nTest Type \\nWas The Error \\nIdentified? \\nTester Comments \\nDisconnected the \\nAutomation Server \\nYes \\nOther connection errors, such as time synchronization \\nissues between 2 sub-systems or incorrect port \\nconfigurations also produce the same error message as a \\ndisconnected system in the logs.  \\n \\nAs such, the tester identified a disconnected Automation \\nServer as the most likely cause of the error but indicated \\nthat they would have to do more testing to rule out \\nother potential causes.  \\nInvalid train \\nparameters \\nYes \\n \\nRemoving steps \\nrequired to create \\na train \\nYes \\nIdentifying the error required the tester to go through \\nthe code. \\n \\n \\n \\n\\n \\nPage | 39  \\n \\n6 CONCLUSION AND RECOMMENDATIONS \\nIn conclusion, this project designed and developed an integrated testing framework for automating \\nthe testing of four communication interfaces used within the AutoHaul® project. The conclusions of \\nthe project are outlined below.  \\n6.1 FIP TESTER \\nA testing tool called the FIP Tester was designed to simulate IXL and send IXL messages to the FIP in \\norder to test that the FIP behaves correctly when responding to these IXL messages. This testing tool \\nwas designed to replace the manual testing done when an end-to-end change, such as adding or \\nremoving an IXL device, was made to the AutoHaul® system. The tool is effective in performing this \\ntask.  \\nHowever, because the FIP Tester does not test the FIP’s behaviour as a response to TCS requests, it \\nhas limited functionality at Hitachi. Therefore, future work could be done to create a TCS Simulator \\nwithin the FIP Tester. This could be used to verify that the FIP responds to TCS messages correctly. \\nFurthermore, the scope of the project did not include sending correct interlocking data to the FIP. \\nTherefore, the developed IXL Simulators inside the FIP Tester only send “dummy” interlocking data to \\nthe FIP. In order to make the testing more useful for Hitachi, the existing Microlok Interlocking \\nSimulation System (MISS) testing tool could be integrated with the FIP Tester. Combined with a TCS \\nSimulator and integration with MISS, the FIP Tester would allow Hitachi to test the entire journey of \\nthe data from the IXL devices to the FIP and finally to the TCS.  \\n6.2 TCS TESTING SUITE \\nThe project covered the testing for three TCS communication interfaces. These were the TCS – ATOC \\ninterface, the TCS – RES interface and the TCS – VSS interface. \\nPrior to the project, a TCS Testing Suite had been developed in Hitachi to automate basic tests. During \\nthis project, the TCS Testing Suite’s functionality was enhanced so that it could autonomously perform \\ntests for these three interfaces. This was done by: \\n\\uf0b7 \\nModifying the helper functions to allow more complex test scenarios to be created,  \\n\\uf0b7 \\nModifying the existing tests to allow them to perform more complex tests and  \\n\\uf0b7 \\nWriting more tests based on Hitachi’s existing test specifications.  \\nThe TCS Testing Suite is a complex software tool that is difficult to navigate and debugging.  To address \\nthis issue, a Log Parser was developed which scanned through various log files generated by a test and \\ncollated the errors into a single file. This made it easier for the user to debug the issue.  \\nThe main limitation of the TCS Testing Suite is that both the helper functions and the tests must be \\nconstantly updated as the AutoHaul® requirements change. Therefore, it requires a lot of maintenance \\nin-order for it to remain useful at Hitachi. To address this issue, work can be done to make the tests \\nand the helper functions more modular. This would significantly reduce the workload that arises when \\nthe system has to be updated as only the helper function would need to be updated.  \\n\\n \\nPage | 40  \\n \\nAdditionally, future work could also include further enhancing the TCS Testing Suite so that it can \\nperform automated testing for all of the TCS interfaces that were excluded from the scope in this \\nproject. \\n6.3 PROJECT IMPROVEMENTS \\nThis project could have been improved by better documentation and project management strategies. \\nIn response to the COVID-19 pandemic, the order in which project activities were conducted was \\nchanged in March 2020. At that stage, work had primarily focused on enhancing the TCS Testing Suite \\nand very little progress had been made on developing a FIP Tester. Due to the seemingly high \\nlikelihood of server access being lost, the project’s focus was switched to the FIP Tester, as that was \\nthe project’s priority. However, progress made on the TCS Testing Suite was not properly documented \\nwhen the project’s priority changed. Therefore, when the project switched back to enhancing the TCS \\nTesting Suite weeks later, a lot of knowledge had to be re-learnt.  \\nFurthermore, Hitachi had implemented updates to the TCS Testing Suite and included new \\ncomponents. Therefore, a lot of the previously written tests were outdated and work had to be re-\\nimplemented. \\nA lot of time could have been saved if the progress made initially had been documented better.  \\n  \\n \\n \\n\\n \\nPage | 41  \\n \\n7 REFERENCES \\n \\n[1]  K. Smith, “Rise of the machines Rio Tinto breaks new ground with AutoHaul,” International \\nRailway Journal, vol. 59, no. 8, pp. 14-18, 2019.  \\n[2]  Mining Media International, “Rio Tinto Achieves First Delivery of Iron Ore With World's Largest \\nRobot,” Engineering and Mining Journal, vol. 219, pp. 4-6, 2018.  \\n[3]  Andrew Stewart , “AutoHaul System Architecture and Design Specification,” Hitachi Rail STS, \\nBrisbane, 2016. \\n[4]  Andrew Stewart, “TCS – ATOC Interface Control Document,” Hitachi Rail STS, Perth, 2017. \\n[5]  Andrew Stewart, “Automation Server Subsystem Requirements Specification,” Hitachi Rail STS, \\nBrisbane, 2019. \\n[6]  Anthony MacDonald, “TCS-Wayside Interface Control Description,” Hitachi Rail STS, Brisbane, \\n2013. \\n[7]  Graeme Reid, “TCS - PRCCI Interface Control,” Hitachi Rail STS, Brisbane, 2020. \\n[8]  T. Rowbotham, “Interlocking,” in Introduction to Signalling, London, Institution of Railway Signal \\nEngineers, 2000.  \\n[9]  Lionel Van Den Berg, “TCS Architecture And Design Specifications,” Hitachi Rail STS, Brisbane, \\n2019. \\n[10] Andrew Stewart, “CTC – Automation Interface Control Document,” Hitachi Rail STS, Perth, 2018. \\n[11] Samuel Dekker, “AUTOMATION MMI User Manual For Train Controllers,” Hitachi Rail STS, Perth, \\n2020. \\n[12] Andrew Stewart, “TCS - VSS Interface Control Document,” Hitachi Rail STS, Perth, 2016. \\n[13] Kent Yip, “Fiber Interface Processor,” Hitachi Rail STS, Perth, 2008. \\n[14] Andrew Stewart, “TCS – RTIO External Systems Interface Control Document,” Hitachi Rail STS, \\nPerth, 2017. \\n[15] Stephane Joubert, “Genisys Protocol Description,” Hitachi Rail STS, Perth, 2013. \\n[16] P. Wigger, “Experience with Safety Integrity Level (SIL) Allocation in Railway Applications,” \\nInstitute for Software, Electronics, Railroad Technology (ISEB), Cologne, 2001. \\n[17] U. Silchanka, “FIP Test Manager,” Hitachi Rail STS, Perth, 2008. \\n\\n \\nPage | 42  \\n \\n[18] Cenelec, “Railway Applications - Communication Signalling and Processing Systems - Software \\nfor Railway Control and Protection Systems,” Cenelec, Brussels, 2011. \\n[19] Michal Cedrych, “TCS Integration Test Specification,” Hitachi Rail STS, Brisbane, 2019. \\n[20] Froglogic, “Squish,” Froglogic, 2020. [Online]. Available: https://www.froglogic.com/squish/. \\n[Accessed 28 February 2020]. \\n[21] H. Bajaj, “Choosing The Right Automation Tool and Framework Is Critical To Project Success,” \\nInfosys, Bengaluru, 2018. \\n[22] M. A. Umar and C. Zhanfang, “A Study of Automated Software Testing: Automation Tools and \\nFrameworks,” International Journal of Computer Science Engineering (IJCSE), vol. 8, pp. 217-225, \\nDecember 2019.  \\n[23] A. Méndez-Porras, M. N. Hidalgo, J. M. García-Chamizo, M. Jenkins and A. M. Porras, “A Top-\\nDown Design Approach for an Automated Testing Framework,” in International Conference on \\nUbiquitous Computing and Ambient Intelligence, Springer, 2015.  \\n[24] Y. Wang, L. Chen, D. Kirkwood, P. Fu, J. Lv and C. Roberts, “Hybrid Online Model-Based Testing \\nfor Communication-Based Train Control Systems,” IEEE Intelligent Transportation Systems \\nMagazine, vol. 10, no. 3, pp. 35-47, 2018.  \\n[25] N. Borisov, A. Kluge, W. Luther and B. Weyers, “User Interface Design for Test and Diagnosis \\nSoftware in Automotive Production Environments,” in International Conference on Ubiquitous \\nComputing and Ambient Intelligence, Springer, 2014.  \\n[26] M. Polo, P. Reales, M. Piattini and C. Ebert, “Test Automation,” IEEE Software, vol. 30, no. 1, pp. \\n84-89, Jan 2013.  \\n[27] T. Jönsson, “Efficiency determination of automated techniques for GUI testing,” School of \\nEngineering in Jönköping, Jönköping, 2014. \\n[28] Smart \\nBear, \\n“Test \\nComplete,” \\nSmart \\nBear, \\n2020. \\n[Online]. \\nAvailable: \\nhttps://smartbear.com/product/testcomplete/overview/. [Accessed 3 June 2020]. \\n[29] A. Adlemos, H. Tan and V. Tarasov, “Test case quality as perceived in Sweden,” in 2018 ACM/IEEE \\n5th International Workshop on Requirements Engineering and Testing, ACM, 2018.  \\n[30] D. Freudenstein, J. Radduenz, M. Junker and S. Eder, “Automated test-design from \\nrequirements: the Specmate tool,” in 5th International Workshop on Requirements Engineering \\nand Testin, ACM, 2018.  \\n[31] A. Méndez-Porras, M. N. Hidalgo, J. M. García-Chamizo, M. Jenkins and A. M. Porras, “A Top-\\nDown Design Approach for an Automated Testing Framework,” in International Conference on \\nUbiquitous Computing and Ambient Intelligence, Cham, 2015.  \\n[32] B. Mountford, “Testing Procedure For TCS Test Suite,” Hitachi Rail STS, Perth, 2020. \\n\\n \\nPage | 43  \\n \\n[33] Andrew Stewart, “VICS Interface Control Document,” Hitachi Rail STS, Perth, 2016. \\n[34] European Committee for Electrotechnical Standardization, “Railway applications – \\nCommunication, signalling and process systems – Software for railway control and protection \\nsystems,” CENELEC, Brussels, 2011. \\n \\n \\n \\n \\n\\n \\nPage | 44  \\n \\n8 APPENDIX A: PROJECT MANAGEMENT SUMMARY \\n8.1 PROJECT TIMELINE AND RESOURCES \\nThe overall project has been broken down into four stages – background research, design, \\nimplementation and documentation. More details about each stage and the resources required for \\nthat stage are provided in Sections 8.1.1 to 8.1.4. The project timeline is provided in Section 8.1.5.   \\n8.1.1 \\nBackground Research Stage \\nTime Period: Week 1 – Week 6 (6 weeks). \\nIn order to understand the full context of the tests that need to be automated, background research \\nwas conducted. This research focused on: \\n\\uf0b7 \\nHow railway systems in general operate. \\n\\uf0b7 \\nThe architecture, design specifications and testing protocols of the AutoHaul® project. \\n8.1.2 \\nDesign Stage \\nTime Period: Week 5 – Week 8 (4 weeks). \\nThe design stage of the project was completed by using the following steps. \\n1. Understanding the existing systems and their advantages and weaknesses.  \\n2. Researching alternative technologies to replace or supplement the existing tools. These \\ntechnologies were required to be open-source and capable of running on an air-gapped \\nsystem.  \\n3. Designing test engines for testing the TCS interfaces and FIP communications. \\n4. Reviewing the test engine with the project supervisor.  \\n8.1.3 \\nImplementation Stage \\nTime Period: Week 8 – Week 19 (11 weeks). \\nThe implementation stage was the longest stage in the project. It focused on implementing a test \\nengine and then verifying that the test engine was reliable. The implementation stage was completed \\nby using the following steps. \\n1. Obtaining initial test data for the TCS and the FIP.  \\n2. Testing basic scenarios for TCS and the FIP. These scenarios were simple and easy to \\nimplement, for example creating a train sheet in the system. The purpose of these tests was \\nto validate the design of the test engine.  \\n3. Revising the test engine as needed. \\n4. Create a testing protocol for the FIP testing. \\n5. Coding tests for the FIP. This included verifying that the tests yield correct results. \\n6. Coding the tests from the pre-existing TCS Integration testing protocol. This included verifying \\nthat the coded tests are performing correctly.  \\n\\n \\nPage | 45  \\n \\nInitially, the TCS testing was to be carried out first as it required more input from other engineers to \\nset-up. However, due to the COVID-19 pandemic, the focus shifted to completing the FIP testing as it \\nwas the project’s priority.  \\n8.1.4 \\nDocumentation Stage \\nTime Period: Intermittent during Week 4 – Week 23 (19 weeks). \\nThe documentation stage focused on presenting the final solution and all of its relevant details. This \\nincluded creating documentation at Hitachi, to be used by engineers working on the AutoHaul® project \\nand documentation for university. The tasks and the time spent on each documentation activity are \\nshown below. \\nFor Hitachi (Week 19 – Week 20 (2 weeks)): \\n\\uf0b7 \\nThe software solution’s design specifications (Week 19), \\n\\uf0b7 \\nThe software solution’s test procedure (Week 19) and \\n\\uf0b7 \\nUser guide (Week 20). \\nFurthermore, for university assessment, the following items are still required to be completed: \\n\\uf0b7 Monthly reflection journals (0.5 days per journal), \\n\\uf0b7 Project Proposal (Week 7 – Week 8), \\n\\uf0b7 Interim report (Week 14 – Week 15),  \\n\\uf0b7 Oral presentation (Week 20 – Week 21) and \\n\\uf0b7 Final report (Week 19 – Week 23). \\nThe original timeline that was submitted in the Project Proposal was modified so that the final report \\nwas allocated an extra two weeks of time. This was done to ensure that there would be ample time \\nfor the document to undergo a review at Hitachi to ensure that no confidential material is being \\npublished.  \\n8.1.5 \\nProject Timeline \\nThe expected project timeline is presented below as a Gantt chart in Figure 30. The tasks to be \\ncompleted are sorted by the stages described in Sections 8.1.1 to 8.1.4. The completion dates for the \\ntasks in the chart acted as milestones for the project.\\n\\n \\nPage | 46  \\n \\n \\n \\nFigure 30 Project timeline\\n\\n \\nPage | 47  \\n \\n8.2 RISK ANALYSIS \\nThe risks for this project were classified as, Operational Health & Safety (OH&S) risks and project \\nscheduling risks. The risks and relevant mitigation actions are detailed below.  The risks have been \\nassigned a residual risk level based on the risk matrix in Section 8.2.3. \\nSince the Project Proposal, the only major change to the risk assessment was to accommodate for \\nrisks posed by the COVID-19 pandemic. \\n8.2.1 \\nOperational Health And Safety Risks \\nThe work being conducted in this project was done in the Hitachi office which was a low risk \\nenvironment.  \\nTable 17 OH&S risks summary \\nHazardous \\nActivity \\nConsequences \\nResidual \\nRisk Level \\nPreventative Tasks \\nMitigation \\nMethods \\nPotential \\nexposure to \\nCOVID-19 and \\nother viruses \\nGetting sick \\nHigh \\n\\uf0b7 Practice the recommended \\npreventative tasks such as \\nmaintaining social distancing \\nand good hygiene practices. \\n\\uf0b7 Use the same office desk and \\nequipment. \\n\\uf0b7 Alert office \\nmembers \\nimmediately.  \\n \\n\\uf0b7 If required, \\nparts of \\ntesting the \\nTCS \\ninterfaces will \\nbe removed \\nfrom the \\nscope of the \\nproject.   \\nUsing testing \\nequipment, \\nwhich has \\nmultiple large \\nmonitors, for \\nextended \\nperiods \\nAches and \\ncramps on \\nneck and back \\nmuscles \\nMedium \\n\\uf0b7 Take regular breaks and \\nperform stretches. \\n\\uf0b7 Stand where possible rather \\nthan tilting neck upwards. \\n\\uf0b7 Rest well and \\nperform non-\\ntest bed \\nrelated tasks \\nfor a few \\ndays. \\nComputer \\nusage for \\nextended \\ntime periods \\nEye strain \\nLow \\n\\uf0b7 Ensure lighting and screen \\npositioning is optimal. \\n\\uf0b7 Take regular breaks and \\nperform eye exercises \\n\\uf0b7 Use \\nlubricating \\neye drops \\ndaily and take \\nrest. \\nTyping for \\nextended \\nperiods of \\ntime \\nRepetitive \\nstrain injuries \\nto wrist \\nLow \\n\\uf0b7 Use an ergonomic keyboard \\nand perform wrist exercises at \\nregular intervals during the \\nday. \\n\\uf0b7 Rest well and \\nuse wrist \\nsupporting. \\nSlips, trips \\nand falls \\nSkin injuries or \\nbroken bones \\nLow \\n\\uf0b7 Remain observant to \\nsurroundings when moving \\naround the office. \\n \\n\\uf0b7 Apply first aid \\nfrom the \\noffice and \\ncontact the \\nHSE officer.  \\n \\n\\n \\nPage | 48  \\n \\n8.2.2 \\nProject Scheduling Risks \\nThese are risks which may cause the project to not be delivered in its ideal form.  \\nTable 18 Scheduling risks summary \\nHazardous \\nActivity \\nConsequences \\nResidual \\nRisk Level \\nPreventative Tasks \\nMitigation Methods \\nThe office is \\nshut down due  \\nto COVID-19 \\nCannot access \\nVMs on \\nservers at \\nHitachi and \\ntherefore \\nsections of \\nwork cannot \\nbe completed \\non time or to \\nrequired \\nstandards.  \\nHigh \\n\\uf0b7 Practice \\nrecommended \\nprevention \\npractices to reduce \\nthe spread of \\nCOVID-19.  \\n\\uf0b7 Install VMWare \\nWorkstation on a \\nlaptop and export \\nVMs from the server \\nonto it. \\n \\n\\uf0b7 If required, parts of \\ntesting the TCS \\ninterfaces will be \\nremoved from the \\nscope of the project.   \\nSickness \\nDelays to the \\nproject. \\nHigh \\n\\uf0b7 Practice \\nrecommended \\nprevention \\npractices to avoid \\nCOVID-19 and \\nother illnesses.   \\n\\uf0b7 Notify supervisor and \\nwork efficiently on \\nreturn.  \\n \\n\\uf0b7 Determine sections \\nof the project to \\npriorities completing \\nif required. \\nDelayed access \\nto required \\nresources \\nSections of \\nwork cannot \\nbe started on \\ntime. \\nMedium \\n\\uf0b7 Plan ahead and \\nrequest access to \\nresources in \\nadvance to \\nrequiring to use \\nthem.  \\n\\uf0b7 Focus on different \\nsections of the \\nproject while waiting \\nfor particular \\nresources.  \\nData losses \\nWork will \\nneed to be \\nrepeated. \\nMedium \\n\\uf0b7 Make weekly \\nbackup and \\ndocument \\nprogress made. \\n\\uf0b7 Revert to the last \\nbackup and continue \\nwork.  \\nProcrastination \\nor mental \\nslumps  \\nDelays to the \\nproject. \\nLow \\n\\uf0b7 Set weekly goals \\nand use \\naccountability \\ntools to track \\nprogress.  \\n\\uf0b7 Work efficiently at \\nother times.  \\n\\n \\nPage | 49  \\n \\n8.2.3 \\nRisk Matrix \\nTable 19 Risk matrix \\n \\n \\n \\nCONSEQUENCE \\n \\n  \\n  \\nNegligible (1) \\nMinor (2) \\nModerate (3) \\nSignificant (4) \\nCatastrophic (5) \\n \\nOH&S \\nFirst aid only \\nMedical treatment \\nrequired \\nProlonged \\nhospitalisation \\nrequired \\nPermanent injury \\nFatality  \\nScheduling \\nNo significant \\ndelays to the \\nproject \\nMinor delays with \\nno lasting impact \\nto the project \\nDelays requiring \\nsignificant effort \\nto recover \\nSections of the \\nproject will not be \\ncompleted \\nProject cannot be \\ncompleted \\nLIKELIHOOD \\nCertain (5) \\nFrequent occurrence \\n(once a week) \\nMedium (11) \\nHigh (16) \\nHigh (20) \\nExtreme (23) \\nExtreme (25) \\nLikely (4) \\nLikely to occur     (once \\na month) \\nLow (7) \\nMedium (12) \\nHigh (17) \\nExtreme (21) \\nExtreme (24) \\nPossible (3) \\nPossible to occur (once \\nevery 6 months)  \\nLow (4) \\nMedium (8) \\nMedium (13) \\nHigh (18) \\nExtreme (22) \\nUnlikely (2) \\nUnlikely to occur (once \\na year) \\nLow (2) \\nLow (5) \\nMedium (9) \\nMedium (14) \\nHigh (19) \\nRare (1) \\nPractically impossible \\n(once in 10 years) \\nLow (1) \\nLow (3) \\nLow (6) \\nMedium (10) \\nHigh (15) \\nRESIDUAL RISK LEVEL AND LEVEL OF AUTHORITY INVOLVEMENT \\nLow Risk \\nMedium Risk \\nHigh Risk \\nExtreme Risk \\n1 to 7 \\n7 to 14 \\n15 to 20 \\n20 to 25 \\nNo authority needed \\nTeam leader \\nAcademic/ Hitachi Supervisor \\nCourse Coordinator \\n\\n \\nPage | 50  \\n \\n8.3 PROJECT DELIVERABLES OUTCOMES \\nAs outlined in Table 7, this project had 9 deliverables. The outcomes of these deliverables are outlined \\nin Table 20. \\nTable 20 Outcomes of key project deliverables \\nDeliverable \\nOutcomes \\n1. Software Solution \\nTwo separate software solutions were developed. The first built on the \\nexisting TCS Testing Suite and the second developed a Test Suite for \\ntesting the FIP’s behavior via the FIP – IXL interface.  \\n2. Architecture Design \\nThe architecture design for both software solutions has been provided \\nin this report.  \\n3. Test Procedure \\nA document containing the test procedure has been submitted to \\nHitachi.  \\n4. User Manual \\nA user manual has been submitted to Hitachi. \\n5. Reflective Journals \\n6. Project Proposal \\n7. Interim Report \\nThese deliverables were submitted to UQ at various stages of the \\nsemester. Figure 30 contains the full details of the submission times.  \\n8. Oral Presentation \\nAn oral presentation which summarised the project was delivered on the \\n11th of June 2020.  \\n9. Final Report \\nThis report is the final report. \\n \\n8.4 OPPORTUNITIES \\nThe major project opportunities of this project are provided in Table 21. \\nTable 21 Project opportunities \\nOpportunity \\nDescription \\nFurther expanding the \\nTCS Testing Suite \\nAs a part of this project, the existing TCS Testing Suite was modified to \\nmake it easier to perform more complicated operations, such as \\nregistering multiple locomotives. Therefore, it will be possible to add \\nadditional tests which can be used to test other TCS functionality and use \\nother TCS interfaces.  \\nTesting \\nthe \\nFIP’s \\nbehavior via a TCS – \\nFIP interface.  \\nThis project focused on testing the FIP’s behavior by focusing on the \\ncommunications between IXL devices. Now that this test suite exists, it is \\nrelatively easy to enhance this test suite so that it includes a TCS simulator \\nas well. This will make it easier to verify that the FIP is behaving correctly \\nto TCS requests and allowing the FIP’s behavior to be validated from the \\nTCS side of operations.  \\n \\n \\n \\n\\n \\nPage | 51  \\n \\n9 APPENDIX B: FIP MESSAGES \\nGenisys Protocol Message Structure \\nMessages sent over the Genisys protocol have five components. All elements are sent as hexadecimal \\ncharacters. The five components of messages are [15]: \\n1. Control character – a character which denotes the type of message being sent.  \\n2. Station address – The address of the station where the IXL is located as a hexadecimal value. \\nA message sent from an IXL would contain its own station number and a message sent from \\nthe FIP would contain the station number of the target IXL.  \\n3. Data bytes – The interlocking data being sent for the TCS to use. Depending on the message \\ntype, data bytes may not be sent.  \\n4. Security checksum – A two byte Cyclic Redundancy Check (CRC) of all message bytes, up to \\nbut not including, the security checksum. Depending on the message type, a security \\nchecksum may not be sent. \\n5. Termination character – The character “f6”.  \\nTable 22 provides more details about each message type. \\nTable 22 FIP – IXL Message types and corresponding control characters [15] \\nMessage Type \\nControl \\nCharacter \\nDescription \\nFIP to IXL Messages \\nMaster Acknowledge Message fa \\nUsed to acknowledge a message and to act as a poll \\nmessage.  \\nPoll Message \\nfb \\nUsed to ask the IXL if it has any new indications it \\nwishes to report. It receives an indication message if \\nthe IXL does wish to send updated data or it receives \\na slave acknowledgement.  \\nControl Command \\nfc \\nUsed to send controls. \\nRecall Indication Command \\nfd \\nRequests the IXL to send all its indications to the FIP. \\nExecute Controls Command \\nfe \\nCauses the IXL to write the controls to its database. \\nIXL to FIP Messages \\nSlave Acknowledge Message \\nf1 \\nSent as a response when no other response is \\nneeded. \\nIndication \\nData \\nResponse \\nMessage \\nf2 \\nUsed to send data to the FIP. \\nControl Checkback Message \\nf3 \\nUsed to verify the controls from the FIP when in \\ncheckback control mode.  \\n \\nAs an example, a slave acknowledge message from station 80 would be sent as “f150f6”. \\n \\n \\n\\n \\nPage | 52  \\n \\n10  APPENDIX C: TCS MESSAGE PROTOCOLS \\n10.1 TCS – ATOC COMMUNICATION PROTOCOL \\nThe message structure for all TCS – ATOC messages is described in Table 23. \\nTable 23 TCS – ATOC message protocol [4] \\nField \\nDescription \\nSequence number \\nA number used to order the messages \\nTime stamp \\nMessage time stamp \\nProtocol version \\nAn identifier for the protocol version \\nSource ID \\nAn identifier for the sender of the message \\nDestination ID \\nAn identifier for the receiver of the message \\nLength \\nThe length of the data being sent. Varies according to the \\ndata. \\nData \\nThe data being communicated between the sub-systems. Is \\nvariable in length. \\nCyclic redundancy checks (CRC) \\nUses CRC-32 \\n10.2 TCS – RES COMMUNICATION PROTOCOL \\nTCS – RTIO External Systems messages all comply with the XML schema. The element and attribute \\nnames depends on the type of message being sent. In general, the elements include: \\n\\uf0b7 \\nMessage type, \\n\\uf0b7 \\nTime stamp and \\n\\uf0b7 \\nMessage details. \\nThere are 12 types of messages that can be sent from the TCS to RTIO External Systems. These are: \\n\\uf0b7 \\nTrain Sheet, \\n\\uf0b7 \\nTrain Step, \\n\\uf0b7 \\nTrack Block, \\n\\uf0b7 \\nFleeting, \\n\\uf0b7 \\nTrack Notification, \\n\\uf0b7 \\nTrain Notification, \\n\\uf0b7 \\nTrain Position, \\n\\uf0b7 \\nHiRail Track Machine Position, \\n\\uf0b7 \\nPoint Tag, \\n\\uf0b7 \\nTurnout Status, \\n\\uf0b7 \\nTemporary Speed Restrictions and \\n\\uf0b7 \\nTrain Speed Restrictions. \\nAdditionally, there are 3 messages that RTIO External Systems might send to the TCS. These are: \\n\\uf0b7 \\nRefresh, \\n\\uf0b7 \\nExecute Schedule and \\n\\uf0b7 \\nNotification. \\n \\n \\n\\n \\nPage | 53  \\n \\n10.3 TCS – VSS COMMUNICATION PROTOCOL \\nThe message structure for all TCS – VSS messages is described in Table 24. \\nTable 24 TCS – VSS message protocol [12] \\nField \\nDescription \\nSource ID \\nAn identifier for the sender of the message. \\nDestination ID \\nAn identifier for the receiver of the message. \\nProtocol version \\nAn identifier for the protocol version. \\nVSS Status \\nCan be Master or Standby. \\nTime stamp \\nMessage time stamp. \\nLength \\nThe length of the data being sent. Varies according to the \\ndata. \\nData \\nThe data being communicated between the sub-systems. Is \\nvariable in length. \\nCyclic redundancy checks (CRC) \\nUses CRC-32. \\n \\n \\n\\n \\nPage | 54  \\n \\n11  APPENDIX D: TEST BENCH SET-UP \\n \\n \\nFigure 31 Test bench set-up in Brisbane \\n \\n \\n \\n \\n \\n \\n \\nTest Management \\nMonitors \\nHuman-Machine \\nInterfaces \\n\\n \\nPage | 55  \\n \\n12 APPENDIX E: FIP TESTER WORKFLOW DIAGRAMS \\n \\nFigure 32 Flowchart of process to test initial testing  \\nsequences \\n \\n\\n \\nPage | 56  \\n \\n \\nFigure 33 Flowchart of process to test site-like \\ntransmission errors \\n \\n\\n \\nPage | 57  \\n \\n13 APPENDIX F: FIP TESTER TESTS \\n \\nFigure 34 Start-up test code snippet \\n \\n \\n \\n \\n\\n \\nPage | 58  \\n \\n14 APPENDIX G: TCS TESTING SUITE ARCHITECTURE \\n \\nFigure 35 TCS Testing Suite architecture diagram [32] \\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64a8cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124555"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "file_path = 'Uniersity Queensland AHE Thesis.pdf'\n",
    "md_text = pymupdf4llm.to_markdown(file_path)\n",
    "\n",
    "import pathlib\n",
    "pathlib.Path(\"UQ AHE Thesis.md\").write_bytes(md_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce597deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='_Faculty of Engineering, Architecture and Information Technology_' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 3': 'Enhanced Rail Test Automation', 'Header 4': 'Student Name: Isha, JOSHI Course Code: ENGG7290 Supervisor: Graeme Smith – Associate Professor School of Information Technology and Electrical Engineering Submission date: 25 [th] June 2020'}\n",
      "\n",
      "\n",
      "page_content='Hitachi Rail STS (Hitachi) has been contracted to deliver and maintain an Automated Train Operation  \n",
      "(ATO) system for Rio Tinto Iron Ore as a part of their AutoHaul® project. This ATO system facilitates  \n",
      "the driverless movement of trains in a railway network in Western Australia. Because of its safety  \n",
      "critical nature, any modifications made to the AutoHaul® system require extensive testing before they  \n",
      "can be rolled out. Presently, this testing is performed manually and uses a lot of testing time and  \n",
      "resources. Hitachi has commissioned the design and development of an automated testing tool which  \n",
      "can be used to reduce the testing time and improve testing efficiency. The testing would focus on the  \n",
      "communication interfaces between various AutoHaul® sub-systems.  \n",
      "The primary aim of this project was to design and develop an integrated testing framework to support  \n",
      "the automated testing of the AutoHaul® project. The testing focused on testing the behaviour of the  \n",
      "Fiber Interface Processor (FIP) and the Train Control Sub-system (TCS) by testing the communication  \n",
      "interfaces between these sub-systems and other AutoHaul® sub-systems.  \n",
      "This report provides an overview of the AutoHaul® system and details about the specific interfaces  \n",
      "that testing is being automated for. A review of literature relevant to designing an automated testing  \n",
      "framework is also present.  \n",
      "This report also delivers a summary of the design and outcomes of the tool used to test the FIP’s  \n",
      "behaviour. This tool, called the FIP Tester, simulates the behaviour of the Interlocking (IXL) devices  \n",
      "that connect to the FIP. It verifies that the FIP is responding correctly to messages sent by the IXL  \n",
      "simulators in order to ensure that the FIP is behaving correctly. Furthermore, the report provides a  \n",
      "summary of the testing activities conducted to validate the behaviour of the FIP Tester.  \n",
      "Similarly, this report provides a summary of the design and project outcomes of enhancing a pre\n",
      "existing tool, called the TCS Testing Suite. Prior to the project, this tool was developed at Hitachi to  \n",
      "begin to automate the TCS testing. It contains the testing framework required to automate the testing  \n",
      "procedure that is conducted by Hitachi.  \n",
      "During this project, the TCS Testing Suite’s capabilities were enhanced to make it capable of running  \n",
      "more complex tests. Additionally, a Log Parser which iterates through various log files and consolidates  \n",
      "all of the errors into a single file was developed to aid users in troubleshooting while using the TCS  \n",
      "Testing Suite. The report also provides a summary of the testing activities performed to validate the  \n",
      "behaviour of the TCS Testing Suite enhancements and the Log Parser.  \n",
      "Overall, the developed FIP Tester and the enhanced TCS Testing Suite can be reliably used to replace  \n",
      "a lot of the manual testing activities conducted at Hitachi.  \n",
      "Page | III' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': 'E XECUTIVE S UMMARY'}\n",
      "\n",
      "\n",
      "page_content='I would like to sincerely thank a number of people who have helped me throughout this project.  \n",
      "Firstly, I would like to thank my supervisor Associated Professor Graeme Smith for the guidance and  \n",
      "advice that was provided throughout the semester which helped shape this project.  \n",
      "I would also like to thank the EAIT Employability team and Dr Christopher Leonardi at the University  \n",
      "of Queensland for all of their help in organising my placement at Hitachi Rail STS.  \n",
      "I would also like to thank Hitachi Rail STS, the host company, for allowing me the opportunity to work  \n",
      "on this project and learn about the railway industry and the AutoHaul® project. In particular I would  \n",
      "like to express my gratitude to my supervisory team at Hitachi, Dr Anthony MacDonald, Lionel Van  \n",
      "Den Berg and most of all Michal Cedrych for all of the support and advice I was given throughout the  \n",
      "course of the project. I would also like to thank Ujas Soni, Andrew Mijat and Benjamin Mountford at  \n",
      "Hitachi for all of the technical advice and support they gave me throughout the project.  \n",
      "Page | IV' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': 'A CKNOWLEDGEMENTS'}\n",
      "\n",
      "\n",
      "page_content='Executive Summary ................................................................................................................................ III  \n",
      "Acknowledgements ................................................................................................................................ IV  \n",
      "List Of Figures ........................................................................................................................................ VII  \n",
      "List Of Tables ........................................................................................................................................ VIII  \n",
      "1.0 Introduction ................................................................................................................................ 1  \n",
      "1.1 Placement Context .................................................................................................................. 1  \n",
      "1.2 Placement Purpose ................................................................................................................. 1  \n",
      "2.0 Technical Background ................................................................................................................. 2  \n",
      "2.1 The AutoHaul® Project ............................................................................................................ 2  \n",
      "2.2 Interface Details And Current Testing Methods ..................................................................... 5  \n",
      "2.2.1 Interlocking – FIP Interface ............................................................................................. 5  \n",
      "2.2.2 TCS Interfaces .................................................................................................................. 7  \n",
      "2.3 Literature Review .................................................................................................................. 10  \n",
      "2.3.1 Designing a Test Automation Framework ..................................................................... 11  \n",
      "2.3.2 Evaluating Testing Tools ................................................................................................ 12  \n",
      "2.3.3 Designing Test Cases ..................................................................................................... 12  \n",
      "2.3.4 Summary of Literature Review ..................................................................................... 13  \n",
      "3.0 Project Description .................................................................................................................... 14  \n",
      "3.1 Aims And Objectives ............................................................................................................. 14  \n",
      "3.2 Scope ..................................................................................................................................... 14  \n",
      "3.3 Project Deliverables .............................................................................................................. 16  \n",
      "3.4 Project Management ............................................................................................................ 16  \n",
      "4 Design ........................................................................................................................................... 17  \n",
      "4.1 FIP Tester .............................................................................................................................. 17  \n",
      "4.1.1 Test Framework Design ................................................................................................. 17  \n",
      "4.1.2 Test Case Design ........................................................................................................... 19  \n",
      "4.1.3 GUI Design ..................................................................................................................... 23  \n",
      "4.1.4 Performance Assessment Design .................................................................................. 24  \n",
      "4.1.5 Documentation ............................................................................................................. 24  \n",
      "4.2 TCS Testing Suite ................................................................................................................... 25  \n",
      "4.2.1 Increasing Testing Capabilities ...................................................................................... 26  \n",
      "4.2.2 Log Parser ...................................................................................................................... 27  \n",
      "4.2.3 Performance Assessment.............................................................................................. 27  \n",
      "4.2.4 Documentation ............................................................................................................. 28  \n",
      "Page | V  \n",
      "5 Project Outcomes ......................................................................................................................... 29  \n",
      "5.1 FIP Tester .............................................................................................................................. 29  \n",
      "5.1.1 The Testing Tool ............................................................................................................ 29  \n",
      "5.1.2 The GUI.......................................................................................................................... 30  \n",
      "5.1.3 Test Cases ...................................................................................................................... 30  \n",
      "5.1.4 Performance Assessment.............................................................................................. 30  \n",
      "5.1.5 Documentation ............................................................................................................. 34  \n",
      "5.2 TCS Testing Suite ................................................................................................................... 34  \n",
      "5.2.1 The Enhanced TCS Testing Suite ................................................................................... 34  \n",
      "5.2.2 The Log Parser ............................................................................................................... 36  \n",
      "5.2.3 Performance Assessment.............................................................................................. 36  \n",
      "6 Conclusion And Recommendations ............................................................................................. 39  \n",
      "6.1 FIP Tester .............................................................................................................................. 39  \n",
      "6.2 TCS Testing Suite ................................................................................................................... 39  \n",
      "6.3 Project Improvements .......................................................................................................... 40  \n",
      "7 References .................................................................................................................................... 41  \n",
      "8 Appendix A: Project Management Summary ............................................................................... 44  \n",
      "8.1 Project Timeline And Resources ........................................................................................... 44  \n",
      "8.1.1 Background Research Stage .......................................................................................... 44  \n",
      "8.1.2 Design Stage .................................................................................................................. 44  \n",
      "8.1.3 Implementation Stage ................................................................................................... 44  \n",
      "8.1.4 Documentation Stage ................................................................................................... 45  \n",
      "8.1.5 Project Timeline ............................................................................................................ 45  \n",
      "8.2 Risk Analysis .......................................................................................................................... 47  \n",
      "8.2.1 Operational Health And Safety Risks ............................................................................ 47  \n",
      "8.2.2 Project Scheduling Risks ................................................................................................ 48  \n",
      "8.2.3 Risk Matrix .................................................................................................................... 49  \n",
      "8.3 Project Deliverables Outcomes ............................................................................................. 50  \n",
      "8.4 Opportunities ........................................................................................................................ 50  \n",
      "9 Appendix B: FIP Messages ............................................................................................................ 51  \n",
      "10 Appendix C: TCS Message Protocols ............................................................................................ 52  \n",
      "10.1 TCS – ATOC Communication Protocol................................................................................... 52  \n",
      "10.2 TCS – RES Communication Protocol ...................................................................................... 52  \n",
      "10.3 TCS – VSS Communication Protocol ...................................................................................... 53  \n",
      "11 Appendix D: Test Bench Set-Up ................................................................................................... 54  \n",
      "12 Appendix E: FIP Tester Workflow Diagrams ................................................................................. 55  \n",
      "Page | VI  \n",
      "13 Appendix F: FIP Tester Tests ........................................................................................................ 57  \n",
      "14 Appendix G: TCS Testing Suite Architecture ................................................................................ 58' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': 'T ABLE OF C ONTENTS'}\n",
      "\n",
      "\n",
      "page_content='Figure 1 AutoHaul® system breakdown .................................................................................................. 1  \n",
      "Figure 2 High level system architecture diagram of relevant sub-systems and interfaces .................... 3  \n",
      "Figure 3 FIP to IXL connections in the field ............................................................................................. 5  \n",
      "Figure 4 FIP to IXL connections in a test environment ........................................................................... 6  \n",
      "Figure 5 Example test from TCS integration test specification document [19] ...................................... 8  \n",
      "Figure 6 TCS Testing Suite behaviour ...................................................................................................... 9  \n",
      "Figure 7 Ranking of test case criteria as determined by Adlemo et al. [29] ......................................... 13  \n",
      "Figure 8 High level overview of FIP Tester tool .................................................................................... 19  \n",
      "Figure 9 Generic test case workflow diagram....................................................................................... 20  \n",
      "Figure 10 Flowchart of process to test new configurations management – See Figure 32 for the startup sequence testing .............................................................................................................................. 20  \n",
      "Figure 11 GUI design sketch .................................................................................................................. 23  \n",
      "Figure 12 High level TCS Testing Suite test engine design .................................................................... 25  \n",
      "Figure 13 Code showing a train being created before the enhancements .......................................... 26  \n",
      "Figure 14 High level log parser design .................................................................................................. 27  \n",
      "Figure 15 System architecture of FIP Tester ......................................................................................... 29  \n",
      "Figure 16 High level overview of designed FIP Tester .......................................................................... 29  \n",
      "Figure 17 GUI for the FIP Tester ............................................................................................................ 30  \n",
      "Figure 18 Confirmation pop-up for the FIP Tester ................................................................................ 30  \n",
      "Figure 19 'tcpdump' output .................................................................................................................. 31  \n",
      "Figure 20 Linux command ‘netcat’ output ............................................................................................ 31  \n",
      "Figure 21 Python terminal output of test case 1 (Start-up sequence) with correct replies as seen by the  \n",
      "FIP Simulator ......................................................................................................................................... 32  \n",
      "Figure 22 Python terminal output of test case 1 (Start-up sequence) with incorrect replies as seen by  \n",
      "the FIP Simulator ................................................................................................................................... 32  \n",
      "Figure 23 FIP Tester log file when incorrect message is received ........................................................ 32  \n",
      "Figure 24 FIP Tester log showing a passing test.................................................................................... 33  \n",
      "Figure 25 FIP Tester log showing a failing test ...................................................................................... 33  \n",
      "Figure 26 Screenshot from user guide .................................................................................................. 34  \n",
      "Figure 27 Code showing the helper function implemented to create multiple trains ......................... 35  \n",
      "Figure 28 Screenshot of the collated errors file ................................................................................... 36  \n",
      "Page | VII  \n",
      "Figure 29 Example of a test case used to verify test behaviour with pass/fail comments ................... 37  \n",
      "Figure 30 Project timeline ..................................................................................................................... 46  \n",
      "Figure 31 Test bench set-up in Brisbane ............................................................................................... 54  \n",
      "Figure 32 Flowchart of process to test initial testing sequences ......................................................... 55  \n",
      "Figure 33 Flowchart of process to test site-like transmission errors .................................................... 56  \n",
      "Figure 34 Start-up test code snippet .................................................................................................... 57  \n",
      "Figure 35 TCS Testing Suite architecture diagram [32] ......................................................................... 58' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': 'L IST O F F IGURES'}\n",
      "\n",
      "\n",
      "page_content='Table 1 Table of abbreviations ............................................................................................................... IX  \n",
      "Table 2 Description of sub-systems ........................................................................................................ 4  \n",
      "Table 3 Polling cycle for FIP and IXL communications [15] ..................................................................... 6  \n",
      "Table 4 TCS interface details ................................................................................................................... 7  \n",
      "Table 5 Types of tests to implement autonomously ............................................................................ 14  \n",
      "Table 6 Interfaces covered in the scope ............................................................................................... 15  \n",
      "Table 7 Key deliverables list .................................................................................................................. 16  \n",
      "Table 8 Summary of project stages ....................................................................................................... 16  \n",
      "Table 9 System requirements for the FIP Tester .................................................................................. 18  \n",
      "Table 10 Test specifications for FIP test 1: Testing the initial start-up sequence ................................ 21  \n",
      "Table 11 Test specifications for FIP test 2: Testing with site-like conditions........................................ 21  \n",
      "Table 12 Test specifications for FIP test 3: Testing how new configurations are handled ................... 23  \n",
      "Table 13 Tests to verify the FIP Tester’s behaviour .............................................................................. 24  \n",
      "Table 14 Results from manually checking the logs in Figure 24 ........................................................... 33  \n",
      "Table 15 Results from manually checking the logs In Figure 25 ........................................................... 34  \n",
      "Table 16 Results of asking a tester to debug using the Log Parser ....................................................... 38  \n",
      "Table 17 OH&S risks summary .............................................................................................................. 47  \n",
      "Table 18 Scheduling risks summary ...................................................................................................... 48  \n",
      "Table 19 Risk matrix .............................................................................................................................. 49  \n",
      "Table 20 Outcomes of key project deliverables .................................................................................... 50  \n",
      "Table 21 Project opportunities ............................................................................................................. 50  \n",
      "Table 22 FIP – IXL Message types and corresponding control characters [15] .................................... 51  \n",
      "Table 23 TCS – ATOC message protocol [4] .......................................................................................... 52  \n",
      "Table 24 TCS – VSS message protocol [12] ........................................................................................... 53  \n",
      "Page | VIII  \n",
      "_Table 1 Table of abbreviations_  \n",
      "|Acronym Long Term|Col2|\n",
      "|---|---|\n",
      "|**AMMI**|Automation Man Machine Interface|\n",
      "|**AS**|Automation Server|\n",
      "|**aTest**|A proprietary tool used at Hitachi to simulate ATOC connections|\n",
      "|**ATO**|Automatic Train Operation|\n",
      "|**ATOC**|Automation Train Operation Controller|\n",
      "|**CTC**|Central Train Control|\n",
      "|**FIP**|Field Interface Processor|\n",
      "|**GUI**|Graphical User Interface|\n",
      "|**HMI**|Human Machine Interface|\n",
      "|**IXL**|Interlocking|\n",
      "|**MISS**|Microlok Interlocking Simulation System|\n",
      "|**RTIO**|Rio Tinto Iron Ore|\n",
      "|**RES**|RTIO External Servers|\n",
      "|**SIL**|Safety Integrity Level|\n",
      "|**TCS**|Train Control Sub-system|\n",
      "|**UDP**|User Datagram Protocol|\n",
      "|**UI**|User Interface|\n",
      "|**VM**|Virtual Machine|\n",
      "|**VSS**|Vital Safety Server|  \n",
      "Page | IX' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': 'L IST O F T ABLES'}\n",
      "\n",
      "\n",
      "page_content='Rio Tinto Iron Ore (RTIO) operates a heavy-haul freight railway network in Western Australia [1]. This  \n",
      "network moves iron ore from mines in the Pilbara region to coastal ports for shipping overseas. The  \n",
      "AutoHaul® project provides an Automated Train Operation (ATO) system to facilitate the driverless  \n",
      "movement of the trains on the mainline in RTIO’s network [2] [3]. As shown in Figure 1, AutoHaul®  \n",
      "uses three main sub-systems to control and monitor locomotives and ensure their safe movement in  \n",
      "the network. These sub-systems are the Trainborne System, the Control Centre and the Wayside  \n",
      "Systems. More details about these sub-systems are given in Section 2.0. Hitachi Rail STS (Hitachi) was  \n",
      "contracted to deliver and maintain this ATO system.  \n",
      "_Figure 1 AutoHaul® system breakdown_  \n",
      "Because the operations of trains in the AutoHaul® network are safety critical, any modifications made  \n",
      "to the AutoHaul® system require extensive testing before they are rolled out. Hitachi’s integration  \n",
      "team is responsible for performing system-wide testing. This includes testing the system as a whole  \n",
      "to ensure that all of the sub-systems are integrating properly with each other and all of the  \n",
      "communication interfaces between different sub-systems work as required. Presently, the integration  \n",
      "team uses manual testing for this purpose.  \n",
      "Due to the AutoHaul® project’s complexity, manual testing is a time consuming and repetitive process  \n",
      "that does not always identify the “edge case” issues in the system. As such, there is a potential to  \n",
      "automate these tests.' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': '1.0 I NTRODUCTION', 'Header 4': '1.1 P LACEMENT C ONTEXT'}\n",
      "\n",
      "\n",
      "page_content='Because Hitachi regularly roll out upgrades and modifications for the AutoHaul® project, they are  \n",
      "investing in methods to reduce testing time by automating tests. As such, Hitachi wish to develop an  \n",
      "automated testing tool that can be used to:  \n",
      "- Automate time-consuming and repetitive tests,  \n",
      "- Free up the tester’s time and allow them to perform different tests and therefore improve  \n",
      "the thoroughness of the testing and  \n",
      "- Reduce the products costs earlier in the testing stage of the product’s lifecycle.  \n",
      "Page | 1' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': '1.0 I NTRODUCTION', 'Header 4': '1.2 P LACEMENT P URPOSE'}\n",
      "\n",
      "\n",
      "page_content='The AutoHaul® project introduced an Automated Train Operation (ATO) system so that trains are able  \n",
      "to move autonomously on the mainline of RTIO’s railway network in Western Australia [3].  \n",
      "As seen in Figure 1, the three sub-systems AutoHaul® uses to ensure the safe movement of trains are  \n",
      "the Trainborne System, the Control Centre and the Wayside Systems [3].  \n",
      "The Trainborne System is installed in each AutoHaul® locomotive and uses the Automatic Train  \n",
      "Operations Controller (ATOC) as its primary control system. The ATOC can be thought of as the train’s  \n",
      "driver. It interfaces to the locomotive’s equipment to perform tasks such as braking, accelerating and  \n",
      "collision detection. It also communicates with the Control Centre to receive instructions and transfer  \n",
      "relevant data [4].  \n",
      "Located in Perth, the Control Centre uses the Train Control Sub-system (TCS) to control the majority  \n",
      "of the AutoHaul® network [3]. It manages the train routing, mission planning and provides a user  \n",
      "interface for operators to use.  \n",
      "The Wayside Systems contains a range of devices that are placed on the side of the tracks at various  \n",
      "points in the network. Collectively, this system performs functions such as train tracking, interlocking  \n",
      "and controlling intersections on the track [5].  \n",
      "A high level system architecture containing the sub-systems and communication interfaces relevant  \n",
      "for this project is provided in Figure 2. A description of the sub-systems is provided in Table 2 and  \n",
      "more details are given about these interfaces in Section 2.2.  \n",
      "Page | 2  \n",
      "_Figure 2 High level system architecture diagram of relevant sub-systems and interfaces_  \n",
      "Page | 3  \n",
      "_Table 2 Description of sub-systems_  \n",
      "|Sub-system Functionality|Col2|\n",
      "|---|---|\n",
      "|**WAYSIDE SYSTEMS**|**WAYSIDE SYSTEMS**|\n",
      "|**Wayside Equipment**|Equipment which is placed on the trackside and is used to monitor the<br>health and status of track-based assets [6].|\n",
      "|**Interlocking (IXL)**|This component of signaling systems ensures that the railway behaves in a<br>safe manner and is fail-safe [7] [8]. IXL devices do this by:<br> <br>Performing vital functions such as route setting,<br> <br>Sending signaling information to the TCS and<br> <br>Receiving commands, such as clearing signals, from the TCS.<br>Hitachi’s Microlok II is used as the IXL devices.|\n",
      "|**TRAINBORNE SYSTEM**|**TRAINBORNE SYSTEM**|\n",
      "|**Locomotive**<br>**Equipment**|A collection of equipment and computer systems that are used to monitor<br>and perform train operations such as interfacing with the ATOC and<br>operating the throttle and brakes [3].<br>|\n",
      "|**Automatic Train**<br>**Operations**<br>**Controller (ATOC)**|The primary control system of the train. It communicates with other sub-<br>systems and controls all of the train’s operations [4].|\n",
      "|**CONTROL CENTRE**|**CONTROL CENTRE**|\n",
      "|**Train Control**<br>**Sub-system (TCS)**|The system used to monitor and control the railway network. It is used to<br>set routes, track train movement, manage alarms and perform monitoring<br>actions that were previously undertaken by drivers [9].|\n",
      "|**Centralised Train**<br>**Control (CTC)**|A train control system that provides the network overview, shows<br>indications and allows route setting and train sheet management [10].|\n",
      "|**Automation Man**<br>**Machine Interface**<br>**(AMMI)**|A user interface to all the trains and locomotives in the AutoHaul® system.<br>It allows users to access the CTC [11].|\n",
      "|**Automation Server**<br>**(AS)**|A messaging service that acts as a gateway for the TCS and the rest of the<br>systems in the AutoHaul® system [5].|\n",
      "|**Vital Safety Server**<br>**(VSS)**|Provides movement authorities to the train based on data from the<br>interlocking and level crossings [12]. The VSS also acts as a user interface<br>and allows users to set commands which are relayed to the rest of the<br>system.|\n",
      "|**Field Interface**<br>**Processor (FIP)**|A device which connects to all of the IXLs in the field and facilitates the<br>exchange of information between the IXLs and the TCS [13].|\n",
      "|**RTIO External**<br>**Systems**|A TIBCO Enterprise Message Service which acts as the interface between<br>the AutoHaul® and RTIO’s other systems [14]. RTIO External Systems are<br>used for functions such as producing an electronic train graph.|  \n",
      "Page | 4' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': '2.0 T ECHNICAL B ACKGROUND', 'Header 4': '2.1 T HE A UTO H AUL ® P ROJECT'}\n",
      "\n",
      "\n",
      "page_content='2.2.1 Interlocking – FIP Interface  \n",
      "_2.2.1.1_ _FIP Functionality_  \n",
      "Each IXL device acts as a User Datagram Protocol (UDP) server which has an open socket on a specific  \n",
      "IP address and port. The FIP runs multiple UDP clients which connect to IXLs on the given IP addresses  \n",
      "and ports to enable bi-directional communication as shown in Figure 3.  \n",
      "_Figure 3 FIP to IXL connections in the field_  \n",
      "Please note that only 3 connections are shown in Figure 3, the AutoHaul® project actually uses many  \n",
      "more IXLs.  \n",
      "_2.2.1.2_ _Data Contents_  \n",
      "The data sent between the FIP and the IXLs uses the Genisys protocol [15]. Each message contains a  \n",
      "control character, the address of the recipient/sender IXL, the interlocking data bytes, a security  \n",
      "checksum and a termination character. More details about the packet structure are provided in  \n",
      "Appendix B: FIP Messages.  \n",
      "The interlocking data bytes coming out of the Microlok IIs are at a Safety Integrity Level (SIL) 4 [16],  \n",
      "and are therefore considered to be highly reliable. The FIP is only responsible for collating this data  \n",
      "and passing it along to the TCS, where it is validated. As such, validating the interlocking data is outside  \n",
      "the scope of this project. Instead, the main focus of the testing is validating the FIP’s behaviour.  \n",
      "To request for data, the FIP uses a pre-defined polling cycle [15] as shown in Table 3. There are five  \n",
      "types of messages that can be sent from the FIP to the IXL. These are poll messages, control messages,  \n",
      "recall messages, execute messages and master acknowledge messages. Furthermore, there are three  \n",
      "types of messages that can be sent back to the FIP. These are indication messages, control check back  \n",
      "messages and slave acknowledgement messages. More details about the messages are provided in  \n",
      "Appendix B: FIP Messages.  \n",
      "Page | 5  \n",
      "_Table 3 Polling cycle for FIP and IXL communications [15]_  \n",
      "|FIP to IXL Message Expected Reply|Col2|Col3|Col4|\n",
      "|---|---|---|---|\n",
      "|**FIP to IXL1**|**FIP to IXL2**|**FIP to IXL3**|**IXL to FIP**|\n",
      "|Recall<br>message|Recall<br>message|Recall<br>message|Indication message from each IXL.|\n",
      "|Control<br>message|Control<br>message|Control<br>message|Indication or slave acknowledge message from each<br>IXL.|\n",
      "|Poll<br>message|Poll<br>message|Poll<br>message|Indication or slave acknowledge message from each<br>IXL.|\n",
      "|Recall<br>message|Poll<br>message|Poll<br>message|Indication message from IXL1 and slave acknowledge<br>messages from other IXLs.|\n",
      "|Poll<br>message|Recall<br>message|Poll<br>message|Indication message from IXL2 and slave acknowledge<br>messages from other IXLs.|\n",
      "|Poll<br>message|Poll<br>message|Recall<br>message|Indication message from IXL3 and slave acknowledge<br>messages from other IXLs.|\n",
      "|Recall<br>message|Poll<br>message|Poll<br>message|Indication message from IXL1 and slave acknowledge<br>messages from other IXLs.|  \n",
      "_2.2.1.3_ _Current Testing Methodology_  \n",
      "In a test environment, setting up the physical connections between the IXLs and the FIP is not feasible  \n",
      "as there are too many hardware requirements. As such, a combination of a Virtual Serial Port Emulator  \n",
      "(VSPE) and a Microlok Interlocking Simulation System (MISS) is used. The FIP requires the incoming  \n",
      "data to come from UDP connections. However, due to being limited by its legacy software, MISS can  \n",
      "only communicate using Transmission Control Protocol (TCP) communications. Therefore, a Socket  \n",
      "Cat (SoCat) router is used to convert messages between TCP and UDP protocols. The testing set-up is  \n",
      "illustrated in Figure 4.  \n",
      "_Figure 4 FIP to IXL connections in a test environment_  \n",
      "Page | 6  \n",
      "Currently the FIP behaviour is tested using two main methods. These are:  \n",
      "1) Verifying that the IXL data received at the TCS is correct. It is assumed that if the data is correct\n",
      "when it arrives at the TCS then the FIP must be behaving correctly.\n",
      "2) Manually checking the log files that are stored in the FIP to verify the messages. This testing is  \n",
      "only conducted when an end-to-end change, such as adding a new IXL device, is made [17].  \n",
      "This project will focus on automating the manual checks that are conducted of the FIP’s behaviour.  \n",
      "2.2.2 TCS Interfaces  \n",
      "As described in Section 2.1, the TCS is the main control system of the AutoHaul® network and is  \n",
      "therefore tested frequently.  \n",
      "_2.2.2.1_ _Relevant Interfaces_  \n",
      "There are three communication interfaces within the scope of this project. These are the TCS – ATOC  \n",
      "interface, the TCS – RES interface and TCS – VSS interface. The key elements of these interfaces are  \n",
      "summarised in Table 4. Additional details about the specific packet structures and message types of  \n",
      "each communication interface are provided in Appendix C: TCS Message Protocols.  \n",
      "_Table 4 TCS interface details_  \n",
      "|Interface Details|Col2|\n",
      "|---|---|\n",
      "|**TCS – ATOC**| <br>The TCS – ATOC interface is a collection of communication channels between<br>the TCS and each ATOC system that is active in the AutoHaul® network.<br> <br>Locomotives use this interface to send locomotive status information to the<br>TCS and receive instructions and software updates from the TCS.<br> <br>The interface uses the TCS – ATOC Protocol [4] as the messaging protocol.<br> <br>A propriety tool, aTest, is used to simulate messages that the ATOC would<br>usually send autonomously. Users can send these messages via a User<br>Interface (UI) or through commands set in a Python script.|\n",
      "|**TCS – RES**| This interface allows the RES to send planned train sheets and timetables to<br>the TCS. It also allows the TCS to send data regarding the actual train<br>movement back to the RES.<br> <br>Communication occurs via a TIBCO Message Service.<br> <br>In the existing test environment, ActiveMQ is used as the messaging service<br>and Python scripts can be used to simulate RTIO messages.<br> <br>The interface uses the TCS – RTIO Protocol [14] as the messaging protocol.|\n",
      "|**TCS – VSS**| This is a bi-directional communication channel that allows the TCS to send<br>locomotive supervision data to the VSS and receive track information back<br>from the VSS.<br> <br>The interface uses the TCS – VSS Protocol [12] as the messaging protocol.<br> <br>A VM running an instance of a VSS is running on the test bench.|  \n",
      "Page | 7  \n",
      "_2.2.2.2_ _Testing Methodology_  \n",
      "Currently, two methods are used to test the TCS interfaces. These are manual testing and the TCS  \n",
      "Testing Suite.  \n",
      "**Manual Testing**  \n",
      "This is the most commonly used form of testing at Hitachi. Testing the TCS involves manually  \n",
      "performing sequences, such as creating a train sheet and verifying that all of the sub-systems behave  \n",
      "as expected. This testing is done at a test bench which runs either a version, or a simulation of all sub\n",
      "systems deployed in the AutoHaul® project. Appendix D: Test Bench Set-Up shows an image of this  \n",
      "test bench.  \n",
      "The expected behaviour of the system is based on the TCS’s functional and behavioural expectations  \n",
      "[18] and is tested with tests laid out in the TCS Integration Test Specification document [19]. Figure 5  \n",
      "shows one test from this document. The test outlines activities that must be done before the test can  \n",
      "begin (preconditions), which are the sequences to perform during the test and the expected results  \n",
      "to each step.  \n",
      "_Figure 5 Example test from TCS integration test specification document [19]_  \n",
      "All of the tests must be run independently to each other. For example, step 1 of Test 3.33 in Figure 5  \n",
      "requires the tester to log into the CTC’s Human Machine Interface (HMI) as the Asset Health Evaluator  \n",
      "(AHE). At the end of Test 3.33 the user must log out as the AHE even if the next test requires them to  \n",
      "login as the AHE again. This adds a lot of time to the testing procedure as the testers must set-up and  \n",
      "remove the test environment for each test.  \n",
      "Furthermore, the testing methodology is very structured and does not leave much time for “creative  \n",
      "testing”. Therefore, the testing does not cover all of the edge cases and unexpected situations that  \n",
      "might arise in the field.  \n",
      "An experienced tester at Hitachi will require approximately an hour to complete the integration  \n",
      "testing. This time estimate assumes that no bugs are found. In reality, testers find that they get half  \n",
      "way through the set of tests before a test fails. After fixing the cause of the failure, they must restart  \n",
      "the testing from the beginning.  \n",
      "Page | 8  \n",
      "**TCS Testing Suite**  \n",
      "A test environment has been created to run some of the more basic tests autonomously. The test  \n",
      "environment is installed on a VM and uses Python scripts to perform the actions listed below.  \n",
      "- Initialise connections with local versions of AutoHaul® servers such as the RES and VSS,  \n",
      "- Connect to the ATOC emulator, aTest and emulate messages sending from ATOC,  \n",
      "- Connect to Squish, a tool created by Froglogic to automate UI testing [20],  \n",
      "- Use Squish to mimic interacting with the UIs and verify that the system is behaving as  \n",
      "expected,  \n",
      "- Use a library of helper functions to facilitate communication between the TCS and all other  \n",
      "AutoHaul® servers, ATOC and Squish and  \n",
      "- Run tests and log the activities in various log files depending on the action being performed.  \n",
      "This behaviour is summarised in Figure 6. Additionally, the file structure and architecture of the TCS  \n",
      "Testing Suite is provided in Appendix G: TCS Testing Suite Architecture.  \n",
      ".  \n",
      "_Figure 6 TCS Testing Suite behaviour_  \n",
      "To use the test suite, a user must run a particular test script in a PyCharm environment. Once a test  \n",
      "script is run it behaves as follows:  \n",
      "1. The AutoHaul® servers and simulators are started. This includes, beginning connections to the  \n",
      "Automation Server, VSS, and RTIO External Servers.  \n",
      "2. A start-up function is implemented. The start-up function sets up all of the pre-conditions (such  \n",
      "as creating and registering a train) that are required for the tests.  \n",
      "3. The tests are run. Generally, between 1 and 5 tests are contained within each script. The tests are  \n",
      "typically implemented by:  \n",
      "a. Making an event occur in the system. For example, creating and registering a train.  \n",
      "b. Verifying that all of the messages related to that event have been received at their  \n",
      "destination. For example, verify that the CTC has received the details for a new train.  \n",
      "Page | 9  \n",
      "c. Using Squish to verify that the relevant user interface has been updated. For example,  \n",
      "making sure that the new train ID is visible to the user.  \n",
      "d. A shut down function is implemented. This function resets the testing environment so  \n",
      "that it is back to its original status. For example, if a train has been registered during the  \n",
      "tests, then the train is deleted.  \n",
      "e. The servers and simulators are shut down. This includes closing all active connections.  \n",
      "4. A Python console displays the outcome of each test to the user.  \n",
      "Like the manual testing, the test suite also uses the TCS Integration Test Specifications documentation  \n",
      "[19] for the test case design. However, not all of the required tests from the document have been  \n",
      "implemented.  \n",
      "The current implementation only includes tests which check the functionality of a particular sub\n",
      "system and that sub-system’s User Interface (UI) currently exist. For example, a test can be run to  \n",
      "check that a train sheet is created and registered in the correct user’s view. But it cannot perform  \n",
      "more complicated tests such as performing load testing to ensure multiple trains have been  \n",
      "registered.  \n",
      "Furthermore, the test suite is a complex system that requires many active connections and uses many  \n",
      "log files to keep track of the events in a test. Therefore debugging a test to determine the root cause  \n",
      "of failure can be a time consuming task. For example, as a part of registering a new train, a TCS  \n",
      "message is sent to the RES and the RES needs to send back a confirmation. If there is an error in  \n",
      "connecting to the RES and therefore no confirmation is received by the TCS, then the following  \n",
      "messages would be recorded in the most relevant log files.  \n",
      "- The TCS and CTC logs would record that the train could not be registered,  \n",
      "- The Automation Server logs would show that there was a timeout in waiting for the  \n",
      "confirmation message and  \n",
      "- The Automation Server logs would record that a connection could not be established with the  \n",
      "RES. However, this message would have been recorded at the start of the start-up activities  \n",
      "and would have been hidden by the newer activities.  \n",
      "It would be up to the user to recognise that the error is due to connection issues, not problems with  \n",
      "the AutoHaul’s ® code or the test’s logic.  \n",
      "This project focused on enhancing the capabilities of the TCS Testing Suite so that more complicated  \n",
      "tests could run. It also provided a method for users to quickly determine the causes of failure in a  \n",
      "system.' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': '2.0 T ECHNICAL B ACKGROUND', 'Header 4': '2.2 I NTERFACE D ETAILS A ND C URRENT T ESTING M ETHODS'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"#####\", \"Header 4\")\n",
    "]\n",
    "\n",
    "markdown_document_path = 'UQ AHE Thesis.md'\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "with open(markdown_document_path) as md_file:\n",
    "    text = md_file.read()\n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{md_header_splits[i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c09f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 455, 185, 417, 402, 320, 238, 84, 593, 2023, 1498, 162, 526, 145, 139, 38, 2193, 1289, 1310, 912, 32, 217, 283, 173, 663, 634, 595, 150, 142, 329, 88, 126, 89, 10, 26, 9, 11]\n"
     ]
    }
   ],
   "source": [
    "i = [len(chunk.page_content.split()) for chunk in md_header_splits]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "389271aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7), (1, 455), (2, 185), (3, 417), (4, 402), (5, 320), (6, 238), (7, 84), (8, 593), (9, 2023), (10, 1498), (11, 162), (12, 526), (13, 145), (14, 139), (15, 38), (16, 2193), (17, 1289), (18, 1310), (19, 912), (20, 32), (21, 217), (22, 283), (23, 173), (24, 663), (25, 634), (26, 595), (27, 150), (28, 142), (29, 329), (30, 88), (31, 126), (32, 89), (33, 10), (34, 26), (35, 9), (36, 11)]\n"
     ]
    }
   ],
   "source": [
    "j = []\n",
    "for num in enumerate(i):\n",
    "    j.append(tuple(num))\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ce2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(j, key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c293d7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2193)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4f66cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The aim of testing the FIP – IXL interface was to check the behaviour of the FIP by ensuring that its  \n",
      "responses to IXL messages are correct. Hence, it was decided that a FIP testing tool (FIP Tester) would  \n",
      "be developed. This tool would simulate IXL devices in a fashion similar to how aTest (Hitachi’s  \n",
      "proprietary ATOC simulation tool) operates in the TCS testing.  \n",
      "4.1.1 Test Framework Design  \n",
      "The design process used to design this testing tool was based on the top-down methodology  \n",
      "recommended by Méndez-Porras et al. [31] in their paper. As recommended by Méndez-Porras, the  \n",
      "system requirements were defined, then a high level system was designed.  \n",
      "**System Requirements**  \n",
      "The requirements for the FIP Tester are provided in Table 9.  \n",
      "Page | 17  \n",
      "_Table 9 System requirements for the FIP Tester_  \n",
      "|Requirement|Col2|\n",
      "|---|---|\n",
      "|**Implementation Requirements**|**Implementation Requirements**|\n",
      "|**1 **|All IXL simulations shall connect to the FIP in the same way that IXL devices do.|\n",
      "|**2 **|IXL packets shall use the Genisys protocol.<br>Instead of actual interlocking data, the IXL simulator shall send “0101”.|\n",
      "|**3 **|No changes shall be made to the FIP’s software to use this testing tool.|\n",
      "|**4 **|The FIP Tester shall integrate with Hitachi’s existing systems.|\n",
      "|**Test Case Requirements**|**Test Case Requirements**|\n",
      "|**5 **|In order to run capacity and load testing, the FIP Tester shall simulate multiple IXL devices at<br>the same time.|\n",
      "|**6 **|The FIP Tester shall test the behavior of the FIP in response to site-like transmission errors.<br>These errors include:<br> <br>Partially received messages,<br> <br>Corrupted messages,<br> <br>Duplicated messages,<br> <br>Lost messages and<br> <br>Messages time-out.|\n",
      "|**7 **|The FIP Tester shall test the behaviour of the FIP in response to a new IXL connecting to it<br>after it has already been running.|\n",
      "|**Software Requirements**|**Software Requirements**|\n",
      "|**8 **|The FIP Tester shall only use open source software libraries.|\n",
      "|**Documentation Requirements**|**Documentation Requirements**|\n",
      "|**9 **|A document outlining the architecture of the software solution and the test procedure used<br>to validate its behavior shall be provided.|\n",
      "|**10**|A user guide which provides instructions shall be provided. The user guide shall provide<br>instructions on:<br> <br>Setting up the FIP Tester,<br> <br>Running tests and<br> <br>Making modifications to the system|\n",
      "|**Output Requirements**|**Output Requirements**|\n",
      "|**11**|The user shall be presented with a pass/fail report at the end of the tests. If a test fails, the<br>report should include the cause of failure.|\n",
      "|**12**|The FIP Tester shall log the testing activities to make it easier to debug issues.|  \n",
      "**High Level Design**  \n",
      "As per Méndez-Porras et al. [31], the next step in designing the framework was to develop a high level  \n",
      "system overview and then breakdown the functions of each element in the framework. As  \n",
      "summarised in Figure 8, the designed behaviour of the FIP tester is as follows.  \n",
      "1. A GUI allows the user to enter test parameters. Example parameters include specifying the  \n",
      "number of IXLs to create.  \n",
      "2. The test engine runs the tests to check the functionality of the FIP. This test engine should  \n",
      "be able to perform the following tasks.  \n",
      "a. Log all testing activities.  \n",
      "b. Initialise IXL simulators and connect them to the FIP based on user specified IP  \n",
      "addresses and port numbers.  \n",
      "c. Run tests to check that all the IXLs are connected and that communications are  \n",
      "following the polling cycle described in Table 3.  \n",
      "Page | 18  \n",
      "d. Test how the FIP behaves in each of the test cases.  \n",
      "e. Shut down the test environment to prevent it from affecting future tests.  \n",
      "3. A pass/fail report is generated to inform the user about the results of the tests. In case a test  \n",
      "fails, the report will include a reason for the failure.  \n",
      "_Figure 8 High level overview of FIP Tester tool_  \n",
      "Key decisions made when designing this framework include:  \n",
      "- Using Python as the programming language because of the availability of multiple open\n",
      "source libraries that will be useful to the project,  \n",
      "- Implementing the program in a Linux VM to increase adoptability in Hitachi.  \n",
      "4.1.2 Test Case Design  \n",
      "There were three test cases for which tests were designed. These were:  \n",
      "1) Testing the initial start-up sequence,  \n",
      "2) Testing with site-like conditions and  \n",
      "3) Testing how new configurations are handled.  \n",
      "The generic process used for tests (1) and (2) are shown in Figure 9. Detailed workflow diagrams for  \n",
      "each test are provided in Appendix E: FIP Tester Workflow Diagrams. For testing site-like conditions,  \n",
      "the 6 types of communication errors that were simulated and their expected response are:  \n",
      "- 25% packet loss,  \n",
      "- 50% packet loss,  \n",
      "- 75% packet loss,  \n",
      "- Duplicate packets being sent,  \n",
      "- Corrupted packets being sent and  \n",
      "- 100% packet loss.  \n",
      "In all of these cases, the FIP is expected to disregard the response that is received from the IXL and  \n",
      "resend the request for the response.  \n",
      "Page | 19  \n",
      "_Figure 9 Generic test case workflow diagram_  \n",
      "The generic process used for test (3) is provided in Figure 10. For each of the three tests, a test  \n",
      "specification was created and is detailed in Table 10 to Table 12.  \n",
      "_Figure 10 Flowchart of process to test new configurations management – See Figure 32 for the start-up sequence testing_  \n",
      "Page | 20  \n",
      "**FIP Test 1: Testing the initial start-up sequence**  \n",
      "Description: Ensures that the FIP sends a recall, control, and poll message as per its polling cycle.  \n",
      "Pre-test actions:  \n",
      "1) Restart the FIP.  \n",
      "2) Ensure the VM hosting the FIP Tester has the correct networking settings.  \n",
      "_Table 10 Test specifications for FIP test 1: Testing the initial start-up sequence_  \n",
      "|Step Description Expected Result<br>No|Col2|Col3|\n",
      "|---|---|---|\n",
      "|**1 **|FIP Tester begins running|A GUI opens that asks the user to enter<br>test configurations.|\n",
      "|**2 **|A recall message is received at each valid IXL|The FIP Tester logs show the received<br>message and display a message that shows<br>that the recall test is passed.|\n",
      "|**3 **|An indication message is sent by each IXL.<br>The FIP should receive this message and send<br>a control message back to the IXL|The FIP Tester logs show the received<br>message and display a message that shows<br>that the control test is passed.|\n",
      "|**4 **|A slave acknowledge message is sent by each<br>IXL. The FIP should receive this message and<br>send a poll message back to the IXL|The FIP Tester logs show the received<br>message and display a message that shows<br>that the poll test is passed.|\n",
      "|**5 **|An indication message is sent by each IXL.<br>The FIP should receive this message and send<br>a recall message back to the IXL|The FIP Tester logs show the received<br>message and display a message that shows<br>that the test is complete.<br>Test ends.|  \n",
      "**FIP Test 2: Testing with site-like conditions**  \n",
      "Description: Ensures that the FIP can respond appropriately when there are communications errors.  \n",
      "Pre-test actions:  \n",
      "1) Restart the FIP.  \n",
      "2) Ensure the VM hosting the FIP Tester has the correct networking settings.  \n",
      "3)  \n",
      "_Table 11 Test specifications for FIP test 2: Testing with site-like conditions_  \n",
      "|Step Description Expected Result<br>No|Col2|Col3|\n",
      "|---|---|---|\n",
      "|**1 **|FIP Tester begins running|A GUI opens that asks the user to enter test<br>configurations.|\n",
      "|**2 **|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message<br>and display a message saying the first recall<br>message has been received.|\n",
      "|**3 **|An indication message is created and<br>25% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.|The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 25% packet loss recall test has passed.|\n",
      "|**4 **|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.|  \n",
      "Page | 21  \n",
      "|5|An indication message is created and<br>50% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.|The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 50% packet loss recall test has passed.|\n",
      "|---|---|---|\n",
      "|**6 **|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.|\n",
      "|**7 **|An indication message is created and<br>75% of the message is sent by each IXL<br>to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.|The FIP Tester logs show the recall message<br>arriving and displays a message that shows that<br>the 75% packet loss recall test has passed.|\n",
      "|**8 **|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.|\n",
      "|**9 **|An indication message is created. It is<br>duplicated and sent by each IXL to the<br>FIP. The FIP should receive this message,<br>realise that it is invalid and resend the<br>recall message.|The FIP Tester logs show the recall message<br>arriving again received message and display a<br>message that shows that the duplicate packet<br>recall test has passed.|\n",
      "|**10**|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.|\n",
      "|**11**|An indication message is created. Its CRC<br>is changed to “0000” and it sent by each<br>IXL to the FIP. The FIP should receive this<br>message, realise that it is invalid and<br>resend the recall message.|The FIP Tester logs show the recall message<br>arriving again received message and display a<br>message that shows that the corrupted packet<br>recall test has passed.|\n",
      "|**12**|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.|\n",
      "|**13**|No reply is sent|The FIP will timeout while waiting for a reply<br>and resent a recall message.|\n",
      "|**14**|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message.<br> <br>The display will show that all control tests have<br>passed.|\n",
      "|**15**|Steps 2 – 14 are repeated except a<br>control message is received instead of a<br>recall message.|The FIP Tester logs show the received message<br>and displays a message saying that part of the<br>testing is complete.<br> <br>The display will show that all control tests have<br>passed.|\n",
      "|**16**|Steps 2 – 14 are repeated except a poll<br>message is received instead of a recall<br>message.|The FIP Tester logs show the received message<br>and display a message.<br> <br>The display will show that all poll tests have<br>passed.|\n",
      "|**17**|A recall message is received at each valid<br>IXL.|The FIP Tester logs show the received message<br>and displays a message that shows that the test<br>is complete.<br> <br>Test ends.|  \n",
      "Page | 22  \n",
      "**FIP Test 3: Testing how new configurations are handled**  \n",
      "Description: Ensures that the FIP behaves correctly when a new IXL is initialised after the FIP is already  \n",
      "running.  \n",
      "Pre-test actions:  \n",
      "1) Restart the FIP.  \n",
      "2) Ensure the VM hosting the FIP Tester has the correct networking settings.  \n",
      "3) Run FIP Test 1 with 5 IXLs initialised.  \n",
      "_Table 12 Test specifications for FIP test 3: Testing how new configurations are handled_  \n",
      "|Step No Description Expected Result|Col2|Col3|\n",
      "|---|---|---|\n",
      "|**1 **|After FIP Test 1 has completed, allow<br>time for 10 recall messages to be<br>received at each IXL.|The logs show that FIP Test 1 has completed<br>and 10 recall messages have been sent and<br>replied to appropriately.|\n",
      "|**2 **|A new IXL is initialised.|The start-up testing is repeated for all of the<br>connected IXLs.|\n",
      "|**3 **|A recall message is received at each<br>valid IXL.|The logs show the received message and<br>display a message that shows that the test is<br>complete. Test ends.|  \n",
      "4.1.3 GUI Design  \n",
      "In order to make the system easier to use, a GUI was designed using a two layer structure as  \n",
      "recommended by Borisov et al. [25]. A sketch of the design is shown in Figure 11.  \n",
      "_Figure 11 GUI design sketch_  \n",
      "Key design decisions made based on the two-layer GUI design strategy [25] were:  \n",
      "- Having a simple layout,  \n",
      "- Incorporating a “Help” button which opens the user guide,  \n",
      "- Including a pop-up dialogue box which asks the user to confirm all the settings,  \n",
      "- Allowing the user to control the complexity and type of testing that would run and  \n",
      "- Mimicking the configurations requirements (Hosts file and configurations file) to the FIP’s  \n",
      "configuration requirements in order to minimise tester effort.  \n",
      "Page | 23  \n",
      "4.1.4 Performance Assessment Design  \n",
      "Table 13 describes the tests that were performed to ensure that the FIP Tester was working as  \n",
      "expected.  \n",
      "_Table 13 Tests to verify the FIP Tester’s behaviour_  \n",
      "|Test Test Method Description Purpose<br>Number|Col2|Col3|Col4|\n",
      "|---|---|---|---|\n",
      "|**1 **|‘netcat’ and<br>‘tcpdump’ at<br>the FIP|The Linux commands ‘netcat’ was used to view<br>the incoming and outgoing traffic on a<br>particular IP and port in the FIP and the FIP<br>Tester.<br> <br>Additionally, the command ‘tcpdump’ was used<br>to ensure that all incoming and outgoing packet<br>contents were correct|Ensured that the<br>networking setup is<br>accurate.|\n",
      "|**2 **|FIP Simulator|Created a UDP client that connects to the FIP<br>Tester on a particular IP address and port that<br>would<br>otherwise<br>have<br>been<br>used<br>to<br>communicate with the FIP.<br> <br>A user then created messages in the Python<br>terminal and sent them to the FIP Tester as if<br>the FIP was sending them. For the test cases<br>outlined in Table 10 to Table 12, the FIP’s<br>expected replies were sent via the FIP<br>Simulator.<br> <br>In addition to those test cases, the user also<br>introduced errors to the system and ensured<br>that the FIP Tester responds correctly to those<br>errors.|Ensured that the FIP<br>Tester’s behavior is<br>accurate.|\n",
      "|**3 **|Manually<br>check FIP<br>Tester Logs|All of the FIP Tester’s activities during tests were<br>logged in the log files.<br> <br>A user manually checked the log files and<br>ensure that the correct sequence of events, as<br>defined by test cases shown in Table 10 to Table<br>12, are correct.|Ensured that the<br>tests were<br>repeatable and<br>correct.<br> <br>It was also used to<br>load test the FIP<br>Tester.|  \n",
      "4.1.5 Documentation  \n",
      "The documentation provided for the FIP – IXL Interface Testing Tool was based on the documentation  \n",
      "required by the CENELEC standard EN 50128:2011 [18]. It includes a user guide which provides details  \n",
      "for initiating and running tests and a set of test specifications.  \n",
      "Page | 24' metadata={'Header 1': '**T HE U NIVERSITY OF Q UEENSLAND**', 'Header 2': '4 D ESIGN', 'Header 4': '4.1 FIP T ESTER'}\n"
     ]
    }
   ],
   "source": [
    "print(md_header_splits[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34462878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "\n",
    "def markdown_to_html_file(md_text, output_file=\"output.html\"):\n",
    "    \n",
    "    # Convert Markdown to HTML, enabling table support\n",
    "    html_body = markdown.markdown(md_text, extensions=['tables', 'fenced_code'])\n",
    "\n",
    "    # Wrap in a full HTML template with some basic styling\n",
    "    html_template = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Markdown Preview</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 40px;\n",
    "                line-height: 1.6;\n",
    "            }}\n",
    "            table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            th, td {{\n",
    "                border: 1px solid #333;\n",
    "                padding: 8px;\n",
    "                text-align: left;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #f2f2f2;\n",
    "            }}\n",
    "            pre {{\n",
    "                background-color: #f4f4f4;\n",
    "                padding: 10px;\n",
    "                overflow-x: auto;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        {html_body}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_template)\n",
    "\n",
    "    print(f\"HTML file generated: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "md_text = open(markdown_document_path, \"r\", encoding=\"utf-8\").read()\n",
    "markdown_to_html_file(md_text, output_file=\"document_preview.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
